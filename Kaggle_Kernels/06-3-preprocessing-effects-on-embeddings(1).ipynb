{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Purpose of this kernel - continuance from kernel 6.2\n",
    "\n",
    "* Are there capitalized letters in any embedding at all?\n",
    "* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "# General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# Preprocessing\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Modeling\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "# Training\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "  # splits train-set into into train and validation folds\n",
    "    \n",
    "# Evaluation\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast Run Testing\n",
    "#total_train_samples = 100000 # max is 1306122\n",
    "#total_test_samples = 2000 # max is 375806\n",
    "total_train_samples = 1306122 # max is 1306122\n",
    "total_test_samples = 375806 # max is 375806\n",
    "\n",
    "# Preprocessing\n",
    "maxlen = 130 # 130 covers about 75% of all bad questions completely\n",
    "\n",
    "# Modeling\n",
    "embedding_dim = 300 # set to 300 to be able to compare with pre-trained embeddings\n",
    "\n",
    "# Training\n",
    "kfolds = 3\n",
    "model_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../input/train.csv\")\n",
    "str_ = 'Train data loaded'\n",
    "os.system('echo '+str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape for this run:  1306122 3\n",
      "Shape data tensor: (1306122,)\n",
      "Shape target tensor: (1306122,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province as a nation in the 1960s?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you encourage people to adopt and not shop?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity affect space geometry?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  ...   target\n",
       "0  00002165364db923c7e6  ...        0\n",
       "1  000032939017120e6e44  ...        0\n",
       "2  0000412ca6e4628ce2cf  ...        0\n",
       "\n",
       "[3 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[:total_train_samples] # for Testing purposes\n",
    "num_samples,n = df.shape\n",
    "print(\"Shape for this run: \", num_samples, n)\n",
    "\n",
    "X = df.loc[:, 'question_text'].values\n",
    "y = df.loc[:, 'target'].values\n",
    "\n",
    "# Since Neural Networks are only able to perform transformations on tensors \n",
    "y = np.asarray(y) # Transformation target labels to numpy array \n",
    "\n",
    "print('Shape data tensor:', X.shape) \n",
    "print('Shape target tensor:', y.shape) # 1D Tensor\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1500) # inrease display column size\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (1)  - tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(texts):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(texts) \n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "        padded_seq = pad_sequences(sequences, maxlen=maxlen)  \n",
    "        word_index = tokenizer.word_index  \n",
    "        \n",
    "        return padded_seq, word_index\n",
    "    \n",
    "# Apply tokenization on whole dataset\n",
    "#padded_seq, word_index = my_tokenizer(X)\n",
    "#os.system('echo Tokenization completed')\n",
    "#print(\"Found {} unique tokens\".format(len(word_index)))\n",
    "#print(\"Top 5 most frequent words: {}\".format(\n",
    "#    {word: word_index[word] for word in list(word_index)[:5]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (2)  - Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings path\n",
    "_glove = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "_paragram =  '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "_wiki_news = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "_google_news = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "embeddings = [{'name': 'glove', 'embeddings_path': _glove},\n",
    "              {'name': 'paragram', 'embeddings_path': _paragram},\n",
    "              {'name': 'fasttext', 'embeddings_path': _wiki_news},\n",
    "              {'name': 'googlenews', 'embeddings_path': _google_news}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of functions to load and analyse embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create embedding matrix\n",
    "embedding_matrices = {}\n",
    "words_in_embedding = {}\n",
    "def create_model_embedding_matrix(embeddings_name,word_index,embeddings_dict):\n",
    "\n",
    "    embedding_dim = 300 # (vector size 300!)\n",
    "    embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
    "    unknown_words_dict = {}\n",
    "    num_known_words = 0  \n",
    "\n",
    "    # Filling up matrix\n",
    "    for word, i in word_index.items(): \n",
    "        \n",
    "        if embeddings_name in ['glove', 'paragram', 'fasttext']:\n",
    "            embedding_vector = embeddings_dict.get(word) # get vector for word from embedding \n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "            else:\n",
    "                unknown_words_dict[word] = word_index[word] \n",
    "                \n",
    "        if embeddings_name == 'googlenews':\n",
    "            try:\n",
    "                embedding_vector = embeddings_dict[word]  \n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "            except:\n",
    "                unknown_words_dict[word] = word_index[word]\n",
    "    try: \n",
    "        words_in_embedding[embeddings_name] = list(embeddings_dict.keys())\n",
    "    except:\n",
    "        try:\n",
    "            words_in_embedding[embeddings_name] = list(embeddings_dict.wv.vocab)\n",
    "        except:\n",
    "            print(\"Error during generation of key list {}\".format(embeddings_name))\n",
    "            print(sys.exc_info()[0])\n",
    "    \n",
    "    print('  Embeddings_matrix created')\n",
    "    print('    Shape embedding_matrix: {}'.format(embedding_matrix.shape))\n",
    "    print('  Found Embeddings for {:.2f}% of all words'\n",
    "          .format((num_known_words / len(word_index))*100))\n",
    "    print('  Unknown Words: {:.2f}%'.format((len(unknown_words_dict) / len(word_index))*100))\n",
    "    # Top 50 unknown words\n",
    "    print(\"Top 50 unknown words: {}\\n\".format(\n",
    "    {w: unknown_words_dict[w] for w in list(unknown_words_dict)[:50]}))\n",
    "    \n",
    "    del num_known_words, unknown_words_dict \n",
    "    del embedding_matrix; gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load + analyze Embeddings\n",
    "def load_and_analyse_Embeddings(embeddings_name, embeddings_path):\n",
    "    \n",
    "    if embeddings_name in ['glove', 'paragram', 'fasttext']:  \n",
    "        embeddings_dict = {} # create empty embedding dictionary\n",
    "        embedding_file = open(embeddings_path, encoding =\"utf8\", errors = 'ignore') # load embedding from path\n",
    "\n",
    "        # Fill embedding dict with word: vector(coefs) pairs\n",
    "        for line in embedding_file:\n",
    "            line_values = line.split(' ') # read in values of respective line (= vector)\n",
    "            word = line_values[0] #  # first value in line represents the word\n",
    "            coefs = np.asarray(line_values[1:], dtype='float32') # all values represent vector\n",
    "            embeddings_dict[word] = coefs # add key(word), value(vector) pairs to dict\n",
    "\n",
    "        embedding_file.close() \n",
    "        \n",
    "        os.system('echo '+ embeddings_name + 'loaded')\n",
    "        print('  ',embeddings_name, 'loaded')\n",
    "        print('  {} word vectors within {} dict'.format(len(embeddings_dict),embeddings_name))\n",
    "        \n",
    "        # Use pre-trained embedding to create final embeddings matrix\n",
    "        create_model_embedding_matrix(embeddings_name,word_index,embeddings_dict)\n",
    "        del embeddings_dict, line_values,word,coefs\n",
    "                \n",
    "    if embeddings_name == 'googlenews':\n",
    "        embeddings_file = KeyedVectors.load_word2vec_format(embeddings_path, binary=True)\n",
    "        \n",
    "        os.system('echo '+ embeddings_name + 'loaded')\n",
    "        print('  ',embeddings_name, 'loaded')\n",
    "        \n",
    "        # Use pre-trained embedding to create final embeddings matrix\n",
    "        create_model_embedding_matrix(embeddings_name,word_index,embeddings_file)\n",
    "        del embeddings_file\n",
    "        \n",
    "    # MEMORY MANAGEMENT!\n",
    "    del embeddings_name, embeddings_path\n",
    "    gc.collect()\n",
    "    \n",
    "   # return embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (3)  - Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition mapping and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \n",
    "                       \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \n",
    "                       \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \n",
    "                       \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n",
    "                       \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n",
    "                       \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n",
    "                       \"I'm\": \"I am\",\"i'm\": \"i am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
    "                       \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                       \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n",
    "                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n",
    "                       \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \n",
    "                       \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \n",
    "                       \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \n",
    "                       \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n",
    "                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                       \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n",
    "                       \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
    "                       \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                       \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n",
    "                       \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
    "                       \"weren't\": \"were not\",\"what`s\": \"what is\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
    "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                       \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                       \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                       \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \n",
    "                       \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "                       \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
    "                       \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n",
    "                       \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n",
    "                       \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "# dict from https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2 \n",
    "correct_spell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite',\n",
    "                    'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater',\n",
    "                    'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization',\n",
    "                    'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n",
    "                    'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What',\n",
    "                    'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are',\n",
    "                    'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many',\n",
    "                    'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best',\n",
    "                    'howdoes': 'how does', 'mastrubation': 'masturbation',\n",
    "                    'mastrubate': 'masturbate', \"mastrubating\": 'masturbating',\n",
    "                    \"mcdonald's\":'mcdonalds',\n",
    "                    'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist',\n",
    "                    'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', \n",
    "                    'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what',\n",
    "                    'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n",
    "                    'demonitization': 'demonetization', 'demonetisation': 'demonetization',\n",
    "                    'pokémon': 'pokemon'}\n",
    "\n",
    "# Kernel \"fork-embeddings-keras-v04\"\n",
    "specials_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \n",
    "                 \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', \n",
    "                 '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', \n",
    "                 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '\\u200b': ' ',\n",
    "                 '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\n",
    "\n",
    "punct = \"/-?!.,#$%\\()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk import word_tokenize\n",
    "\n",
    "def clean_numbers(x):\n",
    "    # replaces one digit by #, two following digits by ## etc.\n",
    "    x = re.sub('[0-9]{5,}', '#####', x) \n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "def preprocessing(x):\n",
    "    x = str(x)\n",
    "    x = x.lower()\n",
    "    x = re.sub('[’‘´`]', \"'\", x)\n",
    "    \n",
    "    x = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in x.split(\" \")])\n",
    "        # removes \"all whitespace characters (space, tab, newline, return, formfeed)\n",
    "    for word in x.split():\n",
    "        if word in correct_spell_dict.keys():\n",
    "            x = x.replace(word, correct_spell_dict[word])\n",
    "        if word in specials_mapping.keys():\n",
    "            x = re.sub(word, specials_mapping.get(word),x)\n",
    "    x = re.sub('\\'s\\s+', ' \\'s ', x)\n",
    "    x = ' '.join(word_tokenize(x))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('echo Applying preprocessing functions..')\n",
    "df[\"question_text\"] = df[\"question_text\"].fillna(\"_nan_\").apply(lambda x: preprocessing(x))\n",
    "os.system('echo prepocessing 1 done')\n",
    "df[\"question_text\"] = df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "os.system('echo clean_numbers done')\n",
    "\n",
    "X = df.loc[:, 'question_text'].values\n",
    "y = np.asarray(df.loc[:, 'target'].values)\n",
    "\n",
    "padded_seq, word_index = my_tokenizer(X) # Tokenization\n",
    "os.system('echo Tokenization 2 completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration loop to compare different embeddings (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running procedure on glove:\n",
      "   glove loaded\n",
      "  2196016 word vectors within glove dict\n",
      "  Embeddings_matrix created\n",
      "    Shape embedding_matrix: (192325, 300)\n",
      "  Found Embeddings for 61.35% of all words\n",
      "  Unknown Words: 38.65%\n",
      "Top 50 unknown words: {'quorans': 1899, 'brexit': 2853, 'cryptocurrencies': 2964, 'redmi': 3665, 'kvpy': 3875, 'paytm': 3886, 'iiser': 3962, 'ethereum': 4037, \"'the\": 4539, 'iisc': 4601, 'jinping': 5573, 'viteee': 6074, 'iocl': 6244, '₹': 6592, 'nmims': 6651, 'rohingya': 6829, 'upes': 6834, 'fortnite': 6845, 'coinbase': 7052, 'nsit': 7119, 'cpec': 7203, 'iitians': 7257, 'oneplus': 7402, 'jadavpur': 7422, \"'i\": 7556, 'udemy': 7818, 'lyft': 7892, 'uceed': 7984, 'bahubali': 8008, 'afcat': 8041, 'coep': 8188, 'bhakts': 8442, 'upwork': 8573, 'machedo': 8725, 'gdpr': 8813, 'nlu': 8835, 'adityanath': 8855, \"'a\": 8892, 'upsee': 8924, 'boruto': 9073, 'bnbr': 9198, 'chsl': 9226, 'kernan': 9376, 'amcat': 9545, 'josaa': 9555, 'udacity': 9595, 'kylo': 9645, 'alshamsi': 9699, 'vishwanathan': 9713, 'iitian': 9756}\n",
      "\n",
      "Running procedure on paragram:\n",
      "   paragram loaded\n",
      "  1703755 word vectors within paragram dict\n",
      "  Embeddings_matrix created\n",
      "    Shape embedding_matrix: (192325, 300)\n",
      "  Found Embeddings for 72.07% of all words\n",
      "  Unknown Words: 27.93%\n",
      "Top 50 unknown words: {'quorans': 1899, 'brexit': 2853, 'cryptocurrencies': 2964, 'redmi': 3665, \"'the\": 4539, '₹': 6592, 'coinbase': 7052, 'oneplus': 7402, \"'i\": 7556, 'uceed': 7984, 'bhakts': 8442, 'upwork': 8573, 'machedo': 8725, 'gdpr': 8813, 'adityanath': 8855, \"'a\": 8892, 'boruto': 9073, 'bnbr': 9198, 'alshamsi': 9699, 'dceu': 9798, 'litecoin': 10005, 'iiest': 10096, 'unacademy': 10174, 'sjws': 10267, \"qur'an\": 10295, 'qoura': 10364, \"5'\": 10456, 'zerodha': 10520, 'tensorflow': 11003, 'doklam': 11365, 'kavalireddi': 11464, 'lnmiit': 11565, '°c': 11718, 'muoet': 11786, 'etc…': 11965, 'nicmar': 12265, \"5'4\": 12364, 'vajiram': 12500, 'adhaar': 12580, 'zebpay': 12817, 'srmjee': 12833, 'elitmus': 12905, \"5'9\": 13100, \"'no\": 13370, 'altcoins': 13382, 'altcoin': 13393, 'hackerrank': 13422, \"5'7\": 13622, 'awdhesh': 13655, 'jiren': 13719}\n",
      "\n",
      "Running procedure on fasttext:\n",
      "   fasttext loaded\n",
      "  999995 word vectors within fasttext dict\n",
      "  Embeddings_matrix created\n",
      "    Shape embedding_matrix: (192325, 300)\n",
      "  Found Embeddings for 47.22% of all words\n",
      "  Unknown Words: 52.78%\n",
      "Top 50 unknown words: {\"''\": 55, 'upsc': 791, \"n't\": 1242, 'aiims': 1590, 'cgl': 1771, 'quorans': 1899, 'jio': 2084, 'manipal': 2148, 'icse': 2365, 'iiit': 2668, 'bitsat': 2675, 'cgpa': 2676, 'ielts': 2718, 'mtech': 2933, 'ncert': 3018, 'clat': 3245, 'isro': 3257, 'pilani': 3623, 'ibps': 3640, 'bhu': 3648, 'redmi': 3665, 'h1b': 3684, 'nift': 3811, 'comedk': 3858, 'thanos': 3870, 'kvpy': 3875, 'paytm': 3886, 'iiser': 3962, 'llb': 3970, 'accenture': 3977, 'ignou': 4110, 'quoran': 4253, 'dtu': 4258, 'aadhar': 4287, 'lenovo': 4485, 'gmat': 4562, 'iisc': 4601, 'kiit': 4685, 'shopify': 4686, 'iims': 4687, 'fiitjee': 4890, 'kejriwal': 4939, 'pgdm': 5037, 'wbjee': 5167, 'trudeau': 5185, 'nri': 5232, 'deloitte': 5323, 'bcom': 5414, 'kcet': 5513, 'jinping': 5573}\n",
      "\n",
      "Running procedure on googlenews:\n",
      "   googlenews loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:33: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Embeddings_matrix created\n",
      "    Shape embedding_matrix: (192325, 300)\n",
      "  Found Embeddings for 39.86% of all words\n",
      "  Unknown Words: 60.14%\n",
      "Top 50 unknown words: {'a': 4, 'to': 5, 'of': 7, 'and': 11, \"'s\": 43, \"''\": 55, 'quora': 105, \"'\": 125, '”': 428, '“': 442, 'instagram': 694, 'upsc': 791, 'bitcoin': 878, 'mbbs': 1111, 'whatsapp': 1123, 'ece': 1429, 'aiims': 1590, 'iim': 1708, 'sbi': 1740, 'cgl': 1771, 'cryptocurrency': 1878, 'quorans': 1899, 'btech': 1931, 'snapchat': 1961, 'obc': 1980, 'jio': 2084, 'manipal': 2148, 'bba': 2210, 'icse': 2365, 'tcs': 2460, 'srm': 2473, 'blockchain': 2567, 'narendra': 2574, 'elon': 2613, 'iiit': 2668, 'bitsat': 2675, 'cgpa': 2676, 'ielts': 2718, 'brexit': 2853, 'mtech': 2933, 'iits': 2944, 'cryptocurrencies': 2964, 'ncert': 3018, 'behaviour': 3109, 'programme': 3236, 'clat': 3245, 'isro': 3257, 'upvotes': 3316, 'bca': 3360, 'defence': 3408}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for embedding in embeddings:\n",
    "    emb_name = embedding['name']\n",
    "    emb_path = embedding['embeddings_path']\n",
    "    print(\"Running procedure on {}:\".format(emb_name))\n",
    "    \n",
    "    load_and_analyse_Embeddings(emb_name, emb_path) # loading embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are there vectors for Interpunctuation in the embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove: Matched!\n",
      "paragram: Matched!\n",
      "fasttext: Matched!\n",
      "googlenews: Matched!\n"
     ]
    }
   ],
   "source": [
    "from re import *\n",
    "\n",
    "re = compile('[,\\.!\"()\"]') # search some interpunctuation marks\n",
    "for embedding in words_in_embedding:\n",
    "    if any(re.match(line) for line in words_in_embedding[embedding]):\n",
    "        print(\"{}: Matched!\".format(embedding))#\n",
    "        \n",
    "# no punctuation in googlenews!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are there any uppercase words in the embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove: Matched!\n",
      "['The', 'It', 'This', 'In', 'You', 'We', 'If', 'And', 'New', 'For']\n",
      "paragram: Matched!\n",
      "['UUUNKKK']\n",
      "fasttext: Matched!\n",
      "['The', 'Wikipedia', 'In', 'UTC', 'If', 'But', 'This', 'It', 'And', 'He']\n",
      "googlenews: Matched!\n",
      "['The', 'It', 'He', 'We', 'In', 'But', 'This', 'They', 'By', 'If']\n"
     ]
    }
   ],
   "source": [
    "re = compile('[A-Z]{1,}\\w+') # match leading upper case letter\n",
    "for embedding in words_in_embedding:\n",
    "        print(\"{}: Matched!\".format(embedding))\n",
    "        print(re.findall(' '.join(words_in_embedding[embedding]))[:10])\n",
    "        \n",
    "# yes there are capitalized letters!\n",
    "# paragram embedding seems not to have capital letters. Only one strange \"UUUNKKK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Having a look at the sentences that contain unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "are quoras indian administrators turning a blind eye to indian quorans who use abusive language and ask insincere questions ?\n"
     ]
    }
   ],
   "source": [
    "unknown_words_to_check = {'quorans': 1899, 'brexit': 2853}\n",
    "\n",
    "#print(list(unknown_words_to_check.keys()))\n",
    "#re = [compile(\"^\"+i+\"$\") for i in list(unknown_words_to_check.keys())]\n",
    "re = compile('quorans')\n",
    "for line in df[\"question_text\"].values:\n",
    "    if any(re.match(word) for word in line.split()):\n",
    "            print(line)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
