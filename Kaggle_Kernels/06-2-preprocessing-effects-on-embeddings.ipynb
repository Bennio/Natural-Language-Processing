{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Purpose of this kernel - continuance from kernel 6\n",
    "\n",
    "* Achieve further preprocessing improvements to increase embeddings coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "# General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# Preprocessing\n",
    "import seaborn as sns\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Modeling\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "# Training\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "  # splits train-set into into train and validation folds\n",
    "    \n",
    "# Evaluation\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast Run Testing\n",
    "#total_train_samples = 100000 # max is 1306122\n",
    "#total_test_samples = 2000 # max is 375806\n",
    "total_train_samples = 1306122 # max is 1306122\n",
    "total_test_samples = 375806 # max is 375806\n",
    "\n",
    "# Preprocessing\n",
    "maxlen = 130 # 130 covers about 75% of all bad questions completely\n",
    "\n",
    "# Modeling\n",
    "embedding_dim = 300 # set to 300 to be able to compare with pre-trained embeddings\n",
    "\n",
    "# Training\n",
    "kfolds = 3\n",
    "model_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../input/train.csv\")\n",
    "str_ = 'Train data loaded'\n",
    "os.system('echo '+str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape for this run:  1306122 3\n",
      "Shape data tensor: (1306122,)\n",
      "Shape target tensor: (1306122,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province as a nation in the 1960s?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you encourage people to adopt and not shop?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity affect space geometry?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  ...   target\n",
       "0  00002165364db923c7e6  ...        0\n",
       "1  000032939017120e6e44  ...        0\n",
       "2  0000412ca6e4628ce2cf  ...        0\n",
       "\n",
       "[3 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[:total_train_samples] # for Testing purposes\n",
    "num_samples,n = df.shape\n",
    "print(\"Shape for this run: \", num_samples, n)\n",
    "\n",
    "X = df.loc[:, 'question_text'].values\n",
    "y = df.loc[:, 'target'].values\n",
    "\n",
    "# Since Neural Networks are only able to perform transformations on tensors \n",
    "y = np.asarray(y) # Transformation target labels to numpy array \n",
    "\n",
    "print('Shape data tensor:', X.shape) \n",
    "print('Shape target tensor:', y.shape) # 1D Tensor\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1500) # inrease display column size\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (1)  - tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(texts):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(texts) \n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "        padded_seq = pad_sequences(sequences, maxlen=maxlen)  \n",
    "        word_index = tokenizer.word_index  \n",
    "        \n",
    "        return padded_seq, word_index\n",
    "    \n",
    "# Apply tokenization on whole dataset\n",
    "#padded_seq, word_index = my_tokenizer(X)\n",
    "#os.system('echo Tokenization completed')\n",
    "#print(\"Found {} unique tokens\".format(len(word_index)))\n",
    "#print(\"Top 5 most frequent words: {}\".format(\n",
    "#    {word: word_index[word] for word in list(word_index)[:5]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (2)  - Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings path\n",
    "_glove = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "_paragram =  '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "_wiki_news = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "_google_news = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "embeddings = [{'name': 'glove', 'embeddings_path': _glove},\n",
    "              {'name': 'paragram', 'embeddings_path': _paragram},\n",
    "              {'name': 'fasttext', 'embeddings_path': _wiki_news},\n",
    "              {'name': 'googlenews', 'embeddings_path': _google_news}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of functions to load and analyse embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create embedding matrix\n",
    "embedding_matrices = {}\n",
    "words_in_embedding = {}\n",
    "def create_model_embedding_matrix(embeddings_name,word_index,embeddings_dict):\n",
    "\n",
    "    embedding_dim = 300 # (vector size 300!)\n",
    "    embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
    "    unknown_words_dict = {}\n",
    "    num_known_words = 0  \n",
    "\n",
    "    # Filling up matrix\n",
    "    for word, i in word_index.items(): \n",
    "        \n",
    "        if embeddings_name in ['glove', 'paragram', 'fasttext']:\n",
    "            embedding_vector = embeddings_dict.get(word) # get vector for word from embedding \n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "            else:\n",
    "                unknown_words_dict[word] = word_index[word] \n",
    "                \n",
    "        if embeddings_name == 'googlenews':\n",
    "            try:\n",
    "                embedding_vector = embeddings_dict[word]  \n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "            except:\n",
    "                unknown_words_dict[word] = word_index[word]\n",
    "    try: \n",
    "        words_in_embedding[embeddings_name] = list(embeddings_dict.keys())\n",
    "    except:\n",
    "        print(\"Error during generation of key list {}\".format(embeddings_name))\n",
    "        print(sys.exc_info()[0])\n",
    "    \n",
    "    print('  Embeddings_matrix created')\n",
    "    print('    Shape embedding_matrix: {}'.format(embedding_matrix.shape))\n",
    "    print('  Found Embeddings for {:.2f}% of all words'\n",
    "          .format((num_known_words / len(word_index))*100))\n",
    "    print('  Unknown Words: {:.2f}%'.format((len(unknown_words_dict) / len(word_index))*100))\n",
    "    # Top 50 unknown words\n",
    "    print(\"Top 50 unknown words: {}\\n\".format(\n",
    "    {w: unknown_words_dict[w] for w in list(unknown_words_dict)[:50]}))\n",
    "    \n",
    "    del num_known_words, unknown_words_dict \n",
    "    del embedding_matrix; gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load + analyze Embeddings\n",
    "def load_and_analyse_Embeddings(embeddings_name, embeddings_path):\n",
    "    \n",
    "    if embeddings_name in ['glove', 'paragram', 'fasttext']:  \n",
    "        embeddings_dict = {} # create empty embedding dictionary\n",
    "        embedding_file = open(embeddings_path, encoding =\"utf8\", errors = 'ignore') # load embedding from path\n",
    "\n",
    "        # Fill embedding dict with word: vector(coefs) pairs\n",
    "        for line in embedding_file:\n",
    "            line_values = line.split(' ') # read in values of respective line (= vector)\n",
    "            word = line_values[0] #  # first value in line represents the word\n",
    "            coefs = np.asarray(line_values[1:], dtype='float32') # all values represent vector\n",
    "            embeddings_dict[word] = coefs # add key(word), value(vector) pairs to dict\n",
    "\n",
    "        embedding_file.close() \n",
    "        \n",
    "        os.system('echo '+ embeddings_name + 'loaded')\n",
    "        print('  ',embeddings_name, 'loaded')\n",
    "        print('  {} word vectors within {} dict'.format(len(embeddings_dict),embeddings_name))\n",
    "        \n",
    "        # Use pre-trained embedding to create final embeddings matrix\n",
    "        create_model_embedding_matrix(embeddings_name,word_index,embeddings_dict)\n",
    "        del embeddings_dict, line_values,word,coefs\n",
    "                \n",
    "    if embeddings_name == 'googlenews':\n",
    "        embeddings_file = KeyedVectors.load_word2vec_format(embeddings_path, binary=True)\n",
    "        \n",
    "        os.system('echo '+ embeddings_name + 'loaded')\n",
    "        print('  ',embeddings_name, 'loaded')\n",
    "        \n",
    "        # Use pre-trained embedding to create final embeddings matrix\n",
    "        create_model_embedding_matrix(embeddings_name,word_index,embeddings_file)\n",
    "        del embeddings_file\n",
    "        \n",
    "    # MEMORY MANAGEMENT!\n",
    "    del embeddings_name, embeddings_path\n",
    "    gc.collect()\n",
    "    \n",
    "   # return embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (3)  - Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition mapping and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \n",
    "                       \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \n",
    "                       \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \n",
    "                       \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n",
    "                       \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n",
    "                       \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n",
    "                       \"I'm\": \"I am\",\"i'm\": \"i am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
    "                       \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                       \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n",
    "                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n",
    "                       \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \n",
    "                       \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \n",
    "                       \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \n",
    "                       \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n",
    "                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                       \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n",
    "                       \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
    "                       \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                       \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n",
    "                       \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
    "                       \"weren't\": \"were not\",\"what`s\": \"what is\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
    "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                       \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                       \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                       \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \n",
    "                       \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "                       \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
    "                       \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n",
    "                       \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n",
    "                       \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "# dict from https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2 \n",
    "correct_spell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite',\n",
    "                    'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater',\n",
    "                    'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization',\n",
    "                    'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n",
    "                    'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What',\n",
    "                    'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are',\n",
    "                    'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many',\n",
    "                    'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best',\n",
    "                    'howdoes': 'how does', 'mastrubation': 'masturbation',\n",
    "                    'mastrubate': 'masturbate', \"mastrubating\": 'masturbating',\n",
    "                    \"mcdonald's\":'mcdonalds',\n",
    "                    'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist',\n",
    "                    'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', \n",
    "                    'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what',\n",
    "                    'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n",
    "                    'demonitization': 'demonetization', 'demonetisation': 'demonetization',\n",
    "                    'pokémon': 'pokemon'}\n",
    "\n",
    "# Kernel \"fork-embeddings-keras-v04\"\n",
    "specials_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \n",
    "                 \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', \n",
    "                 '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', \n",
    "                 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '\\u200b': ' ',\n",
    "                 '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}\n",
    "\n",
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def further_cleaning(x):\n",
    "    x = str(x)\n",
    "    x = x.lower()\n",
    "    x = re.sub('[’‘´`]', \"'\", x)\n",
    "    for word in x.split():\n",
    "        if word in specials_mapping.keys():\n",
    "            x = re.sub(word, specials_mapping.get(word),x)\n",
    "        if word in contraction_mapping.keys():\n",
    "            x = re.sub(word, contraction_mapping.get(word),x)\n",
    "    x = re.sub('\\'s\\s', ' ', x)\n",
    "    for p in punct:\n",
    "        x = x.replace(p, f' {p} ')\n",
    "    return x\n",
    "\n",
    "def clean_numbers(x):\n",
    "    # replaces one digit by #, two following digits by ## etc.\n",
    "    x = re.sub('[0-9]{5,}', '#####', x) \n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "def correct_spelling(x):\n",
    "    x = str(x)\n",
    "    x = x.lower()\n",
    "    for word in x.split():\n",
    "        if word in correct_spell_dict.keys():\n",
    "            x = x.replace(word, correct_spell_dict[word])\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('echo Applying preprocessing functions..')\n",
    "df[\"question_text\"] = df[\"question_text\"].apply(lambda x: correct_spelling(x))\n",
    "os.system('echo correct_spelling done')\n",
    "df[\"question_text\"] = df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "os.system('echo clean_numbers done')\n",
    "df[\"question_text\"] = df[\"question_text\"].apply(lambda x: further_cleaning(x))\n",
    "os.system('echo further_cleaning done')\n",
    "\n",
    "X = df.loc[:, 'question_text'].values\n",
    "y = np.asarray(df.loc[:, 'target'].values)\n",
    "\n",
    "padded_seq, word_index = my_tokenizer(X) # Tokenization\n",
    "os.system('echo Tokenization 2 completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration loop to compare different embeddings (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running procedure on glove:\n",
      "   glove loaded\n",
      "  2196016 word vectors within glove dict\n",
      "  Embeddings_matrix created\n",
      "    Shape embedding_matrix: (187145, 300)\n",
      "  Found Embeddings for 63.16% of all words\n",
      "  Unknown Words: 36.84%\n",
      "Top 50 unknown words: {'quorans': 1900, 'brexit': 2842, 'cryptocurrencies': 2965, 'redmi': 3658, 'kvpy': 3874, 'paytm': 3884, 'iiser': 3962, 'ethereum': 4035, 'iisc': 4600, 'jinping': 5579, '₹': 5769, 'viteee': 6083, 'iocl': 6251, 'nmims': 6658, 'rohingya': 6834, 'upes': 6840, 'fortnite': 6855, 'coinbase': 7057, 'nsit': 7125, 'cpec': 7184, 'iitians': 7297, 'oneplus': 7412, 'jadavpur': 7435, 'udemy': 7829, 'lyft': 7899, 'uceed': 7990, 'bahubali': 8019, 'afcat': 8053, 'coep': 8202, 'bhakts': 8423, 'upwork': 8586, 'machedo': 8734, 'gdpr': 8827, 'nlu': 8846, 'adityanath': 8863, 'upsee': 8932, 'boruto': 9082, 'bnbr': 9212, 'chsl': 9236, 'kernan': 9388, 'amcat': 9563, 'josaa': 9574, 'udacity': 9612, 'kylo': 9660, 'alshamsi': 9715, 'vishwanathan': 9731, 'iitian': 9773, 'dceu': 9810, 'litecoin': 10015, 'unacademy': 10109}\n",
      "\n",
      "Running procedure on paragram:\n",
      "   paragram loaded\n",
      "  1703755 word vectors within paragram dict\n",
      "  Embeddings_matrix created\n",
      "    Shape embedding_matrix: (187145, 300)\n",
      "  Found Embeddings for 74.19% of all words\n",
      "  Unknown Words: 25.81%\n",
      "Top 50 unknown words: {'quorans': 1900, 'brexit': 2842, 'cryptocurrencies': 2965, 'redmi': 3658, '₹': 5769, 'coinbase': 7057, 'oneplus': 7412, 'uceed': 7990, 'bhakts': 8423, 'upwork': 8586, 'machedo': 8734, 'gdpr': 8827, 'adityanath': 8863, 'boruto': 9082, 'bnbr': 9212, 'alshamsi': 9715, 'dceu': 9810, 'litecoin': 10015, 'unacademy': 10109, 'iiest': 10113, 'sjws': 10202, 'qoura': 10373, 'zerodha': 10531, 'tensorflow': 11017, 'doklam': 11385, 'kavalireddi': 11478, 'lnmiit': 11582, 'muoet': 11798, 'etc…': 12087, 'nicmar': 12276, 'vajiram': 12504, 'adhaar': 12583, 'zebpay': 12833, 'srmjee': 12851, 'elitmus': 12923, 'altcoins': 13391, 'altcoin': 13402, 'hackerrank': 13435, 'awdhesh': 13671, 'jiren': 13734, 'ryzen': 13995, 'baahubali': 14258, 'koinex': 14341, 'binance': 14855, 'mhcet': 14876, 'byju': 15051, 'srmjeee': 15402, 'beerus': 15647, 'sgsits': 15738, 'skripal': 15789}\n",
      "\n",
      "Running procedure on fasttext:\n",
      "   fasttext loaded\n",
      "  999995 word vectors within fasttext dict\n",
      "  Embeddings_matrix created\n",
      "    Shape embedding_matrix: (187145, 300)\n",
      "  Found Embeddings for 48.51% of all words\n",
      "  Unknown Words: 51.49%\n",
      "Top 50 unknown words: {'upsc': 791, 'aiims': 1589, 'cgl': 1775, 'quorans': 1900, 'jio': 2083, 'manipal': 2150, 'icse': 2365, 'iiit': 2671, 'bitsat': 2673, 'cgpa': 2679, 'ielts': 2715, 'mtech': 2933, 'ncert': 3020, 'clat': 3245, 'isro': 3255, 'pilani': 3623, 'ibps': 3626, 'bhu': 3646, 'redmi': 3658, 'h1b': 3686, 'nift': 3811, 'comedk': 3856, 'thanos': 3869, 'kvpy': 3874, 'paytm': 3884, 'iiser': 3962, 'llb': 3969, 'accenture': 3975, 'ignou': 4109, 'quoran': 4233, 'dtu': 4256, 'aadhar': 4284, 'lenovo': 4487, 'gmat': 4559, 'iisc': 4600, 'kiit': 4684, 'shopify': 4685, 'iims': 4686, 'fiitjee': 4890, 'kejriwal': 4939, 'pgdm': 5038, 'wbjee': 5157, 'trudeau': 5191, 'nri': 5236, 'deloitte': 5324, 'bcom': 5415, 'kcet': 5515, 'jinping': 5579, 'articleship': 5637, 'virat': 5640}\n",
      "\n",
      "Running procedure on googlenews:\n",
      "   googlenews loaded\n",
      "Error during generation of key list googlenews\n",
      "<class 'AttributeError'>\n",
      "  Embeddings_matrix created\n",
      "    Shape embedding_matrix: (187145, 300)\n",
      "  Found Embeddings for 40.99% of all words\n",
      "  Unknown Words: 59.01%\n",
      "Top 50 unknown words: {'a': 4, 'to': 5, 'of': 7, 'and': 11, \"'\": 62, 'quora': 104, '”': 428, '“': 443, 'instagram': 696, 'upsc': 791, 'bitcoin': 876, 'mbbs': 1110, 'whatsapp': 1122, 'ece': 1427, 'aiims': 1589, 'iim': 1707, 'sbi': 1742, 'cgl': 1775, 'cryptocurrency': 1878, 'quorans': 1900, 'btech': 1932, 'snapchat': 1961, 'obc': 1983, 'jio': 2083, 'manipal': 2150, 'bba': 2210, 'icse': 2365, 'tcs': 2459, 'srm': 2472, 'blockchain': 2559, 'narendra': 2573, 'elon': 2614, 'iiit': 2671, 'bitsat': 2673, 'cgpa': 2679, 'ielts': 2715, 'brexit': 2842, 'mtech': 2933, 'iits': 2945, 'cryptocurrencies': 2965, 'ncert': 3020, 'behaviour': 3107, 'programme': 3236, 'clat': 3245, 'isro': 3255, 'upvotes': 3318, 'bca': 3361, 'defence': 3407, 'grey': 3449, 'mueller': 3560}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for embedding in embeddings:\n",
    "    emb_name = embedding['name']\n",
    "    emb_path = embedding['embeddings_path']\n",
    "    print(\"Running procedure on {}:\".format(emb_name))\n",
    "    \n",
    "    load_and_analyse_Embeddings(emb_name, emb_path) # loading embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(\"mcdonald's\" in words_in_embedding['glove'])\n",
    "print(\"mcdonalds\" in words_in_embedding['glove'])\n",
    "\n",
    "print(\"woman's\" in words_in_embedding['glove'])\n",
    "print(\"woman\" in words_in_embedding['glove'])\n",
    "\n",
    "print(\"people's\" in words_in_embedding['glove'])\n",
    "print(\"people\" in words_in_embedding['glove'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparing Different Stemming  & Lemmatisation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragram_out = {'quorans': 1901, 'brexit': 2852, 'redmi': 3659, \"'the\": 4605, '₹': 6002, 'coinbase': 7040, '“the': 7246, 'oneplus': 7473, 'uceed': 7976, \"'i\": 8422, 'bhakts': 8429, 'upwork': 8561, \"5'\": 8756, 'machedo': 8780, 'gdpr': 8801, 'adityanath': 8843, 'boruto': 9065, 'bnbr': 9183, \"isn't\": 9232, '“i': 9623, 'alshamsi': 9683, 'dceu': 9793, \"parents'\": 9895, 'litecoin': 9994, 'iiest': 10091, 'unacademy': 10164, 'sjws': 10252, \"qur'an\": 10285, 'qoura': 10354, \"aren't\": 10361, 'zerodha': 10511, 'tensorflow': 10983, 'doklam': 11349, 'kavalireddi': 11449, 'lnmiit': 11546, '°c': 11707, 'muoet': 11774, \"others'\": 11947, \"countries'\": 12032, \"us'\": 12066, \"you'\": 12147, 'etc…': 12164, 'nicmar': 12259, 'vajiram': 12495, 'adhaar': 12569, 'zebpay': 12809, 'srmjee': 12827, 'elitmus': 12899, \"5'4\": 13044}\n",
    "unknown_words_list = paragram_out.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word           Porter Stemmerlancaster StemmerSnowball StemmerLemma          remove_suffix_s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'remove_suffix_s' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-4c92252fb138>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m                                                   \u001b[0;34m,\u001b[0m \u001b[0menglishStemmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                                                   \u001b[0mwordnet_lemmatizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemmatize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"v\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                                                  remove_suffix_s(word)))\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'remove_suffix_s' is not defined"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer \n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "lancaster = LancasterStemmer()\n",
    "englishStemmer=SnowballStemmer(\"english\")\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(\"{0:15}{1:10}{2:10}{3:15}{4:15}{5:15}\".format(\"Word\",\"Porter Stemmer\",\"lancaster Stemmer\",\n",
    "                                        \"Snowball Stemmer\", \"Lemma\", \"remove_suffix_s\"))\n",
    "for word in unknown_words_list:\n",
    "    print(\"{0:15}{1:10}{2:10}{3:15}{4:15}{5:15}\".format(word,porter.stem(word),lancaster.stem(word)\n",
    "                                                  , englishStemmer.stem(word), \n",
    "                                                  wordnet_lemmatizer.lemmatize(word, pos = \"v\"),\n",
    "                                                 remove_suffix_s(word)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
