{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### Purpose of this kernel\n\n* Averaring embeddings Paragram + GloVe with CuDNNLSTM"},{"metadata":{},"cell_type":"markdown","source":"#### Import libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture --no-stdout\n\n# General\nimport pandas as pd\nimport numpy as np\nimport os\nimport gc\nimport sys\nimport time\n\n# Preprocessing\nimport seaborn as sns\nimport re\nfrom re import *\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import KeyedVectors\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\nfrom nltk.stem import SnowballStemmer \nfrom nltk import pos_tag, word_tokenize\nfrom nltk.corpus import wordnet as wn\nlemmatizer = nltk.WordNetLemmatizer()\n\n# Modeling\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense\nfrom keras.layers import SimpleRNN, GRU, Bidirectional, LSTM,CuDNNLSTM, CuDNNGRU\nfrom keras.layers import SpatialDropout1D, Dropout\nfrom keras.layers.pooling import GlobalMaxPooling1D\n\n# Training\n# from sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\n# Evaluation\nfrom keras.callbacks import Callback\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check status and availability of GPU\nimport tensorflow as tf\nprint(\"GPU on?  - \", tf.test.is_gpu_available())\nprint(\"Available GPUs: \", tf.test.gpu_device_name())\n\n# confirm Keras sees the GPU\nfrom keras import backend\nassert len(backend.tensorflow_backend._get_available_gpus()) > 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fast Run Testing\n#total_train_samples = 30000 # max is 1306122\ntotal_train_samples = 1306122 # max is 1306122\n\n# Preprocessing\nmaxlen = 130 # 130 - covers about 75% of all bad questions completely\n\n# Training\nkfolds = 3 # 80/20 split\nmodel_epochs = 10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/quora-insincere-questions-classification/train.csv\")\nstr_ = 'Train data loaded'\nos.system('echo '+str_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[:total_train_samples] # for Testing purposes\nnum_samples,n = df.shape\nprint(\"Shape for this run: \", num_samples, n)\n\nX = df.loc[:, 'question_text'].values\ny = df.loc[:, 'target'].values\n\n# Since Neural Networks are only able to perform transformations on tensors \ny = np.asarray(y) # Transformation target labels to numpy array \n\nprint('Shape data tensor:', X.shape) \nprint('Shape target tensor:', y.shape) # 1D Tensor\n\npd.set_option('display.max_colwidth', 1500) # inrease display column size\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preparation (1)  - tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_tokenizer(texts):\n        tokenizer = Tokenizer()\n        tokenizer.fit_on_texts(texts) \n        sequences = tokenizer.texts_to_sequences(texts)\n        padded_seq = pad_sequences(sequences, maxlen=maxlen)  \n        word_index = tokenizer.word_index  \n        \n        return padded_seq, word_index","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_words = len(word_index)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preparation (2)  - Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Embeddings path\n_glove = '/kaggle/input/quora-insincere-questions-classification/embeddings/glove.840B.300d/glove.840B.300d.txt'\n_paragram =  '/kaggle/input/quora-insincere-questions-classification/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n_wiki_news = '/kaggle/input/quora-insincere-questions-classification/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n_google_news = '/kaggle/input/quora-insincere-questions-classification/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n\nembeddings = [\n              {'name': 'glove', 'embeddings_path': _glove},\n              {'name': 'paragram', 'embeddings_path': _paragram}#,\n              #{'name': 'fasttext', 'embeddings_path': _wiki_news},\n              #{'name': 'googlenews', 'embeddings_path': _google_news}\n                ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Definition of functions to load and analyse embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Functions for lemmatization from http://textmining.wp.hs-hannover.de/Preprocessing.html\n\ndef wntag(pttag):\n    if pttag in ['JJ', 'JJR', 'JJS']:\n        return wn.ADJ\n    elif pttag in ['NN', 'NNS', 'NNP', 'NNPS']:\n        return wn.NOUN\n    elif pttag in ['RB', 'RBR', 'RBS']:\n        return wn.ADV\n    elif pttag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n        return wn.VERB\n    return None\n\ndef lemmatize(lemmatizer,word,pos):\n    if pos == None:\n        return word\n    else:\n        return lemmatizer.lemmatize(word,pos)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to create embedding matrix\nembedding_matrices = {}\nwords_in_embedding = {}\ndef create_model_embedding_matrix(embeddings_name,word_index,max_words, embeddings_dict):\n\n    embedding_dim = 300 # (vector size 300!)\n    embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n    unknown_words_list = []\n    num_known_words = 0  \n        \n    ps = PorterStemmer()\n    ps_counter = 0\n    lc = LancasterStemmer()\n    lc_counter = 0\n    sb = SnowballStemmer(\"english\")\n    sb_counter = 0\n    lemma_counter = 0\n\n                # Filling up matrix\n    for word, i in word_index.items(): \n\n                if embeddings_name in ['glove', 'paragram', 'fasttext']:\n\n                        embedding_vector = embeddings_dict.get(word) # get vector for word from embedding \n                        if embedding_vector is not None:\n                            embedding_matrix[i] = embedding_vector\n                            num_known_words +=1\n                            continue # if embedding found - process next word\n\n                        word_c = word.lower()\n                        embedding_vector = embeddings_dict.get(word_c)\n                        if embedding_vector is not None:\n                            embedding_matrix[i] = embedding_vector\n                            num_known_words +=1\n                            continue # if embedding found - process next word\n\n                        word_c = word.capitalize()\n                        embedding_vector = embeddings_dict.get(word_c)\n                        if embedding_vector is not None:\n                            embedding_matrix[i] = embedding_vector\n                            num_known_words +=1\n                            continue # if embedding found - process next word\n\n                        word_c = word.upper()\n                        embedding_vector = embeddings_dict.get(word_c)\n                        if embedding_vector is not None:\n                            embedding_matrix[i] = embedding_vector\n                            num_known_words +=1\n                            continue # if embedding found - process next word\n\n                        word_c = ps.stem(word)\n                        embedding_vector = embeddings_dict.get(word_c)\n                        if embedding_vector is not None:\n                            embedding_matrix[i] = embedding_vector\n                            num_known_words +=1\n                            ps_counter +=1\n                            continue # if embedding found - process next word\n\n                        word_c = lc.stem(word)\n                        embedding_vector = embeddings_dict.get(word_c)\n                        if embedding_vector is not None:\n                            embedding_matrix[i] = embedding_vector\n                            num_known_words +=1\n                            lc_counter +=1\n                            continue # if embedding found - process next word\n\n                        word_c = sb.stem(word)\n                        embedding_vector = embeddings_dict.get(word_c)\n                        if embedding_vector is not None:\n                            embedding_matrix[i] = embedding_vector\n                            num_known_words +=1\n                            sb_counter +=1\n                            continue # if embedding found - process next word\n\n                        word_c = lemmatize(lemmatizer,pos_tag([word])[0][0],wntag(pos_tag([word])[0][1]))\n                        embedding_vector = embeddings_dict.get(word_c)\n                        if embedding_vector is not None:\n                            embedding_matrix[i] = embedding_vector\n                            num_known_words +=1\n                            lemma_counter +=1\n                            continue # if embedding found - process next word\n\n                        else:\n                            unknown_words_list.append(word)\n\n                if embeddings_name == 'googlenews':\n\n                        try:\n                            word_c = word\n                            embedding_vector = embeddings_dict[word_c]  \n                            if embedding_vector is not None:\n                                embedding_matrix[i] = embedding_vector\n                                num_known_words +=1\n                                continue # if embedding found - process next word\n\n                            word_c = word.lower()\n                            embedding_vector = embeddings_dict[word_c]  \n                            if embedding_vector is not None:\n                                embedding_matrix[i] = embedding_vector\n                                num_known_words +=1\n                                continue # if embedding found - process next word\n\n                            word_c = word.capitalize()\n                            embedding_vector = embeddings_dict[word_c]\n                            if embedding_vector is not None:\n                                embedding_matrix[i] = embedding_vector\n                                num_known_words +=1 \n                                continue # if embedding found - process next word\n\n                            word_c = word.upper()\n                            embedding_vector = embeddings_dict[word_c]   \n                            if embedding_vector is not None:\n                                embedding_matrix[i] = embedding_vector\n                                num_known_words +=1\n                                continue # if embedding found - process next word\n\n                            word_c = ps.stem(word)\n                            embedding_vector = embeddings_dict[word_c]  \n                            if embedding_vector is not None:\n                                embedding_matrix[i] = embedding_vector\n                                num_known_words +=1\n                                ps_counter +=1\n                                continue # if embedding found - process next word\n\n                            word_c = lc.stem(word)\n                            embedding_vector = embeddings_dict[word_c] \n                            if embedding_vector is not None:\n                                embedding_matrix[i] = embedding_vector\n                                num_known_words +=1\n                                lc_counter +=1\n                                continue # if embedding found - process next word\n\n                            word_c = sb.stem(word)\n                            embedding_vector = embeddings_dict[word_c] \n                            if embedding_vector is not None:\n                                embedding_matrix[i] = embedding_vector\n                                num_known_words +=1\n                                sb_counter +=1\n                                continue # if embedding found - process next word\n\n                            word_c = lemmatize(lemmatizer,pos_tag([word])[0][0],wntag(pos_tag([word])[0][1]))\n                            embedding_vector = embeddings_dict[word_c] \n                            if embedding_vector is not None:\n                                embedding_matrix[i] = embedding_vector\n                                num_known_words +=1\n                                lemma_counter +=1\n                                continue # if embedding found - process next word\n\n                        except:\n                            unknown_words_list.append(word)\n\n    try: \n        words_in_embedding[embeddings_name] = list(embeddings_dict.keys())\n    except:\n        try:\n            words_in_embedding[embeddings_name] = list(embeddings_dict.wv.vocab)\n        except:\n            print(\"Error during generation of key list {}\".format(embeddings_name))\n            print(sys.exc_info()[0])\n\n    print('  Embeddings_matrix created')\n    print('  Shape embedding_matrix: {}'.format(embedding_matrix.shape))\n    print('  Found Embeddings for {:.2f}% of all words'\n          .format((num_known_words / len(word_index))*100))\n    print(\"  num_known_words :\", num_known_words)\n    print(\"  num words in word_index: \", len(word_index))\n    print('  Unknown Words: {:.2f}%'.\n          format(((len(unknown_words_list)) / len(word_index))*100))\n    print(\"  Words found by PorterStemmer: {}\".format(ps_counter))\n    print(\"  Words found by LancasterStemmer: {}\".format(lc_counter))\n    print(\"  Words found by SnowballStemmer: {}\".format(sb_counter))\n    print(\"  Words found by Lemmatisation: {}\".format(lemma_counter))\n\n    # Top 50 unknown words\n    print(\"  Top 50 unknown words:\\n {}\\n\".format(unknown_words_list[:50]))\n\n    del num_known_words, unknown_words_list,ps,lc,sb, ps_counter, lc_counter, sb_counter\n    del lemma_counter; gc.collect()\n    \n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to load + analyze Embeddings\ndef load_and_analyse_Embeddings(embeddings_name, embeddings_path, max_words):\n    \n    if embeddings_name in ['glove', 'paragram', 'fasttext']:  \n        embeddings_dict = {} # create empty embedding dictionary\n        embedding_file = open(embeddings_path, encoding =\"utf8\", errors = 'ignore') # load embedding from path\n\n        # Fill embedding dict with word: vector(coefs) pairs\n        for line in embedding_file:\n            line_values = line.split(' ') # read in values of respective line (= vector)\n            word = line_values[0] #  # first value in line represents the word\n            coefs = np.asarray(line_values[1:], dtype='float32') # all values represent vector\n            embeddings_dict[word] = coefs # add key(word), value(vector) pairs to dict\n\n        embedding_file.close() \n        \n        os.system('echo '+ embeddings_name + 'loaded')\n        print('  ',embeddings_name, 'loaded')\n        print('  {} word vectors within {} dict'.format(len(embeddings_dict),embeddings_name))\n        \n        # Use pre-trained embedding to create final embeddings matrix\n        embedding_matrix = create_model_embedding_matrix(embeddings_name,word_index,max_words, embeddings_dict)\n        del embeddings_dict, line_values,word,coefs\n                \n    if embeddings_name == 'googlenews':\n        embeddings_file = KeyedVectors.load_word2vec_format(embeddings_path, binary=True)\n        \n        os.system('echo '+ embeddings_name + 'loaded')\n        print('  ',embeddings_name, 'loaded')\n        \n        # Use pre-trained embedding to create final embeddings matrix\n        embedding_matrix = create_model_embedding_matrix(embeddings_name,word_index,max_words, embeddings_file)\n        del embeddings_file\n        \n    # MEMORY MANAGEMENT!\n    gc.collect()\n    \n    return embedding_matrix","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preparation (3)  - Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"#### Definition mapping and functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \n                       \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \n                       \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \n                       \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n                       \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n                       \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n                       \"I'm\": \"I am\",\"i'm\": \"i am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n                       \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n                       \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n                       \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \n                       \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \n                       \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \n                       \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n                       \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n                       \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n                       \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n                       \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n                       \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n                       \"weren't\": \"were not\",\"what`s\": \"what is\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n                       \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n                       \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n                       \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \n                       \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n                       \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n                       \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n                       \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n                       \"you're\": \"you are\", \"you've\": \"you have\"}\n\n# dict from https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2 \ncorrect_spell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite',\n                    'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater',\n                    'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization',\n                    'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n                    'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What',\n                    'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are',\n                    'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many',\n                    'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best',\n                    'howdoes': 'how does', 'mastrubation': 'masturbation',\n                    'mastrubate': 'masturbate', \"mastrubating\": 'masturbating',\n                    \"mcdonald's\":'mcdonalds',\n                    'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist',\n                    'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', \n                    'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what',\n                    'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n                    'demonitization': 'demonetization', 'demonetisation': 'demonetization',\n                    'pokémon': 'pokemon', 'quoras': 'quora', 'quorans': 'quora'}\n\n# Kernel \"fork-embeddings-keras-v04\"\nspecials_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \n                 \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', \n                 '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', \n                 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '\\u200b': ' ',\n                 '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': '', 'ε−': ''}\n\npunct = \"/-?!.,#$%\\()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&' + '\\''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(x):\n    x = str(x)\n    x = re.sub('[’‘´`]', \"'\", x) \n    \n    # replaces one digit by #, two following digits by ## etc.\n    x = re.sub('[0-9]{5,}', '#####', str(x)) \n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    x = re.sub('[0-9]\\'[0-9]', 'feet inches', x) # e.g. 5'5 → feet inches\n    \n    for word in x.split():\n        if word.lower() in contraction_mapping.keys():\n            x = x.replace(word, contraction_mapping[word.lower()])\n        if word in correct_spell_dict.keys():\n            x = x.replace(word, correct_spell_dict[word])\n        if word in specials_mapping.keys():\n            x = x.replace(word, specials_mapping[word])\n        if word[0] in punct and len(word) != 1: # remove punctuation directly in front of word\n            x = x.replace(word[0], '') \n        \n    x = ' '.join(word_tokenize(x)) # separates puncutation from words\n               \n    return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Apply preprocessing functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"runtime_dict = {}\nstart_prep = time.time()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"os.system('echo Applying preprocessing functions..')\ndf[\"question_text\"] = df[\"question_text\"].fillna(\" \").apply(lambda x: preprocessing(x))\nos.system('echo prepocessing done')\n\nX = df.loc[:, 'question_text'].values\ny = np.asarray(df.loc[:, 'target'].values)\n\npadded_seq, word_index = my_tokenizer(X) # Tokenization\nos.system('echo Tokenization completed')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"end_prep = time.time()\nduration_data_prep = end_prep - start_prep\nruntime_dict['Data Preparation'] = round(duration_data_prep)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Iteration loop to compare different embeddings (3)"},{"metadata":{"trusted":true},"cell_type":"code","source":"for embedding in embeddings:\n    emb_name = embedding['name']\n    emb_path = embedding['embeddings_path']\n    print(\"Running procedure on {}:\".format(emb_name))\n    \n    # loading embedding\n    embedding_matrix = load_and_analyse_Embeddings(emb_name, emb_path, max_words) \n    print(embedding_matrix.shape)\n    print(type(embedding_matrix))\n\n    # average embeddings \n    if embedding['name'] == 'glove':\n            avg_embedding = np.multiply(0.7, embedding_matrix)\n    if embedding['name'] == 'paragram':\n            avg_embedding += np.multiply(0.3, embedding_matrix)\n    del embedding_matrix; gc.collect()\n\nos.system(\"echo embedding created\")\nprint(avg_embedding.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"end_embeddings= time.time()\nduration_embeddings = end_embeddings - end_prep\nprint(duration_embeddings)\nruntime_dict['Embeddings'] = round(duration_embeddings)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"max_words","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_embedding.shape[1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"avg_embedding","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_keras_model():\n    model = Sequential()\n    model.add(Embedding(input_dim = max_words+1, # 10k different words/integers\n                        output_dim = avg_embedding.shape[1], \n                        weights = [avg_embedding],\n                        trainable = False)) \n    \n    model.add(SpatialDropout1D(0.3))\n    #model.add(Bidirectional(CuDNNLSTM(32, return_sequences=True)))\n    model.add(Bidirectional(CuDNNLSTM(32)))\n    model.add(Dense(1, activation='sigmoid')) # final -  binary classifier\n    \n    model.compile(optimizer='rmsprop',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n        \n    return model\n\nget_keras_model().summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomMetric(Callback):\n\n    # Create Instance at the beginning of each epoch\n    def on_train_begin(self, logs={}):\n        self.best_val_f1s = [] # collects best f1 after each epoch with best threshold\n    \n    # Function called at the end of ery epoch\n    def on_epoch_end(self, epoch, logs={}):\n        t0 = time.time()\n        \n        val_predict = np.asarray(self.model.predict(self.validation_data[0])) \n        val_target = self.validation_data[1]\n        \n        # Find best threshold for prediction\n        best_f1 = 0               \n        for threshold in np.arange(0.2,0.401, 0.01):\n            val_f1 = f1_score(y_true = val_target, y_pred = val_predict > threshold)\n            if val_f1 > best_f1:\n                best_f1 = val_f1\n\n        self.best_val_f1s.append(best_f1)\n        \n        t1 = time.time()\n        if epoch % 2 == 0:\n            print(\"  -- epoch: {}\".format(epoch))  \n            print(\"Execution time on_epoch_end {}\".format(t1-t0))\n            os.system(\"echo  -- epoch: {}\".format(epoch))\n            os.system(\"echo Execution time on_epoch_end {}\".format(t1-t0))\n        return","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"my_y_val_preds = {} # dictionary to collect model predictions at the end of each split\nmy_y_val_targets = {} # dictionary of true classes at the end of each split\nmy_history_dict = {} # to collect accuracy of each epoch\nmy_metrics_dict = {} # to collect best f1 of each epoch\nbest_f1_dict = {} # final evaluation at the end of training (after last epoch)\nbest_threshold_dict = {} # final evaluation at the end of training (after last epoch)\n\n# fold_list contains train and validation indices (folds) for each split\n# fold_list contains train and validation indices (folds) for each split\nfolds = StratifiedShuffleSplit(n_splits=kfolds, test_size=0.2, random_state=123).split(padded_seq, y)\n\ntf.logging.set_verbosity(tf.logging.ERROR) # dont show warnings (e.g. tensorflow version problems)\nfor i, (train_indices, val_indices) in enumerate(folds):\n    start_fold = time.time()\n    \n    print('\\nSplit: {}  \\n '.format(i))\n    os.system(\"echo running split {}\".format(i))\n    X_train, X_val = padded_seq[train_indices], padded_seq[val_indices] \n    y_train, y_val = y[train_indices], y[val_indices] \n\n    model = get_keras_model() # create new model for current split\n    my_metrics = CustomMetric() # create new metrics instance\n \n    # Training process is logged in history object for visualisation purposes\n    # within each split setting the model is trained several epochs (complete fit)\n    history = model.fit(X_train, y_train,\n                        epochs = model_epochs, \n                        batch_size= 512,\n                        verbose = 0, \n                        validation_data=(X_val, y_val),\n                        callbacks = [my_metrics])\n    \n    ############## at the end of each training process: ##################\n    \n    my_history_dict[i] = history\n    my_metrics_dict[i] = my_metrics\n        \n    y_val_pred = model.predict(X_val) # prediction on \n    my_y_val_preds[i] = y_val_pred \n    my_y_val_targets[i] = y_val\n    \n    # Find best threshold for prediction\n    best_f1 = 0\n    best_threshold = 0\n    for threshold in np.arange(0.1,0.5, 0.01):\n        # calucate f1 score for allowed thresholds\n        f1_score_threshold = f1_score(y_true = y_val ,\n                                              y_pred = y_val_pred > threshold) # 0 or 1\n        if f1_score_threshold > best_f1:\n            best_f1 = f1_score_threshold\n            best_threshold = threshold\n            best_f1_dict[i] = best_f1\n            best_threshold_dict[i] = best_threshold\n            \n    stop_fold = time.time()\n    print(\"Execution time Fold {}: {}.\".format(i, (stop_fold - start_fold)))\n    os.system(\"echo Execution time Fold {}: {}.\".format(i, (stop_fold - start_fold)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"thresh_avg = 0\nthresh_sum = 0\nf1_avg = 0\nf1_sum = 0\n\nfor key, value in best_f1_dict.items():\n    print(\"Split: {} : Best F1 score: {:6.4f} reached with a threshold of {:6.4f}\"\n          .format(key, best_f1_dict[key], best_threshold_dict[key]))\n    thresh_sum += best_threshold_dict[key] \n    thresh_avg = thresh_sum/kfolds\n    f1_sum += best_f1_dict[key] \n    f1_avg = f1_sum/kfolds\n   \nprint(\"\")\nprint(\"Threshold for prediction: {:6.4f}\".format(thresh_avg))\nprint(\"Average F1-Score: {:5.3f}\".format(f1_avg))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in my_history_dict:\n    \n    print('Split: {} '.format(i))\n    loss = my_history_dict[i].history['loss']\n    val_loss = my_history_dict[i].history['val_loss']\n    epochs = np.arange(1, len(loss) +1, 1) # x-axis\n\n    # Plotting:\n    f = plt.figure(figsize=(10,3))\n    # plots loss\n    f.add_subplot(1, 2, 1) # number of rows, number of columns,subplot you're currently on.\n    plt.xticks(epochs)\n    plt.plot(epochs, loss, 'bo', label='Loss Training', color = 'black')\n    plt.plot(epochs, val_loss, 'b', label='Loss Validation')\n    plt.title('Loss function Training / Validation')\n    plt.legend()\n \n    # plots f1 score\n    f.add_subplot(1, 2, 2)\n    best_val_f1s = my_metrics_dict[i].best_val_f1s\n   # plt.figure()\n    plt.xticks(epochs)\n    plt.plot(epochs, best_val_f1s, 'b', label='F1 Validation')\n    plt.title('F1 Validation')\n    plt.legend()\n    \n    #plt.subplots_adjust(wspace=0.30) # width reserved for blank space between subplots\n    f.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_and_eval= time.time()\nduration_train_and_eval = train_and_eval - end_embeddings\nprint(duration_train_and_eval)\nruntime_dict['Modeling'] = round(duration_train_and_eval)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Runtime Evaluation"},{"metadata":{"trusted":true},"cell_type":"code","source":"import plotly.graph_objects as go\nimport pandas as pd\n\ntop_labels = ['Data Preparation', 'Embeddings', 'Modeling']\n\ncolors = ['rgba(38, 24, 74, 0.8)', 'rgba(71, 58, 131, 0.8)','rgba(122, 120, 168, 0.8)']\n\nx_data = []\nfor phase, dur in runtime_dict.items():\n    x_data.append(dur)\nx_data = [x_data]\n\ny_data = ['Runtime [s]']\n\nfig = go.Figure()\n\nfor i in range(0, len(x_data[0])):\n    for xd, yd in zip(x_data, y_data):\n        fig.add_trace(go.Bar(\n            x=[xd[i]], y=[yd],\n            orientation='h',\n            width = 0.4, # width of bars\n            marker=dict(\n                color=colors[i],\n                line=dict(color='rgb(248, 248, 249)')\n            )\n        ))\n\nfig.update_layout(\n    xaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=True,\n        zeroline=False,\n        domain=[0.15, 1]\n    ),\n    yaxis=dict(\n        showgrid=False,\n        showline=False,\n        showticklabels=False,\n        zeroline=False,\n    ),\n    barmode='stack',\n    paper_bgcolor='rgb(248, 248, 255)',\n    plot_bgcolor='rgb(248, 248, 255)',\n    margin=dict(l=120, r=10, t=140, b=40),\n    showlegend=False,\n)\n\nannotations = []\n\nfor yd, xd in zip(y_data, x_data):\n    # labeling the y-axis\n    annotations.append(dict(xref='paper', yref='y',\n                            x=0.14, y=yd,\n                            xanchor='right',\n                            text=str(yd),\n                            font=dict(family='Arial', size=14,\n                                      color='rgb(67, 67, 67)'),\n                            showarrow=False, align='right'))\n    # labeling the first percentage of each bar (x_axis)\n    annotations.append(dict(xref='x', yref='y',\n                            x=xd[0] / 2, y=yd,\n                            text=str(xd[0]),\n                            font=dict(family='Arial', size=14,\n                                      color='rgb(248, 248, 255)'),\n                            showarrow=False))\n    # labeling the first Likert scale (on the top)\n    if yd == y_data[-1]:\n        annotations.append(dict(xref='x', yref='paper',\n                                x=xd[0] / 2, y=1.1,\n                                text=top_labels[0],\n                                font=dict(family='Arial', size=14,\n                                          color='rgb(67, 67, 67)'),\n                                showarrow=False))\n    space = xd[0]\n    for i in range(1, len(xd)):\n            # labeling the rest of percentages for each bar (x_axis)\n            annotations.append(dict(xref='x', yref='y',\n                                    x=space + (xd[i]/2), y=yd,\n                                    text=str(xd[i]),\n                                    font=dict(family='Arial', size=14,\n                                              color='rgb(248, 248, 255)'),\n                                    showarrow=False))\n            # labeling the Likert scale\n            if yd == y_data[-1]:\n                annotations.append(dict(xref='x', yref='paper',\n                                        x=space + (xd[i]/2), y=1.1,\n                                        text=top_labels[i],\n                                        font=dict(family='Arial', size=14,\n                                                  color='rgb(67, 67, 67)'),\n                                        showarrow=False))\n            space += xd[i]\n\nfig.update_layout(annotations=annotations)\n\nfig.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}