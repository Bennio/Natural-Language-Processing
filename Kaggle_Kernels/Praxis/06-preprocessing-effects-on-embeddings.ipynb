{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Purpose of this kernel\n",
    "This kernel wants to take a closer look on Preprcessing. In contrast to the kernels 01-05 here the effect of different preprocessing steps on the Embeddings will be examined.\n",
    "\n",
    "The kernels 01-05 gave the following insights and raised the following questions:\n",
    "* Does it make sense to limit the number of words the model is trained with? So far all kernel models only were allowed to use maximum 10k words. Since generally neuronal networks benefit from increasing data volumes this barrier is removed here.\n",
    "\n",
    "* The maxlen hyperparameter was arbitrary set to 100. Different maxlens shall be examined here.\n",
    "\n",
    "* As discussed here: https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings **target of this kernel is to get the vocabulary as close to the embeddings as possible**. Obviously, as shown in kernel 01 the amount of data is not enough to derive better Embeddings manually than using a pre-trained embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "# General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# Preprocessing\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "stop = set(stopwords.words('english'))\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# Modeling\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "# Training\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "  # splits train-set into into train and validation folds\n",
    "    \n",
    "# Evaluation\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast Run Testing\n",
    "#total_train_samples = 15000 # max is 1306122\n",
    "#total_test_samples = 2000 # max is 375806\n",
    "total_train_samples = 1306122 # max is 1306122\n",
    "total_test_samples = 375806 # max is 375806\n",
    "\n",
    "# Preprocessing\n",
    "maxlen = 130 # 130 covers about 75% of all bad questions completely\n",
    "\n",
    "# Modeling\n",
    "embedding_dim = 300 # set to 300 to be able to compare with pre-trained embeddings\n",
    "\n",
    "# Training\n",
    "kfolds = 3\n",
    "model_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../input/train.csv\")\n",
    "str_ = 'Train data loaded'\n",
    "os.system('echo '+str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape for this run:  1306122 3\n",
      "Shape data tensor: (1306122,)\n",
      "Shape target tensor: (1306122,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province as a nation in the 1960s?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you encourage people to adopt and not shop?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity affect space geometry?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  ...   target\n",
       "0  00002165364db923c7e6  ...        0\n",
       "1  000032939017120e6e44  ...        0\n",
       "2  0000412ca6e4628ce2cf  ...        0\n",
       "\n",
       "[3 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[:total_train_samples] # for Testing purposes\n",
    "num_samples,n = df.shape\n",
    "print(\"Shape for this run: \", num_samples, n)\n",
    "\n",
    "X = df.loc[:, 'question_text'].values\n",
    "y = df.loc[:, 'target'].values\n",
    "\n",
    "# Since Neural Networks are only able to perform transformations on tensors \n",
    "y = np.asarray(y) # Transformation target labels to numpy array \n",
    "\n",
    "print('Shape data tensor:', X.shape) \n",
    "print('Shape target tensor:', y.shape) # 1D Tensor\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1500) # inrease display column size\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inspecting negative questions to derive a good hyperparameter maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>443216</th>\n",
       "      <td>56da6b6875d686b48fde</td>\n",
       "      <td>What is [math]\\frac{\\int_{1x^5}^{3x^{-5}} \\tan(\\tan({\\boxed{\\int_{1x^0}^{1x^2} \\sum_{\\varpi=1}^{\\infty} \\int_{2x^{-3}}^{2x^2} \\sum_{\\alpha=7}^{\\infty} \\underbrace{\\sqrt[2]{1x^5}}_{\\text{Gauss's Law of Theoretical Probability.}} d\\tau dx}}^{1x^0})) d\\mu}{\\int_{2x^{-3}}^{1x^5} \\cos(\\int_{2x^2}^{1x^{-3}} \\frac{\\sqrt[2]{\\overbrace{\\underbrace{\\frac{3x^3+3x^5}{\\sqrt[3]{2x^{-3}}}}_{\\text{Gauss's Law of Theoretical Probability.}} \\times \\overbrace{\\tan(2x^0)}^{\\text{Gauss's Law of Theoretical Probability.}}-\\sum_{4=7}^{\\infty} \\boxed{3x^{-5}}}^{\\text{Inverse Function.}}}}{{\\boxed{\\int_{2x^2}^{2x^4} 3x^1 d9} \\div \\sum_{6=6}^{\\infty} \\sqrt[3]{2x^2}+\\sqrt[4]{\\sin(2x^0+3x^0)}}^{2x^{-4}}+\\boxed{\\frac{\\vec{\\boxed{\\sum_{\\gamma=10}^{\\infty} 1x^{-5}}}}{\\frac{\\sum_{\\iota=2}^{\\infty} 1x^{-5}-\\frac{3x^{-1}}{1x^{-4}}}{\\sin(\\tan(3x^{-2}))}}}} \\times \\boxed{\\sqrt[2]{{{{\\sqrt[5]{2x^5}}^{2x^{-1}}}^{2x^{-1}} \\div \\sum_{\\chi=6}^{\\infty} \\int_{1x^4}^{2x^{-4}} 3x^2 d\\vartheta+{2x^{-3}}^{2x^{-5}}}^{3x^{-4}}}} d\\mu) d\\iota}[/math]?</td>\n",
       "      <td>1</td>\n",
       "      <td>1017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163583</th>\n",
       "      <td>1ffca149bd0a19cd714c</td>\n",
       "      <td>What is [math]\\overbrace{\\sum_{\\vartheta=8}^{\\infty} \\vec{\\frac{\\sum_{\\kappa=7}^{\\infty} \\overbrace{1x^0}^{\\text{Read carefully.}}-3x^{-1} \\div 1x^5+{\\sqrt[3]{2x^{-3}}}^{1x^0}+\\vec{\\vec{{3x^{-3}}^{1x^{-2}}}}}{\\sum_{\\dagger=9}^{\\infty} \\vec{\\boxed{\\boxed{3x^{-1}}+3x^1 \\times 1x^{-5}}}}} \\div \\sin(\\boxed{\\boxed{\\vec{3x^{-5}}}+\\sqrt[4]{2x^{-4}}+\\vec{2x^{-3}} \\div \\sin(\\sqrt[5]{\\int_{1x^5}^{2x^5} 2x^{-3} d\\varrho}) \\times \\vec{{\\underbrace{2x^1}_{\\text{Prove This.}}}^{3x^4} \\div \\sqrt[5]{2x^{-3}}+\\sum_{\\theta=8}^{\\infty} 1x^4}}) \\times {\\boxed{\\vec{\\sum_{\\nu=8}^{\\infty} \\sum_{4=6}^{\\infty} \\sum_{\\xi=9}^{\\infty} \\boxed{3x^1}-\\boxed{\\sqrt[3]{\\sqrt[3]{2x^{-2}}}}}}}^{1x^3}-\\cos({{\\tan(\\sum_{0=6}^{\\infty} \\tan(\\overbrace{\\frac{\\boxed{1x^1}-\\sqrt[3]{3x^{-2}}}{\\sum_{\\eta=10}^{\\infty} 1x^{-3} \\div 1x^1}}^{\\text{Molar Quantity.}}))}^{1x^3}}^{1x^{-4}})}^{\\text{Expanded.}}[/math]?</td>\n",
       "      <td>1</td>\n",
       "      <td>878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165040</th>\n",
       "      <td>2041ae71c5a8c0cba026</td>\n",
       "      <td>To you, does being a Christian mean  inviting in the spirit of Jesus into you and suppressing our own spirit? 'Thy will not mine' and all that? Do you like living as a zombie of someone else's spirit - however perfect it may be? Don't you want to experience and improve your own will and spirit and live your life as you, not Jesus?</td>\n",
       "      <td>1</td>\n",
       "      <td>332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>315732</th>\n",
       "      <td>3de13bd2110379eada25</td>\n",
       "      <td>[math]\\frac{\\text{d}x}{\\text{d}t}=Ax , {\\text{d}x} x^{-1}=A {\\text{d}t} [/math] , [math]{\\text{now taking transpose on both sides}}[/math] [math] \\int_{C}^{x} x^{-T}{\\text{d}x^{T}}=\\int_{0}^{t}A {\\text{d}t} {\\text{ solution becomes }} {\\text x =(e^{t^{T}A^{T}})^{T} C}, {\\text {but actual answer } x = (e^{At}) C, how?} [/math]</td>\n",
       "      <td>1</td>\n",
       "      <td>327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1221556</th>\n",
       "      <td>ef6ae86505a114e2e85f</td>\n",
       "      <td>Whereas Hillary Clinton's illegal, private email server was configured by some moonlighting State Department underling, and whereas that underling has taken the fifth, and whereas that underling's lawyer is from one of the most powerful and expensive law firms in DC., should someone with ordinary intelligence smell a rat?</td>\n",
       "      <td>1</td>\n",
       "      <td>323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>327381</th>\n",
       "      <td>402ce4678756ee1c6f4a</td>\n",
       "      <td>Which of the options listed below would most accurately fit [math](f ∘ g)(x),f(x) = \\frac{x}{x+4}, g(x) = \\frac{7}{x}[/math]? —A.[math](f ∘ g)(x) = \\frac{7x+28}{x}[/math] —B.[math](f ∘ g)(x) = \\frac{7x+4}{x}[/math] —C.[math](f ∘ g)(x) = \\frac{7}{7+4x}[/math] —D.[math](f ∘ g)(x) = \\frac{7x+28}{11}[/math]</td>\n",
       "      <td>1</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1215721</th>\n",
       "      <td>ee431b31ca30ebbbc256</td>\n",
       "      <td>How do you show that [math]\\displaystyle\\sum_{m\\geq 3}\\left[4\\binom{1/2}{m}+\\binom{1/2}{m-1}\\right]\\frac{1}{\\Gamma\\left(m-\\tfrac{3}{2}\\right)}\\int\\limits_{0}^{\\infty}\\frac{x^{m-3/2}}{e^x-1}\\,dx=\\int\\limits_{0}^{\\infty}\\frac{3 e^{-x} \\left(2-2 e^x+x+e^x x\\right)}{2 \\sqrt{\\pi } x^{5/2}(e^x-1)}\\,dx[/math]?</td>\n",
       "      <td>1</td>\n",
       "      <td>304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841016</th>\n",
       "      <td>a4d47a71254401cedbd5</td>\n",
       "      <td>How do I evaluate [math]\\displaystyle\\lim _{ h\\rightarrow \\infty }{ \\left( \\sum _{ i=1 }^{ i=h }{ \\frac { \\left( \\left( \\sum _{ j=1 }^{ j=i }{ \\frac { \\left( 2j+1 \\right) ! }{ \\prod _{ k=1 }^{ k=j }{ \\left( 2k-1 \\right) } } } \\right) +2 \\right) }{ { \\left( (i+1)! \\right)}^{ 2 } } } \\right) }[/math]?</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1094891</th>\n",
       "      <td>d694ae9a3db89fb8630c</td>\n",
       "      <td>Occasionally, when we were young my sister watched porn and would ask to suck my dick... from then on... as and when she wanted... she would do it and I felt really good. Recently, she wanted me to suck her breast..it felt good she has beautiful breasts. I wanna ask if it's ok to fuck her?</td>\n",
       "      <td>1</td>\n",
       "      <td>290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23277</th>\n",
       "      <td>04902e91065f7d4a8677</td>\n",
       "      <td>Dieudonne is a wildly popular Moroccan-French comedian whose material is shockingly anti-Semitic such as mocking concentration camp victims. His diatribe is feeding a growing wave of hatred and violence against Jews in Europe not seen since WWII Should he be free to foment race hatred?</td>\n",
       "      <td>1</td>\n",
       "      <td>286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          qid  ...   length\n",
       "443216   56da6b6875d686b48fde  ...     1017\n",
       "163583   1ffca149bd0a19cd714c  ...      878\n",
       "165040   2041ae71c5a8c0cba026  ...      332\n",
       "315732   3de13bd2110379eada25  ...      327\n",
       "1221556  ef6ae86505a114e2e85f  ...      323\n",
       "327381   402ce4678756ee1c6f4a  ...      304\n",
       "1215721  ee431b31ca30ebbbc256  ...      304\n",
       "841016   a4d47a71254401cedbd5  ...      300\n",
       "1094891  d694ae9a3db89fb8630c  ...      290\n",
       "23277    04902e91065f7d4a8677  ...      286\n",
       "\n",
       "[10 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('mode.chained_assignment', None)\n",
    "bad = df[df.target == 1] \n",
    "bad['length'] = bad['question_text'].apply(len)\n",
    "bad_length = bad.sort_values(by = 'length', axis = 0, ascending = False)\n",
    "bad_length.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f29356b09b0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.boxplot(bad_length.length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>80810.0</td>\n",
       "      <td>80810.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.0</td>\n",
       "      <td>98.064163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.0</td>\n",
       "      <td>55.186227</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>55.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>86.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.0</td>\n",
       "      <td>130.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1017.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        target        length\n",
       "count  80810.0  80810.000000\n",
       "mean       1.0     98.064163\n",
       "std        0.0     55.186227\n",
       "min        1.0      1.000000\n",
       "25%        1.0     55.000000\n",
       "50%        1.0     86.000000\n",
       "75%        1.0    130.000000\n",
       "max        1.0   1017.000000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bad.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f2948ebb978>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bad_length.length[bad_length.length <=300].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del bad, bad_length\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (1)  - tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 222161 unique tokens\n",
      "Top 5 most frequent words: ['the', 'what', 'is', 'a', 'to']\n"
     ]
    }
   ],
   "source": [
    "def my_tokenizer(texts):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(texts) \n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "        padded_seq = pad_sequences(sequences, maxlen=maxlen)  \n",
    "        word_index = tokenizer.word_index  \n",
    "        \n",
    "        return padded_seq, word_index\n",
    "    \n",
    "# Apply tokenization on whole dataset\n",
    "padded_seq, word_index = my_tokenizer(X)\n",
    "os.system('echo Tokenization completed')\n",
    "print(\"Found {} unique tokens\".format(len(word_index)))\n",
    "print(\"Top 5 most frequent words: {}\".format(sorted(word_index, key=word_index.get)[:5])) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (2)  - Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings path\n",
    "_glove = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "_paragram =  '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "_wiki_news = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "_google_news = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "embeddings = [{'name': 'glove', 'embeddings_path': _glove},\n",
    "              {'name': 'paragram', 'embeddings_path': _paragram},\n",
    "              {'name': 'fasttext', 'embeddings_path': _wiki_news},\n",
    "              {'name': 'googlenews', 'embeddings_path': _google_news}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of functions to load and analyse embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create embedding matrix\n",
    "def create_model_embedding_matrix(embeddings_name,word_index,embeddings_dict):\n",
    "\n",
    "    embedding_dim = 300 # (vector size 300!)\n",
    "    embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
    "    known_words_list = []\n",
    "    unknown_words_list = []\n",
    "\n",
    "    # Filling up matrix\n",
    "    for word, i in word_index.items(): \n",
    "        \n",
    "        if embeddings_name in ['glove', 'paragram', 'fasttext']:\n",
    "            embedding_vector = embeddings_dict.get(word) # get vector for word from embedding \n",
    "            if embedding_vector is not None:\n",
    "                known_words_list.append(word)\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "            else:\n",
    "                unknown_words_list.append(word)\n",
    "                \n",
    "        if embeddings_name == 'googlenews':\n",
    "            try:\n",
    "                embedding_vector = embeddings_dict[word]  \n",
    "                known_words_list.append(word)\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "            except:\n",
    "                unknown_words_list.append(word)\n",
    "            \n",
    "\n",
    "\n",
    "    print('  Embeddings_matrix created')\n",
    "    print('    Shape embedding_matrix: {}'.format(embedding_matrix.shape))\n",
    "    print('  Found Embeddings for {:.2f}% of all words'\n",
    "          .format((len(known_words_list) / len(word_index))*100))\n",
    "    print('  Unknown Words: {:.2f}%'.format((len(unknown_words_list) / len(word_index))*100)) \n",
    "    print(\"{}\\n\".format(unknown_words_list[:30])) # Top 10 unknown words\n",
    "    del known_words_list, unknown_words_list; gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load + analyze Embeddings\n",
    "def load_and_analyse_Embeddings(embeddings_name, embeddings_path):\n",
    "    \n",
    "    if embeddings_name in ['glove', 'paragram', 'fasttext']:  \n",
    "        embeddings_dict = {} # create empty embedding dictionary\n",
    "        embedding_file = open(embeddings_path, encoding =\"utf8\", errors = 'ignore') # load embedding from path\n",
    "\n",
    "        # Fill embedding dict with word: vector(coefs) pairs\n",
    "        for line in embedding_file:\n",
    "            line_values = line.split(' ') # read in values of respective line (= vector)\n",
    "            word = line_values[0] #  # first value in line represents the word\n",
    "            coefs = np.asarray(line_values[1:], dtype='float32') # all values represent vector\n",
    "            embeddings_dict[word] = coefs # add key(word), value(vector) pairs to dict\n",
    "\n",
    "        embedding_file.close() \n",
    "        \n",
    "        os.system('echo '+ embeddings_name + 'loaded')\n",
    "        print('  ',embeddings_name, 'loaded')\n",
    "        print('  {} word vectors within {} dict'.format(len(embeddings_dict),embeddings_name))\n",
    "        \n",
    "        # Use pre-trained embedding to create final embeddings matrix\n",
    "        create_model_embedding_matrix(embeddings_name,word_index,embeddings_dict)\n",
    "        del embeddings_dict, line_values,word,coefs\n",
    "                \n",
    "    if embeddings_name == 'googlenews':\n",
    "        embeddings_file = KeyedVectors.load_word2vec_format(embeddings_path, binary=True)\n",
    "        \n",
    "        os.system('echo '+ embeddings_name + 'loaded')\n",
    "        print('  ',embeddings_name, 'loaded')\n",
    "        \n",
    "        # Use pre-trained embedding to create final embeddings matrix\n",
    "        create_model_embedding_matrix(embeddings_name,word_index,embeddings_file)\n",
    "        del embeddings_file\n",
    "        \n",
    "    # MEMORY MANAGEMENT!\n",
    "    del embeddings_name, embeddings_path\n",
    "    gc.collect()\n",
    "    \n",
    "   # return embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration loop to compare different embeddings (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running procedure on glove:\n",
      "   glove loaded\n",
      "  2196016 word vectors within glove dict\n",
      "  Embeddings_matrix created\n",
      "    Shape embedding_matrix: (222162, 300)\n",
      "  Found Embeddings for 55.50% of all words\n",
      "  Unknown Words: 44.50%\n",
      "[\"what's\", \"i'm\", \"isn't\", \"i've\", 'i’m', \"you've\", 'don’t', \"aren't\", 'what’s', \"won't\", \"trump's\", \"they're\", \"haven't\", \"shouldn't\", \"he's\", 'it’s', 'can’t', \"wouldn't\", 'quorans', \"who's\", 'doesn’t', \"today's\", \"someone's\", \"there's\", \"people's\", \"wasn't\", \"one's\", \"hasn't\", \"couldn't\", \"india's\"]\n",
      "\n",
      "Running procedure on paragram:\n",
      "   paragram loaded\n",
      "  1703755 word vectors within paragram dict\n",
      "  Embeddings_matrix created\n",
      "    Shape embedding_matrix: (222162, 300)\n",
      "  Found Embeddings for 64.70% of all words\n",
      "  Unknown Words: 35.30%\n",
      "[\"what's\", \"isn't\", 'i’m', \"you've\", 'don’t', \"aren't\", 'what’s', \"won't\", \"trump's\", \"they're\", \"haven't\", \"shouldn't\", \"he's\", 'it’s', 'can’t', \"wouldn't\", 'quorans', \"who's\", 'doesn’t', \"today's\", \"someone's\", \"there's\", \"people's\", \"wasn't\", \"one's\", \"hasn't\", \"couldn't\", \"india's\", \"she's\", 'i’ve']\n",
      "\n",
      "Running procedure on fasttext:\n",
      "   fasttext loaded\n",
      "  999995 word vectors within fasttext dict\n",
      "  Embeddings_matrix created\n",
      "    Shape embedding_matrix: (222162, 300)\n",
      "  Found Embeddings for 42.63% of all words\n",
      "  Unknown Words: 57.37%\n",
      "[\"don't\", \"what's\", \"i'm\", \"can't\", \"doesn't\", \"it's\", \"isn't\", \"didn't\", \"i've\", 'i’m', \"you've\", 'don’t', 'upsc', \"aren't\", \"you're\", 'what’s', \"won't\", \"trump's\", \"they're\", \"haven't\", \"shouldn't\", \"he's\", 'it’s', 'can’t', 'aiims', \"wouldn't\", 'cgl', \"that's\", 'quorans', \"who's\"]\n",
      "\n",
      "Running procedure on googlenews:\n",
      "   googlenews loaded\n",
      "  Embeddings_matrix created\n",
      "    Shape embedding_matrix: (222162, 300)\n",
      "  Found Embeddings for 34.44% of all words\n",
      "  Unknown Words: 65.56%\n",
      "['a', 'to', 'of', 'and', 'quora', \"i'm\", '2017', '2018', '10', '12', '100', '20', \"i've\", '15', 'i’m', '12th', 'instagram', '11', 'don’t', '30', '50', 'upsc', '18', '000', 'bitcoin', 'what’s', '16', '14', \"trump's\", 'mbbs']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for embedding in embeddings:\n",
    "    emb_name = embedding['name']\n",
    "    emb_path = embedding['embeddings_path']\n",
    "    print(\"Running procedure on {}:\".format(emb_name))\n",
    "    \n",
    "    load_and_analyse_Embeddings(emb_name, emb_path) # loading embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Glove, Paragram and Fasttext seem to have problems with auxiliary verbs and common online speech\n",
    "      * mapping dict to dissolve these words this necessary.\n",
    "An appropriate dictionary was found in this kernel: https://www.kaggle.com/noexittv/embeddings-keras-v04 \n",
    "* As already discovered in this kernel: https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings obviously stop words were removed before GoogleNews Embedding was trained. For this reason the embedding does not provide any dense vectors. Moreover there seems to be a problem with numbers\n",
    "      * clean_numbers() function from mentioned kernel is used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (3)  - Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition mapping and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "auxiliary_verbs_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \n",
    "                       \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \n",
    "                       \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \n",
    "                       \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n",
    "                       \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n",
    "                       \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n",
    "                       \"I'm\": \"I am\",\"i'm\": \"i am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
    "                       \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                       \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n",
    "                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n",
    "                       \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \n",
    "                       \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \n",
    "                       \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \n",
    "                       \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n",
    "                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                       \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n",
    "                       \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
    "                       \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                       \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n",
    "                       \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
    "                       \"weren't\": \"were not\",\"what`s\": \"what is\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
    "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                       \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                       \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                       \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \n",
    "                       \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "                       \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
    "                       \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n",
    "                       \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n",
    "                       \"you're\": \"you are\", \"you've\": \"you have\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def dissolve_verbs(x):\n",
    "    x = str(x)\n",
    "    x = x.lower()\n",
    "    for word in x.split():\n",
    "        x = re.sub('[’’]', \"'\", x)\n",
    "        if word in auxiliary_verbs_mapping.keys():\n",
    "            x = re.sub(word, auxiliary_verbs_mapping.get(word),x)\n",
    "    return x\n",
    "\n",
    "def clean_numbers(x):\n",
    "\n",
    "    x = re.sub('[0-9]{5,}', '#####', x) # matches normal character class: replace numbers range 0-9 \n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"question_text\"] = df[\"question_text\"].apply(lambda x: clean_numbers(x))\n",
    "df[\"question_text\"] = df[\"question_text\"].apply(lambda x: dissolve_verbs(x))\n",
    "\n",
    "X = df.loc[:, 'question_text'].values\n",
    "y = np.asarray(df.loc[:, 'target'].values)\n",
    "\n",
    "padded_seq, word_index = my_tokenizer(X) # Tokenization\n",
    "os.system('echo Tokenization 2 completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration loop to compare different embeddings (2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running procedure on glove:\n",
      "   glove loaded\n",
      "  2196016 word vectors within glove dict\n",
      "  Embeddings_matrix created\n",
      "    Shape embedding_matrix: (209498, 300)\n",
      "  Found Embeddings for 55.88% of all words\n",
      "  Unknown Words: 44.12%\n",
      "[\"i'm\", \"what's\", \"trump's\", \"someone's\", \"today's\", 'quorans', \"people's\", \"one's\", \"india's\", \"women's\", \"isn't\", \"i've\", \"world's\", \"master's\", 'brexit', \"person's\", \"you've\", 'cryptocurrencies', \"china's\", \"men's\", \"earth's\", \"country's\", 'redmi', \"aren't\", \"america's\", 'kvpy', 'paytm', \"bachelor's\", 'iiser', 'ethereum']\n",
      "\n",
      "Running procedure on paragram:\n",
      "   paragram loaded\n",
      "  1703755 word vectors within paragram dict\n",
      "  Embeddings_matrix created\n",
      "    Shape embedding_matrix: (209498, 300)\n",
      "  Found Embeddings for 65.37% of all words\n",
      "  Unknown Words: 34.63%\n",
      "[\"what's\", \"trump's\", \"someone's\", \"today's\", 'quorans', \"people's\", \"one's\", \"india's\", \"women's\", \"isn't\", \"world's\", \"master's\", 'brexit', \"person's\", \"you've\", 'cryptocurrencies', \"china's\", \"men's\", \"earth's\", \"country's\", 'redmi', \"aren't\", \"america's\", \"bachelor's\", \"man's\", \"obama's\", \"he's\", \"friend's\", \"won't\", \"god's\"]\n",
      "\n",
      "Running procedure on fasttext:\n",
      "   fasttext loaded\n",
      "  999995 word vectors within fasttext dict\n",
      "  Embeddings_matrix created\n",
      "    Shape embedding_matrix: (209498, 300)\n",
      "  Found Embeddings for 43.17% of all words\n",
      "  Unknown Words: 56.83%\n",
      "[\"i'm\", \"don't\", 'upsc', \"what's\", \"trump's\", \"it's\", \"can't\", 'aiims', \"doesn't\", 'cgl', \"someone's\", \"today's\", 'quorans', \"people's\", \"one's\", 'jio', 'manipal', \"india's\", 'icse', \"women's\", \"isn't\", \"i've\", \"didn't\", 'bitsat', 'cgpa', 'iiit', 'ielts', \"world's\", \"master's\", \"person's\"]\n",
      "\n",
      "Running procedure on googlenews:\n",
      "   googlenews loaded\n",
      "  Embeddings_matrix created\n",
      "    Shape embedding_matrix: (209498, 300)\n",
      "  Found Embeddings for 36.54% of all words\n",
      "  Unknown Words: 63.46%\n",
      "['a', 'to', 'of', 'and', 'quora', \"i'm\", 'instagram', 'upsc', 'bitcoin', \"trump's\", 'mbbs', 'whatsapp', 'favourite', \"'s\", 'ece', 'aiims', 'centre', 'colour', 'sbi', 'cgl', 'iim', \"someone's\", 'cryptocurrency', 'quorans', 'btech', 'obc', 'snapchat', 'jio', 'manipal', 'bba']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for embedding in embeddings:\n",
    "    emb_name = embedding['name']\n",
    "    emb_path = embedding['embeddings_path']\n",
    "    print(\"Running procedure on {}:\".format(emb_name))\n",
    "    \n",
    "    load_and_analyse_Embeddings(emb_name, emb_path) # loading embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
