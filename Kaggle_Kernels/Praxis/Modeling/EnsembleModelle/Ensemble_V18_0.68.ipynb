{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Purpose of this kernel\n",
    "\n",
    "- Train 2 Models with embedding concatenation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-stdout\n",
    "\n",
    "# General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "import time\n",
    "\n",
    "# Preprocessing\n",
    "import seaborn as sns\n",
    "import re\n",
    "from re import *\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer \n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "# Modeling\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "from keras.layers import SimpleRNN, GRU, Bidirectional, LSTM,CuDNNLSTM, CuDNNGRU\n",
    "from keras.layers import SpatialDropout1D, Dropout\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# Training\n",
    "# from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# Evaluation\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU on?  -  True\n",
      "Available GPUs:  /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "# Check status and availability of GPU\n",
    "import tensorflow as tf\n",
    "print(\"GPU on?  - \", tf.test.is_gpu_available())\n",
    "print(\"Available GPUs: \", tf.test.gpu_device_name())\n",
    "\n",
    "# confirm Keras sees the GPU\n",
    "from keras import backend\n",
    "assert len(backend.tensorflow_backend._get_available_gpus()) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/quora-insincere-questions-classification/sample_submission.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/train.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/test.csv\n",
      "/kaggle/input/quora-insincere-questions-classification/embeddings/paragram_300_sl999/paragram_300_sl999.txt\n",
      "/kaggle/input/quora-insincere-questions-classification/embeddings/paragram_300_sl999/README.txt\n",
      "/kaggle/input/quora-insincere-questions-classification/embeddings/glove.840B.300d/glove.840B.300d.txt\n",
      "/kaggle/input/quora-insincere-questions-classification/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec\n",
      "/kaggle/input/quora-insincere-questions-classification/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin\n"
     ]
    }
   ],
   "source": [
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast Run Testing\n",
    "#total_train_samples = 5000 # max is 1306122\n",
    "total_train_samples = 1306122 # max is 1306122\n",
    "\n",
    "# Preprocessing\n",
    "maxlen = 130 # 130 - covers about 75% of all bad questions completely\n",
    "max_words = 9999999 # if all words shall be used, type huge number here\n",
    "\n",
    "# Training\n",
    "kfolds = 2 # 80/20 split\n",
    "model_epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"/kaggle/input/quora-insincere-questions-classification/train.csv\")\n",
    "str_ = 'Train data loaded'\n",
    "os.system('echo '+str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape for this run:  1306122 3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province as a nation in the 1960s?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you encourage people to adopt and not shop?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity affect space geometry?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  \\\n",
       "0  00002165364db923c7e6   \n",
       "1  000032939017120e6e44   \n",
       "2  0000412ca6e4628ce2cf   \n",
       "\n",
       "                                                                       question_text  \\\n",
       "0           How did Quebec nationalists see their province as a nation in the 1960s?   \n",
       "1  Do you have an adopted dog, how would you encourage people to adopt and not shop?   \n",
       "2                Why does velocity affect time? Does velocity affect space geometry?   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[:total_train_samples] # for Testing purposes\n",
    "num_samples,n = df.shape\n",
    "print(\"Shape for this run: \", num_samples, n)\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1500) # inrease display column size\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (1)  - tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(texts):\n",
    "        tokenizer = Tokenizer() \n",
    "        tokenizer.fit_on_texts(texts) \n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "        padded_seq = pad_sequences(sequences, maxlen=maxlen)  \n",
    "        word_index = tokenizer.word_index  \n",
    "        \n",
    "        return padded_seq, word_index, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of functions to load and analyse embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for lemmatization from http://textmining.wp.hs-hannover.de/Preprocessing.html\n",
    "\n",
    "def wntag(pttag):\n",
    "    if pttag in ['JJ', 'JJR', 'JJS']:\n",
    "        return wn.ADJ\n",
    "    elif pttag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        return wn.NOUN\n",
    "    elif pttag in ['RB', 'RBR', 'RBS']:\n",
    "        return wn.ADV\n",
    "    elif pttag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def lemmatize(lemmatizer,word,pos):\n",
    "    if pos == None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(word,pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create embedding matrix\n",
    "embedding_matrices = {}\n",
    "words_in_embedding = {}\n",
    "def create_model_embedding_matrix(embeddings_name,word_index,max_words, embeddings_dict):\n",
    "\n",
    "    embedding_dim = 300 # (vector size 300!)\n",
    "    embedding_matrix = np.zeros((max_words+1, embedding_dim))\n",
    "    unknown_words_list = []\n",
    "    num_known_words = 0  \n",
    "        \n",
    "    ps = PorterStemmer()\n",
    "    ps_counter = 0\n",
    "    lc = LancasterStemmer()\n",
    "    lc_counter = 0\n",
    "    sb = SnowballStemmer(\"english\")\n",
    "    sb_counter = 0\n",
    "    lemma_counter = 0\n",
    "\n",
    "    # Filling up matrix\n",
    "    for word, i in word_index.items(): \n",
    "        \n",
    "        if embeddings_name in ['glove', 'paragram', 'fasttext'] and i <= max_words:\n",
    "            \n",
    "            embedding_vector = embeddings_dict.get(word) # get vector for word from embedding \n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                continue # if embedding found - process next word\n",
    "                \n",
    "            word_c = word.lower()\n",
    "            embedding_vector = embeddings_dict.get(word_c)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                continue # if embedding found - process next word\n",
    "                \n",
    "            word_c = word.capitalize()\n",
    "            embedding_vector = embeddings_dict.get(word_c)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                continue # if embedding found - process next word\n",
    "                \n",
    "            word_c = word.upper()\n",
    "            embedding_vector = embeddings_dict.get(word_c)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                continue # if embedding found - process next word\n",
    "                \n",
    "            word_c = ps.stem(word)\n",
    "            embedding_vector = embeddings_dict.get(word_c)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                ps_counter +=1\n",
    "                continue # if embedding found - process next word\n",
    "                \n",
    "            word_c = lc.stem(word)\n",
    "            embedding_vector = embeddings_dict.get(word_c)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                lc_counter +=1\n",
    "                continue # if embedding found - process next word\n",
    "                \n",
    "            word_c = sb.stem(word)\n",
    "            embedding_vector = embeddings_dict.get(word_c)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                sb_counter +=1\n",
    "                continue # if embedding found - process next word\n",
    "                \n",
    "            word_c = lemmatize(lemmatizer,pos_tag([word])[0][0],wntag(pos_tag([word])[0][1]))\n",
    "            embedding_vector = embeddings_dict.get(word_c)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                lemma_counter +=1\n",
    "                continue # if embedding found - process next word\n",
    "                \n",
    "            else:\n",
    "                unknown_words_list.append(word)\n",
    "                \n",
    "        if embeddings_name == 'googlenews' and i <= max_words:\n",
    "            \n",
    "            try:\n",
    "                word_c = word\n",
    "                embedding_vector = embeddings_dict[word_c]  \n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1\n",
    "                    continue # if embedding found - process next word\n",
    "\n",
    "                word_c = word.lower()\n",
    "                embedding_vector = embeddings_dict[word_c]  \n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1\n",
    "                    continue # if embedding found - process next word\n",
    "                \n",
    "                word_c = word.capitalize()\n",
    "                embedding_vector = embeddings_dict[word_c]\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1 \n",
    "                    continue # if embedding found - process next word\n",
    "                \n",
    "                word_c = word.upper()\n",
    "                embedding_vector = embeddings_dict[word_c]   \n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1\n",
    "                    continue # if embedding found - process next word\n",
    "                    \n",
    "                word_c = ps.stem(word)\n",
    "                embedding_vector = embeddings_dict[word_c]  \n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1\n",
    "                    ps_counter +=1\n",
    "                    continue # if embedding found - process next word\n",
    "                    \n",
    "                word_c = lc.stem(word)\n",
    "                embedding_vector = embeddings_dict[word_c] \n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1\n",
    "                    lc_counter +=1\n",
    "                    continue # if embedding found - process next word\n",
    "                    \n",
    "                word_c = sb.stem(word)\n",
    "                embedding_vector = embeddings_dict[word_c] \n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1\n",
    "                    sb_counter +=1\n",
    "                    continue # if embedding found - process next word\n",
    "                    \n",
    "                word_c = lemmatize(lemmatizer,pos_tag([word])[0][0],wntag(pos_tag([word])[0][1]))\n",
    "                embedding_vector = embeddings_dict[word_c] \n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1\n",
    "                    lemma_counter +=1\n",
    "                    continue # if embedding found - process next word\n",
    "                    \n",
    "            except:\n",
    "                unknown_words_list.append(word)\n",
    "                \n",
    "    try: \n",
    "        words_in_embedding[embeddings_name] = list(embeddings_dict.keys())\n",
    "    except:\n",
    "        try:\n",
    "            words_in_embedding[embeddings_name] = list(embeddings_dict.wv.vocab)\n",
    "        except:\n",
    "            print(\"Error during generation of key list {}\".format(embeddings_name))\n",
    "            print(sys.exc_info()[0])\n",
    "    \n",
    "   \n",
    "    print('  Embeddings_matrix created')\n",
    "    print('  Shape embedding_matrix: {}'.format(embedding_matrix.shape))\n",
    "    print('  Found Embeddings for {:.2f}% of all words'\n",
    "          .format((num_known_words / max_words)*100))\n",
    "    print(\"  num_known_words :\", num_known_words)\n",
    "    print(\"  num words in word_index: \", max_words)\n",
    "    print('  Unknown Words: {:.2f}%'.\n",
    "          format(((len(unknown_words_list)) / max_words)*100))\n",
    "    print(\"  Words found by PorterStemmer: {}\".format(ps_counter))\n",
    "    print(\"  Words found by LancasterStemmer: {}\".format(lc_counter))\n",
    "    print(\"  Words found by SnowballStemmer: {}\".format(sb_counter))\n",
    "    print(\"  Words found by Lemmatisation: {}\".format(lemma_counter))\n",
    "          \n",
    "    # Top 50 unknown words\n",
    "    print(\"  Top 50 unknown words:\\n {}\\n\".format(unknown_words_list[:50]))\n",
    "    \n",
    "    del num_known_words, unknown_words_list,ps,lc,sb, ps_counter, lc_counter, sb_counter\n",
    "    del lemma_counter; gc.collect()\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load + analyze Embeddings\n",
    "def load_and_analyse_Embeddings(embeddings_name, embeddings_path, max_words):\n",
    "    \n",
    "    if embeddings_name in ['glove', 'paragram', 'fasttext']:  \n",
    "        embeddings_dict = {} # create empty embedding dictionary\n",
    "        embedding_file = open(embeddings_path, encoding =\"utf8\", errors = 'ignore') # load embedding from path\n",
    "\n",
    "        # Fill embedding dict with word: vector(coefs) pairs\n",
    "        for line in embedding_file:\n",
    "            line_values = line.split(' ') # read in values of respective line (= vector)\n",
    "            word = line_values[0] #  # first value in line represents the word\n",
    "            coefs = np.asarray(line_values[1:], dtype='float32') # all values represent vector\n",
    "            embeddings_dict[word] = coefs # add key(word), value(vector) pairs to dict\n",
    "\n",
    "        embedding_file.close() \n",
    "        \n",
    "        os.system('echo '+ embeddings_name + 'loaded')\n",
    "        print('  ',embeddings_name, 'loaded')\n",
    "        print('  {} word vectors within {} dict'.format(len(embeddings_dict),embeddings_name))\n",
    "        \n",
    "        # Use pre-trained embedding to create final embeddings matrix\n",
    "        embedding_matrix = create_model_embedding_matrix(embeddings_name,word_index,max_words, embeddings_dict)\n",
    "        del embeddings_dict, line_values,word,coefs\n",
    "                \n",
    "    if embeddings_name == 'googlenews':\n",
    "        embeddings_file = KeyedVectors.load_word2vec_format(embeddings_path, binary=True)\n",
    "        \n",
    "        os.system('echo '+ embeddings_name + 'loaded')\n",
    "        print('  ',embeddings_name, 'loaded')\n",
    "        \n",
    "        # Use pre-trained embedding to create final embeddings matrix\n",
    "        embedding_matrix = create_model_embedding_matrix(embeddings_name,word_index,max_words, embeddings_file)\n",
    "        del embeddings_file\n",
    "        \n",
    "    # MEMORY MANAGEMENT!\n",
    "    del embeddings_name, embeddings_path\n",
    "    gc.collect()\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_embeddings(conc_embedding, embedding_matrix):\n",
    "    \n",
    "    if conc_embedding is not None:\n",
    "        conc_embedding = np.concatenate((conc_embedding,embedding_matrix), axis = 1 )\n",
    "        print(\"Added embedding. New shape: {}\".format(conc_embedding.shape))\n",
    "    else:\n",
    "        conc_embedding = embedding_matrix\n",
    "        print(\"Added embedding. First shape: {}\".format(conc_embedding.shape))\n",
    "        \n",
    "    del embedding_matrix; gc.collect()\n",
    "    return conc_embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (3)  - Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition mapping and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \n",
    "                       \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \n",
    "                       \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \n",
    "                       \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n",
    "                       \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n",
    "                       \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n",
    "                       \"I'm\": \"I am\",\"i'm\": \"i am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
    "                       \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                       \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n",
    "                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n",
    "                       \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \n",
    "                       \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \n",
    "                       \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \n",
    "                       \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n",
    "                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                       \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n",
    "                       \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
    "                       \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                       \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n",
    "                       \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
    "                       \"weren't\": \"were not\",\"what`s\": \"what is\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
    "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                       \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                       \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                       \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \n",
    "                       \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "                       \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
    "                       \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n",
    "                       \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n",
    "                       \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "# dict from https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2 \n",
    "correct_spell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite',\n",
    "                    'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater',\n",
    "                    'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization',\n",
    "                    'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n",
    "                    'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What',\n",
    "                    'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are',\n",
    "                    'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many',\n",
    "                    'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best',\n",
    "                    'howdoes': 'how does', 'mastrubation': 'masturbation',\n",
    "                    'mastrubate': 'masturbate', \"mastrubating\": 'masturbating',\n",
    "                    \"mcdonald's\":'mcdonalds',\n",
    "                    'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist',\n",
    "                    'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', \n",
    "                    'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what',\n",
    "                    'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n",
    "                    'demonitization': 'demonetization', 'demonetisation': 'demonetization',\n",
    "                    'pokémon': 'pokemon', 'quoras': 'quora', 'quorans': 'quora'}\n",
    "\n",
    "# Kernel \"fork-embeddings-keras-v04\"\n",
    "specials_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \n",
    "                 \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', \n",
    "                 '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', \n",
    "                 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '\\u200b': ' ',\n",
    "                 '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': '', 'ε−': ''}\n",
    "\n",
    "punct = \"/-?!.,#$%\\()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&' + '\\''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(x):\n",
    "    x = str(x)\n",
    "    x = re.sub('[’‘´`]', \"'\", x) \n",
    "    \n",
    "    # replaces one digit by #, two following digits by ## etc.\n",
    "    x = re.sub('[0-9]{5,}', '#####', str(x)) \n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    x = re.sub('[0-9]\\'[0-9]', 'feet inches', x) # e.g. 5'5 → feet inches\n",
    "    \n",
    "    for word in x.split():\n",
    "        if word.lower() in contraction_mapping.keys():\n",
    "            x = x.replace(word, contraction_mapping[word.lower()])\n",
    "        if word in correct_spell_dict.keys():\n",
    "            x = x.replace(word, correct_spell_dict[word])\n",
    "        if word in specials_mapping.keys():\n",
    "            x = x.replace(word, specials_mapping[word])\n",
    "        if word[0] in punct and len(word) != 1: # remove punctuation directly in front of word\n",
    "            x = x.replace(word[0], '') \n",
    "        \n",
    "    x = ' '.join(word_tokenize(x)) # separates puncutation from words\n",
    "               \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime_dict = {}\n",
    "start_prep = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Number of words in word_index:  189036\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('echo Applying preprocessing functions..')\n",
    "df[\"question_text\"] = df[\"question_text\"].fillna(\" \").apply(lambda x: preprocessing(x))\n",
    "os.system('echo prepocessing done')\n",
    "\n",
    "X = df.loc[:, 'question_text'].values\n",
    "y = np.asarray(df.loc[:, 'target'].values)\n",
    "\n",
    "padded_seq, word_index, tokenizer = my_tokenizer(X) # translate text to numerical values\n",
    "max_words = min(max_words, len(word_index)) # e.g.10k words or all words\n",
    "      # index +1 because fill process of matrix starts at 1\n",
    "print(\" Number of words in word_index: \", len(word_index))\n",
    "\n",
    "del X; gc.collect()\n",
    "os.system('echo Tokenization completed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_prep = time.time()\n",
    "duration_data_prep = end_prep - start_prep\n",
    "runtime_dict['Data Preparation'] = round(duration_data_prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "padded_seq, y = shuffle(padded_seq, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration loop to compare different embeddings (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings path\n",
    "_glove = '/kaggle/input/quora-insincere-questions-classification/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "_paragram =  '/kaggle/input/quora-insincere-questions-classification/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "_wiki_news = '/kaggle/input/quora-insincere-questions-classification/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "_google_news = '/kaggle/input/quora-insincere-questions-classification/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "embeddings = [\n",
    "              {'name': 'glove', 'embeddings_path': _glove},\n",
    "               {'name': 'paragram', 'embeddings_path': _paragram},\n",
    "              {'name': 'fasttext', 'embeddings_path': _wiki_news} #, Fasttext\n",
    "              #{'name': 'googlenews', 'embeddings_path': _google_news}\n",
    "                ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keras_model(conc_embedding):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = max_words+1, # 10k different words/integers\n",
    "                        output_dim = conc_embedding.shape[1], \n",
    "                        weights = [conc_embedding],\n",
    "                        trainable = False)) \n",
    "    \n",
    "    model.add(SpatialDropout1D(0.3))\n",
    "    model.add(Bidirectional(CuDNNLSTM(32, return_sequences=True)))\n",
    "    model.add(Bidirectional(CuDNNLSTM(32)))\n",
    "    model.add(Dense(1, activation='sigmoid')) # final -  binary classifier\n",
    "    \n",
    "    model.compile(optimizer='rmsprop',\n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation of Glove and Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   glove loaded\n",
      "  2196016 word vectors within glove dict\n",
      "  Embeddings_matrix created\n",
      "  Shape embedding_matrix: (189037, 300)\n",
      "  Found Embeddings for 77.41% of all words\n",
      "  num_known_words : 146341\n",
      "  num words in word_index:  189036\n",
      "  Unknown Words: 22.59%\n",
      "  Words found by PorterStemmer: 3504\n",
      "  Words found by LancasterStemmer: 4721\n",
      "  Words found by SnowballStemmer: 87\n",
      "  Words found by Lemmatisation: 81\n",
      "  Top 50 unknown words:\n",
      " ['brexit', 'cryptocurrencies', 'redmi', 'coinbase', 'oneplus', 'upwork', 'machedo', 'gdpr', 'adityanath', 'boruto', 'bnbr', 'alshamsi', 'dceu', 'litecoin', 'unacademy', 'iiest', \"qur'an\", 'zerodha', 'tensorflow', 'doklam', 'kavalireddi', 'lnmiit', 'muoet', 'etc…', 'nicmar', 'vajiram', '°c', 'zebpay', 'srmjee', 'elitmus', 'altcoins', 'altcoin', 'hackerrank', 'awdhesh', 'ryzen', 'baahubali', 'koinex', 'demonetisation', 'mhcet', 'byju', 'srmjeee', 'sgsits', 'ftre', 'skripal', 'nanodegree', 'gurugram', 'hotstar', 'mhtcet', 'x²', 'bmsce']\n",
      "\n",
      "Added embedding. First shape: (189037, 300)\n",
      "   fasttext loaded\n",
      "  999995 word vectors within fasttext dict\n",
      "  Embeddings_matrix created\n",
      "  Shape embedding_matrix: (189037, 300)\n",
      "  Found Embeddings for 71.67% of all words\n",
      "  num_known_words : 135479\n",
      "  num words in word_index:  189036\n",
      "  Unknown Words: 28.33%\n",
      "  Words found by PorterStemmer: 3433\n",
      "  Words found by LancasterStemmer: 5013\n",
      "  Words found by SnowballStemmer: 79\n",
      "  Words found by Lemmatisation: 146\n",
      "  Top 50 unknown words:\n",
      " [\"''\", \"n't\", 'quorans', 'comedk', 'kvpy', 'quoran', 'wbjee', 'fortnite', 'marksheet', 'oneplus', 'uceed', 'bhakts', 'machedo', 'bnbr', 'chsl', 'naoh', 'josaa', 'alshamsi', 'devops', 'unacademy', 'sjws', 'aliexpress', \"qur'an\", 'zerodha', 'nmat', 'mongodb', 'tensorflow', 'intps', 'hairfall', 'jiit', 'doklam', 'kavalireddi', 'lnmiit', 'muoet', 'angularjs', 'etc…', 'woocommerce', \"d'angelo\", 'metoo', 'nicmar', 'vajiram', '°c', 'adhaar', 'modiji', 'zebpay', 'srmjee', 'elitmus', 'infjs', 'pubg', 'hackerrank']\n",
      "\n",
      "Added embedding. New shape: (189037, 600)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenation of GloVe and Fasttext\n",
    "conc_glove_fasttext = None\n",
    "\n",
    "os.system(\"echo Loading GloVe\")\n",
    "glove_embedding_matrix = load_and_analyse_Embeddings('glove', _glove, max_words) \n",
    "# concatenation embeddings \n",
    "conc_glove_fasttext = concatenate_embeddings(conc_glove_fasttext, glove_embedding_matrix)\n",
    "\n",
    "os.system(\"echo Loading Fasttext\")\n",
    "embedding_matrix = load_and_analyse_Embeddings('fasttext', _wiki_news, max_words) \n",
    "# concatenation embeddings \n",
    "conc_glove_fasttext = concatenate_embeddings(conc_glove_fasttext, embedding_matrix)\n",
    "del embedding_matrix; gc.collect()\n",
    "\n",
    "os.system(\"echo Concatenation GloVe and Fasttext created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "264.1496078968048\n"
     ]
    }
   ],
   "source": [
    "end_embeddings= time.time()\n",
    "duration_embeddings = end_embeddings - end_prep\n",
    "print(duration_embeddings)\n",
    "runtime_dict['Model 1 Embeddings'] = round(duration_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, None, 600)         113422200 \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, None, 600)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, None, 64)          162304    \n",
      "_________________________________________________________________\n",
      "bidirectional_2 (Bidirection (None, 64)                25088     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 113,609,657\n",
      "Trainable params: 187,457\n",
      "Non-trainable params: 113,422,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "get_keras_model(conc_glove_fasttext).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = get_keras_model(conc_glove_fasttext) # create new model for current split\n",
    "\n",
    "model_1.fit(padded_seq, y,\n",
    "            epochs = model_epochs, \n",
    "            batch_size= 512,\n",
    "            verbose = 0)     \n",
    "\n",
    "del conc_glove_fasttext; gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-718.5001041889191\n"
     ]
    }
   ],
   "source": [
    "end_model1= time.time()\n",
    "duration_model1_training = end_embeddings - end_model1\n",
    "print(duration_model1_training)\n",
    "runtime_dict['Model 1 Training'] = round(duration_model1_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenation of Glove and Paragram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   paragram loaded\n",
      "  1703755 word vectors within paragram dict\n",
      "  Embeddings_matrix created\n",
      "  Shape embedding_matrix: (189037, 300)\n",
      "  Found Embeddings for 78.83% of all words\n",
      "  num_known_words : 149009\n",
      "  num words in word_index:  189036\n",
      "  Unknown Words: 21.17%\n",
      "  Words found by PorterStemmer: 4308\n",
      "  Words found by LancasterStemmer: 5359\n",
      "  Words found by SnowballStemmer: 90\n",
      "  Words found by Lemmatisation: 79\n",
      "  Top 50 unknown words:\n",
      " ['brexit', 'cryptocurrencies', 'redmi', 'coinbase', 'oneplus', 'upwork', 'machedo', 'gdpr', 'adityanath', 'boruto', 'bnbr', 'alshamsi', 'dceu', 'litecoin', 'unacademy', 'iiest', \"qur'an\", 'zerodha', 'tensorflow', 'doklam', 'kavalireddi', 'lnmiit', 'muoet', 'etc…', 'nicmar', 'vajiram', '°c', 'zebpay', 'srmjee', 'elitmus', 'altcoins', 'altcoin', 'hackerrank', 'awdhesh', 'baahubali', 'koinex', 'demonetisation', 'mhcet', 'byju', 'srmjeee', 'sgsits', 'ftre', 'skripal', 'nanodegree', 'gurugram', 'hotstar', 'mhtcet', 'x²', 'bmsce', 'what\\u200b']\n",
      "\n",
      "Added embedding. New shape: (189037, 600)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system(\"echo Re-Use Glove Embedding Matrix\")\n",
    "conc_glove_paragram = glove_embedding_matrix\n",
    "\n",
    "os.system(\"echo Loading Paragram\")\n",
    "embedding_matrix = load_and_analyse_Embeddings('paragram', _paragram, max_words) \n",
    "# concatenation embeddings \n",
    "conc_glove_paragram = concatenate_embeddings(conc_glove_paragram, embedding_matrix)\n",
    "del embedding_matrix, glove_embedding_matrix; gc.collect()\n",
    "\n",
    "os.system(\"echo Concatenation GloVe and Paragram created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 600)         113422200 \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_3 (Spatial (None, None, 600)         0         \n",
      "_________________________________________________________________\n",
      "bidirectional_5 (Bidirection (None, None, 64)          162304    \n",
      "_________________________________________________________________\n",
      "bidirectional_6 (Bidirection (None, 64)                25088     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 113,609,657\n",
      "Trainable params: 187,457\n",
      "Non-trainable params: 113,422,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "get_keras_model(conc_glove_paragram).summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2 = get_keras_model(conc_glove_paragram) # create new model for current split\n",
    "\n",
    "model_2.fit(padded_seq, y,\n",
    "            epochs = model_epochs, \n",
    "            batch_size= 512,\n",
    "            verbose = 0)     \n",
    "\n",
    "del conc_glove_paragram; gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Ensemble and Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0000163e3ea7c7a74cd7</td>\n",
       "      <td>Why do so many women become so rude and arrogant when they get just a little bit of wealth and power?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>00002bd4fb5d505b9161</td>\n",
       "      <td>When should I apply for RV college of engineering and BMS college of engineering? Should I wait for the COMEDK result or am I supposed to apply before the result?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>00007756b4a147d2b0b3</td>\n",
       "      <td>What is it really like to be a nurse practitioner?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>000086e4b7e1c7146103</td>\n",
       "      <td>Who are entrepreneurs?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0000c4c3fbe8785a3090</td>\n",
       "      <td>Is education really making good people nowadays?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  \\\n",
       "0  0000163e3ea7c7a74cd7   \n",
       "1  00002bd4fb5d505b9161   \n",
       "2  00007756b4a147d2b0b3   \n",
       "3  000086e4b7e1c7146103   \n",
       "4  0000c4c3fbe8785a3090   \n",
       "\n",
       "                                                                                                                                                        question_text  \n",
       "0                                                               Why do so many women become so rude and arrogant when they get just a little bit of wealth and power?  \n",
       "1  When should I apply for RV college of engineering and BMS college of engineering? Should I wait for the COMEDK result or am I supposed to apply before the result?  \n",
       "2                                                                                                                  What is it really like to be a nurse practitioner?  \n",
       "3                                                                                                                                              Who are entrepreneurs?  \n",
       "4                                                                                                                    Is education really making good people nowadays?  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv(\"/kaggle/input/quora-insincere-questions-classification/test.csv\")\n",
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "os.system('echo applying preprocessing functions to testset..')\n",
    "test[\"question_text\"] = test[\"question_text\"].fillna(\" \").apply(lambda x: preprocessing(x))\n",
    "os.system('echo prepocessing testset done')\n",
    "\n",
    "X_test = test.loc[:, 'question_text'].values\n",
    "\n",
    "# Transformation questions to  sequence\n",
    "sequences = tokenizer.texts_to_sequences(X_test)\n",
    "padded_seq_test = pad_sequences(sequences, maxlen=maxlen)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375806/375806 [==============================] - 20s 54us/step\n",
      "375806/375806 [==============================] - 20s 54us/step\n",
      "Submission dataframe has the same length as test dataframe with shape:(375806, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0000163e3ea7c7a74cd7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>00002bd4fb5d505b9161</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>00007756b4a147d2b0b3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>000086e4b7e1c7146103</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0000c4c3fbe8785a3090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  prediction\n",
       "0  0000163e3ea7c7a74cd7           1\n",
       "1  00002bd4fb5d505b9161           0\n",
       "2  00007756b4a147d2b0b3           0\n",
       "3  000086e4b7e1c7146103           0\n",
       "4  0000c4c3fbe8785a3090           0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensemble Model\n",
    "preds_1 = model_1.predict(padded_seq_test, batch_size = 512, verbose = 1)\n",
    "preds_2 = model_2.predict(padded_seq_test, batch_size = 512, verbose = 1)\n",
    "final_preds = 0.5 * preds_1 + 0.5 * preds_2\n",
    "\n",
    "# Create a submission dataframe and append relevant columns\n",
    "submission = pd.DataFrame()\n",
    "submission['qid'] = test['qid'].values\n",
    "submission['prediction'] = (final_preds > 0.38).astype(int) # round sigmoid results to integers\n",
    "\n",
    "# Do test and my submission Dataframe have the same length?\n",
    "if len(submission) == len(test):\n",
    "    print(\"Submission dataframe has the same length as test dataframe with shape:{}\".format(submission.shape))\n",
    "else:\n",
    "    print(\"Something is wrong!\")\n",
    "    \n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submission as Output\n",
    "submission.to_csv('submission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {
     "16a7b0f1d6f742c882aad05623091c4e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "1f728f801c744ab595ebf72752b5ced5": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "363d600149ac4c968a11021436879bd8": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "3a79e0ad1040477bab16ff71753a29f6": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_45961449249c491591ec9cdedbb66734",
       "placeholder": "​",
       "style": "IPY_MODEL_40549984a90747adbee35000b8038f4d",
       "value": "100% 1306122/1306122 [03:33&lt;00:00, 6118.05it/s]"
      }
     },
     "40549984a90747adbee35000b8038f4d": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     },
     "45961449249c491591ec9cdedbb66734": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "4b7d0a559b3049c1b6428add65f2e527": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_c4a51bdd8ea949429502e4d6673c952a",
        "IPY_MODEL_3a79e0ad1040477bab16ff71753a29f6"
       ],
       "layout": "IPY_MODEL_c935cd1be9a64ad1b0b34bcbba50d95e"
      }
     },
     "58aa8c9e0b1949309b9eb474eedab1f4": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HTMLModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HTMLModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HTMLView",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_363d600149ac4c968a11021436879bd8",
       "placeholder": "​",
       "style": "IPY_MODEL_eb74fbd46e534438af6f51757d0822d3",
       "value": "0/|/| 0/? [00:00&lt;?, ?it/s]"
      }
     },
     "714a4965c0154db3bb6f4beb125ae739": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "85c932b4570f4a2ea7af8b123d0e9c69": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "HBoxModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "HBoxModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "HBoxView",
       "box_style": "",
       "children": [
        "IPY_MODEL_bba3a80bfaf444d7b5abd28ad60b3460",
        "IPY_MODEL_58aa8c9e0b1949309b9eb474eedab1f4"
       ],
       "layout": "IPY_MODEL_1f728f801c744ab595ebf72752b5ced5"
      }
     },
     "bba3a80bfaf444d7b5abd28ad60b3460": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_cae1b9b403054b14924940ca65a3a969",
       "max": 1,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_714a4965c0154db3bb6f4beb125ae739",
       "value": 1
      }
     },
     "c4a51bdd8ea949429502e4d6673c952a": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "IntProgressModel",
      "state": {
       "_dom_classes": [],
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "IntProgressModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/controls",
       "_view_module_version": "1.4.0",
       "_view_name": "ProgressView",
       "bar_style": "success",
       "description": "",
       "description_tooltip": null,
       "layout": "IPY_MODEL_16a7b0f1d6f742c882aad05623091c4e",
       "max": 1306122,
       "min": 0,
       "orientation": "horizontal",
       "style": "IPY_MODEL_ea5b3d6bbaee467a9600adbbf170b745",
       "value": 1306122
      }
     },
     "c935cd1be9a64ad1b0b34bcbba50d95e": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "cae1b9b403054b14924940ca65a3a969": {
      "model_module": "@jupyter-widgets/base",
      "model_module_version": "1.1.0",
      "model_name": "LayoutModel",
      "state": {
       "_model_module": "@jupyter-widgets/base",
       "_model_module_version": "1.1.0",
       "_model_name": "LayoutModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "LayoutView",
       "align_content": null,
       "align_items": null,
       "align_self": null,
       "border": null,
       "bottom": null,
       "display": null,
       "flex": null,
       "flex_flow": null,
       "grid_area": null,
       "grid_auto_columns": null,
       "grid_auto_flow": null,
       "grid_auto_rows": null,
       "grid_column": null,
       "grid_gap": null,
       "grid_row": null,
       "grid_template_areas": null,
       "grid_template_columns": null,
       "grid_template_rows": null,
       "height": null,
       "justify_content": null,
       "left": null,
       "margin": null,
       "max_height": null,
       "max_width": null,
       "min_height": null,
       "min_width": null,
       "order": null,
       "overflow": null,
       "overflow_x": null,
       "overflow_y": null,
       "padding": null,
       "right": null,
       "top": null,
       "visibility": null,
       "width": null
      }
     },
     "ea5b3d6bbaee467a9600adbbf170b745": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "ProgressStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "ProgressStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "bar_color": null,
       "description_width": ""
      }
     },
     "eb74fbd46e534438af6f51757d0822d3": {
      "model_module": "@jupyter-widgets/controls",
      "model_module_version": "1.4.0",
      "model_name": "DescriptionStyleModel",
      "state": {
       "_model_module": "@jupyter-widgets/controls",
       "_model_module_version": "1.4.0",
       "_model_name": "DescriptionStyleModel",
       "_view_count": null,
       "_view_module": "@jupyter-widgets/base",
       "_view_module_version": "1.1.0",
       "_view_name": "StyleView",
       "description_width": ""
      }
     }
    },
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
