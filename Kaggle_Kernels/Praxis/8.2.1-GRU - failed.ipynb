{"cells":[{"metadata":{},"cell_type":"markdown","source":"#### Purpose of this kernel\n\n- GRU model architecture with dropout=0.2, recurrent_dropout=0.2\n- increase of epochs to 10 "},{"metadata":{},"cell_type":"markdown","source":"#### Import libraries"},{"metadata":{"trusted":true},"cell_type":"code","source":"%%capture --no-stdout\n\n# General\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm, tqdm_notebook \ntqdm_notebook().pandas()\nimport os\nimport gc\nimport sys\nimport time\n\n# Preprocessing\nimport seaborn as sns\nimport re\nfrom re import *\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom gensim.models import KeyedVectors\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\nfrom nltk.stem import SnowballStemmer \nfrom nltk import pos_tag, word_tokenize\nfrom nltk.corpus import wordnet as wn\nlemmatizer = nltk.WordNetLemmatizer()\n\n# Modeling\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, Flatten, Dense, SimpleRNN, GRU\n\n# Training\nfrom sklearn.model_selection import StratifiedKFold\n  # splits train-set into into train and validation folds\n    \n# Evaluation\nfrom keras.callbacks import Callback\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check status and availability of GPU\nimport tensorflow as tf\nprint(\"GPU on?  - \", tf.test.is_gpu_available())\nprint(\"Available GPUs: \", tf.test.gpu_device_name())\n\n# confirm Keras sees the GPU\nfrom keras import backend\nassert len(backend.tensorflow_backend._get_available_gpus()) > 0","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Hyperparameters"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Fast Run Testing\n#total_train_samples = 100000 # max is 1306122\n#total_test_samples = 2000 # max is 375806\ntotal_train_samples = 1306122 # max is 1306122\ntotal_test_samples = 375806 # max is 375806\n\n# Preprocessing\nmaxlen = 130 # 130 - covers about 75% of all bad questions completely\nmax_words = 9999999 # if all words shall be used, type huge number here\n\n# Modeling\nembedding_dim = 300 # set to 300 to be able to compare with pre-trained embeddings\n\n# Training\nkfolds = 5 # 80/20 split\nmodel_epochs = 10","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"df = pd.read_csv(\"../input/train.csv\")\nstr_ = 'Train data loaded'\nos.system('echo '+str_)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = df[:total_train_samples] # for Testing purposes\nnum_samples,n = df.shape\nprint(\"Shape for this run: \", num_samples, n)\n\nX = df.loc[:, 'question_text'].values\ny = df.loc[:, 'target'].values\n\n# Since Neural Networks are only able to perform transformations on tensors \ny = np.asarray(y) # Transformation target labels to numpy array \n\nprint('Shape data tensor:', X.shape) \nprint('Shape target tensor:', y.shape) # 1D Tensor\n\npd.set_option('display.max_colwidth', 1500) # inrease display column size\ndf.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Preprocessing"},{"metadata":{},"cell_type":"markdown","source":"### Data Preparation (1)  - tokenization"},{"metadata":{"trusted":true},"cell_type":"code","source":"def my_tokenizer(texts):\n        tokenizer = Tokenizer() \n        tokenizer.fit_on_texts(texts) \n        sequences = tokenizer.texts_to_sequences(texts)\n        padded_seq = pad_sequences(sequences, maxlen=maxlen)  \n        word_index = tokenizer.word_index  \n        \n        return padded_seq, word_index","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preparation (2)  - Embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Embeddings path\n_glove = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n_paragram =  '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n_wiki_news = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n_google_news = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n\nembeddings = [\n                #{'name': 'glove', 'embeddings_path': _glove},\n              {'name': 'paragram', 'embeddings_path': _paragram}#,\n              #{'name': 'fasttext', 'embeddings_path': _wiki_news},\n              #{'name': 'googlenews', 'embeddings_path': _google_news}\n                ]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Definition of functions to load and analyse embeddings"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Functions for lemmatization from http://textmining.wp.hs-hannover.de/Preprocessing.html\n\ndef wntag(pttag):\n    if pttag in ['JJ', 'JJR', 'JJS']:\n        return wn.ADJ\n    elif pttag in ['NN', 'NNS', 'NNP', 'NNPS']:\n        return wn.NOUN\n    elif pttag in ['RB', 'RBR', 'RBS']:\n        return wn.ADV\n    elif pttag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n        return wn.VERB\n    return None\n\ndef lemmatize(lemmatizer,word,pos):\n    if pos == None:\n        return word\n    else:\n        return lemmatizer.lemmatize(word,pos)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to create embedding matrix\nembedding_matrices = {}\nwords_in_embedding = {}\ndef create_model_embedding_matrix(embeddings_name,word_index,max_words, embeddings_dict):\n\n    embedding_dim = 300 # (vector size 300!)\n    embedding_matrix = np.zeros((max_words+1, embedding_dim))\n    unknown_words_list = []\n    num_known_words = 0  \n        \n    ps = PorterStemmer()\n    ps_counter = 0\n    lc = LancasterStemmer()\n    lc_counter = 0\n    sb = SnowballStemmer(\"english\")\n    sb_counter = 0\n    lemma_counter = 0\n\n    # Filling up matrix\n    for word, i in word_index.items(): \n        \n        if embeddings_name in ['glove', 'paragram', 'fasttext'] and i <= max_words:\n            \n            embedding_vector = embeddings_dict.get(word) # get vector for word from embedding \n            if embedding_vector is not None:\n                embedding_matrix[i] = embedding_vector\n                num_known_words +=1\n                continue # if embedding found - process next word\n                \n            word_c = word.lower()\n            embedding_vector = embeddings_dict.get(word_c)\n            if embedding_vector is not None:\n                embedding_matrix[i] = embedding_vector\n                num_known_words +=1\n                continue # if embedding found - process next word\n                \n            word_c = word.capitalize()\n            embedding_vector = embeddings_dict.get(word_c)\n            if embedding_vector is not None:\n                embedding_matrix[i] = embedding_vector\n                num_known_words +=1\n                continue # if embedding found - process next word\n                \n            word_c = word.upper()\n            embedding_vector = embeddings_dict.get(word_c)\n            if embedding_vector is not None:\n                embedding_matrix[i] = embedding_vector\n                num_known_words +=1\n                continue # if embedding found - process next word\n                \n            word_c = ps.stem(word)\n            embedding_vector = embeddings_dict.get(word_c)\n            if embedding_vector is not None:\n                embedding_matrix[i] = embedding_vector\n                num_known_words +=1\n                ps_counter +=1\n                continue # if embedding found - process next word\n                \n            word_c = lc.stem(word)\n            embedding_vector = embeddings_dict.get(word_c)\n            if embedding_vector is not None:\n                embedding_matrix[i] = embedding_vector\n                num_known_words +=1\n                lc_counter +=1\n                continue # if embedding found - process next word\n                \n            word_c = sb.stem(word)\n            embedding_vector = embeddings_dict.get(word_c)\n            if embedding_vector is not None:\n                embedding_matrix[i] = embedding_vector\n                num_known_words +=1\n                sb_counter +=1\n                continue # if embedding found - process next word\n                \n            word_c = lemmatize(lemmatizer,pos_tag([word])[0][0],wntag(pos_tag([word])[0][1]))\n            embedding_vector = embeddings_dict.get(word_c)\n            if embedding_vector is not None:\n                embedding_matrix[i] = embedding_vector\n                num_known_words +=1\n                lemma_counter +=1\n                continue # if embedding found - process next word\n                \n            else:\n                unknown_words_list.append(word)\n                \n        if embeddings_name == 'googlenews' and i <= max_words:\n            \n            try:\n                word_c = word\n                embedding_vector = embeddings_dict[word_c]  \n                if embedding_vector is not None:\n                    embedding_matrix[i] = embedding_vector\n                    num_known_words +=1\n                    continue # if embedding found - process next word\n\n                word_c = word.lower()\n                embedding_vector = embeddings_dict[word_c]  \n                if embedding_vector is not None:\n                    embedding_matrix[i] = embedding_vector\n                    num_known_words +=1\n                    continue # if embedding found - process next word\n                \n                word_c = word.capitalize()\n                embedding_vector = embeddings_dict[word_c]\n                if embedding_vector is not None:\n                    embedding_matrix[i] = embedding_vector\n                    num_known_words +=1 \n                    continue # if embedding found - process next word\n                \n                word_c = word.upper()\n                embedding_vector = embeddings_dict[word_c]   \n                if embedding_vector is not None:\n                    embedding_matrix[i] = embedding_vector\n                    num_known_words +=1\n                    continue # if embedding found - process next word\n                    \n                word_c = ps.stem(word)\n                embedding_vector = embeddings_dict[word_c]  \n                if embedding_vector is not None:\n                    embedding_matrix[i] = embedding_vector\n                    num_known_words +=1\n                    ps_counter +=1\n                    continue # if embedding found - process next word\n                    \n                word_c = lc.stem(word)\n                embedding_vector = embeddings_dict[word_c] \n                if embedding_vector is not None:\n                    embedding_matrix[i] = embedding_vector\n                    num_known_words +=1\n                    lc_counter +=1\n                    continue # if embedding found - process next word\n                    \n                word_c = sb.stem(word)\n                embedding_vector = embeddings_dict[word_c] \n                if embedding_vector is not None:\n                    embedding_matrix[i] = embedding_vector\n                    num_known_words +=1\n                    sb_counter +=1\n                    continue # if embedding found - process next word\n                    \n                word_c = lemmatize(lemmatizer,pos_tag([word])[0][0],wntag(pos_tag([word])[0][1]))\n                embedding_vector = embeddings_dict[word_c] \n                if embedding_vector is not None:\n                    embedding_matrix[i] = embedding_vector\n                    num_known_words +=1\n                    lemma_counter +=1\n                    continue # if embedding found - process next word\n                    \n            except:\n                unknown_words_list.append(word)\n                \n    try: \n        words_in_embedding[embeddings_name] = list(embeddings_dict.keys())\n    except:\n        try:\n            words_in_embedding[embeddings_name] = list(embeddings_dict.wv.vocab)\n        except:\n            print(\"Error during generation of key list {}\".format(embeddings_name))\n            print(sys.exc_info()[0])\n    \n    # Save embedding matrix:\n    embedding_matrices[embeddings_name] = embedding_matrix\n    \n    \n    print('  Embeddings_matrix created')\n    print('  Shape embedding_matrix: {}'.format(embedding_matrix.shape))\n    print('  Found Embeddings for {:.2f}% of all words'\n          .format((num_known_words / max_words)*100))\n    print(\"  num_known_words :\", num_known_words)\n    print(\"  num words in word_index: \", max_words)\n    print('  Unknown Words: {:.2f}%'.\n          format(((len(unknown_words_list)) / max_words)*100))\n    print(\"  Words found by PorterStemmer: {}\".format(ps_counter))\n    print(\"  Words found by LancasterStemmer: {}\".format(lc_counter))\n    print(\"  Words found by SnowballStemmer: {}\".format(sb_counter))\n    print(\"  Words found by Lemmatisation: {}\".format(lemma_counter))\n          \n    # Top 50 unknown words\n    print(\"  Top 50 unknown words:\\n {}\\n\".format(unknown_words_list[:50]))\n    \n    del num_known_words, unknown_words_list,ps,lc,sb, ps_counter, lc_counter, sb_counter\n    del embedding_matrix, lemma_counter; gc.collect() ","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Function to load + analyze Embeddings\ndef load_and_analyse_Embeddings(embeddings_name, embeddings_path, max_words):\n    \n    if embeddings_name in ['glove', 'paragram', 'fasttext']:  \n        embeddings_dict = {} # create empty embedding dictionary\n        embedding_file = open(embeddings_path, encoding =\"utf8\", errors = 'ignore') # load embedding from path\n\n        # Fill embedding dict with word: vector(coefs) pairs\n        for line in embedding_file:\n            line_values = line.split(' ') # read in values of respective line (= vector)\n            word = line_values[0] #  # first value in line represents the word\n            coefs = np.asarray(line_values[1:], dtype='float32') # all values represent vector\n            embeddings_dict[word] = coefs # add key(word), value(vector) pairs to dict\n\n        embedding_file.close() \n        \n        os.system('echo '+ embeddings_name + 'loaded')\n        print('  ',embeddings_name, 'loaded')\n        print('  {} word vectors within {} dict'.format(len(embeddings_dict),embeddings_name))\n        \n        # Use pre-trained embedding to create final embeddings matrix\n        create_model_embedding_matrix(embeddings_name,word_index,max_words, embeddings_dict)\n        del embeddings_dict, line_values,word,coefs\n                \n    if embeddings_name == 'googlenews':\n        embeddings_file = KeyedVectors.load_word2vec_format(embeddings_path, binary=True)\n        \n        os.system('echo '+ embeddings_name + 'loaded')\n        print('  ',embeddings_name, 'loaded')\n        \n        # Use pre-trained embedding to create final embeddings matrix\n        create_model_embedding_matrix(embeddings_name,word_index,max_words, embeddings_file)\n        del embeddings_file\n        \n    # MEMORY MANAGEMENT!\n    del embeddings_name, embeddings_path\n    gc.collect()\n    \n   # return embeddings_dict","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Data Preparation (3)  - Data Cleaning"},{"metadata":{},"cell_type":"markdown","source":"#### Definition mapping and functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \n                       \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \n                       \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \n                       \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n                       \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n                       \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n                       \"I'm\": \"I am\",\"i'm\": \"i am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n                       \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n                       \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n                       \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \n                       \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \n                       \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \n                       \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n                       \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n                       \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n                       \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n                       \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n                       \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n                       \"weren't\": \"were not\",\"what`s\": \"what is\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n                       \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n                       \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n                       \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \n                       \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n                       \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n                       \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n                       \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n                       \"you're\": \"you are\", \"you've\": \"you have\"}\n\n# dict from https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2 \ncorrect_spell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite',\n                    'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater',\n                    'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization',\n                    'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n                    'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What',\n                    'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are',\n                    'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many',\n                    'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best',\n                    'howdoes': 'how does', 'mastrubation': 'masturbation',\n                    'mastrubate': 'masturbate', \"mastrubating\": 'masturbating',\n                    \"mcdonald's\":'mcdonalds',\n                    'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist',\n                    'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', \n                    'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what',\n                    'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n                    'demonitization': 'demonetization', 'demonetisation': 'demonetization',\n                    'pokémon': 'pokemon', 'quoras': 'quora', 'quorans': 'quora'}\n\n# Kernel \"fork-embeddings-keras-v04\"\nspecials_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \n                 \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', \n                 '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', \n                 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '\\u200b': ' ',\n                 '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': '', 'ε−': ''}\n\npunct = \"/-?!.,#$%\\()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&' + '\\''","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocessing(x):\n    x = str(x)\n    x = re.sub('[’‘´`]', \"'\", x) \n    \n    # replaces one digit by #, two following digits by ## etc.\n    x = re.sub('[0-9]{5,}', '#####', str(x)) \n    x = re.sub('[0-9]{4}', '####', x)\n    x = re.sub('[0-9]{3}', '###', x)\n    x = re.sub('[0-9]{2}', '##', x)\n    x = re.sub('[0-9]\\'[0-9]', 'feet inches', x) # e.g. 5'5 → feet inches\n    \n    for word in x.split():\n        if word.lower() in contraction_mapping.keys():\n            x = x.replace(word, contraction_mapping[word.lower()])\n        if word in correct_spell_dict.keys():\n            x = x.replace(word, correct_spell_dict[word])\n        if word in specials_mapping.keys():\n            x = x.replace(word, specials_mapping[word])\n        if word[0] in punct and len(word) != 1: # remove punctuation directly in front of word\n            x = x.replace(word[0], '') \n        \n    x = ' '.join(word_tokenize(x)) # separates puncutation from words\n               \n    return x","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Apply preprocessing functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"os.system('echo Applying preprocessing functions..')\ndf[\"question_text\"] = df[\"question_text\"].fillna(\"_nan_\").progress_apply(lambda x: preprocessing(x))\nos.system('echo prepocessing done')\n\nX = df.loc[:, 'question_text'].values\ny = np.asarray(df.loc[:, 'target'].values)\n\npadded_seq, word_index = my_tokenizer(X) # translate text to numerical values\nmax_words = min(max_words, len(word_index)) # e.g.10k words or all words\n      # index +1 because fill process of matrix starts at 1\nprint(len(word_index))\nos.system('echo Tokenization completed')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Iteration loop to compare different embeddings (3)"},{"metadata":{"trusted":true},"cell_type":"code","source":"for embedding in embeddings:\n    emb_name = embedding['name']\n    emb_path = embedding['embeddings_path']\n    print(\"Running procedure on {}:\".format(emb_name))\n    \n    load_and_analyse_Embeddings(emb_name, emb_path, max_words) # loading embedding\n\nos.system(\"echo load_and_analyse_Embeddings done\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Modeling\n\n* starting with a simple neural network\n* model is encapsulated into a function for clarity reasons and to make model easier for later submission \n* literature recommends not to use pre-trained embeddings in case of the availability of a large dataset. 1.3 million questions are available in the train-set, which should be worth a try. The Embedding Layer tries to derive optimal vectors for the input words. After training the weights of the Embedding Layer represent these vectors.\n* state-of-the-art loss function for binary classification tasks: **binary_crossentropy**\n* **optimizer rmsprop** is in most-cases a good choice according to current research literature\n* using the default learning rate of rmsprop is recommended and applied here"},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_keras_model():\n    model = Sequential()\n    model.add(Embedding(input_dim = max_words+1, # 10k different words/integers\n                        output_dim = embedding_dim, \n                        weights = [embedding_matrices['paragram']],\n                        trainable = False)) \n    \n    model.add(GRU(32,dropout=0.2,recurrent_dropout=0.2))\n    model.add(Dense(1, activation='sigmoid')) # final -  binary classifier\n    \n    model.compile(optimizer='rmsprop',\n                  loss='binary_crossentropy',\n                  metrics=['accuracy'])\n        \n    return model\n\nget_keras_model().summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training + Evaluation\n\n* Input for model are batches of sequences\n        * Input shape: 2D tensor(matrix): (batch_size, sequence_length).\n* each sequence has a maximum length of maxlen (here: 100)\n* Embedding Layer translates Integers of each sequence into dense vectors\n  (comp. https://keras.io/layers/embeddings/)\n      * input_length: Length of input sequence\n* embedding_matrix translates integers into into 3D Tensores of shape:\n      * Output shape: (batch_size, sequence_length, output_dim)\n\n\n* accuracy metric is not suitable is cases target classes are quite unequal distributed\n* ROC-AUC is also not applicable for the same reason\n* Suitable Evaluation metrics: Precision, Recall and F1-Score as combination of both. Since these metric functions were removed in Keras 2.0, they are implemented within an own callback below. # class based on:\nhttps://medium.com/@thongonary/how-to-compute-f1-score-for-each-epoch-in-keras-a1acd17715a2"},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomMetric(Callback):\n\n    # Create Instance at the beginning of each epoch\n    def on_train_begin(self, logs={}):\n        self.best_val_f1s = [] # collects best f1 after each epoch with best threshold\n    \n    # Function called at the end of ery epoch\n    def on_epoch_end(self, epoch, logs={}):\n        t0 = time.time()\n        \n        val_predict = np.asarray(self.model.predict(self.validation_data[0])) \n        val_target = self.validation_data[1]\n        \n        # Find best threshold for prediction\n        best_f1 = 0               \n        for threshold in np.arange(0.1,0.501, 0.01):\n            val_f1 = f1_score(y_true = val_target, y_pred = val_predict > threshold)\n            if val_f1 > best_f1:\n                best_f1 = val_f1\n\n        self.best_val_f1s.append(best_f1)\n        \n        t1 = time.time()\n        #if epoch % 2 == 0:\n        print(\"  -- epoch: {}\".format(epoch))  \n        print(\"Execution time on_epoch_end {}\".format(t1-t0))\n        os.system(\"echo  -- epoch: {}\".format(epoch))\n        os.system(\"echo Execution time on_epoch_end {}\".format(t1-t0))\n        return","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* StratifiedKFold: training is performed \"kfold\"- time, within each fold several epochs are run."},{"metadata":{"trusted":true},"cell_type":"code","source":"my_y_val_preds = {} # dictionary to collect model predictions at the end of each split\nmy_y_val_targets = {} # dictionary of true classes at the end of each split\nmy_history_dict = {} # to collect accuracy of each epoch\nmy_metrics_dict = {} # to collect best f1 of each epoch\nbest_f1_dict = {} # final evaluation at the end of training (after last epoch)\nbest_threshold_dict = {} # final evaluation at the end of training (after last epoch)\n\n# fold_list contains train and validation indices (folds) for each split\nfolds = list(StratifiedKFold(n_splits=kfolds, shuffle= True, random_state=123)\n             .split(padded_seq, y))\n\ntf.logging.set_verbosity(tf.logging.ERROR) # dont show warnings (e.g. tensorflow version problems)\nfor i, (train_indices, val_indices) in enumerate(folds):\n    \n    print('\\nSplit: {}  \\n '.format(i))\n    os.system(\"echo running split {}\".format(i))\n    X_train, X_val = padded_seq[train_indices], padded_seq[val_indices] \n    y_train, y_val = y[train_indices], y[val_indices] \n\n    model = get_keras_model() # create new model for current split\n    my_metrics = CustomMetric() # create new metrics instance\n \n    # Training process is logged in history object for visualisation purposes\n    # within each split setting the model is trained several epochs (complete fit)\n    history = model.fit(X_train, y_train,\n                        epochs = model_epochs, \n                        batch_size= 512,\n                        verbose = 1, \n                        validation_data=(X_val, y_val),\n                        callbacks = [my_metrics])\n    \n    ############## at the end of each training process: ##################\n    \n    my_history_dict[i] = history\n    my_metrics_dict[i] = my_metrics\n        \n    y_val_pred = model.predict(X_val) # prediction on \n    my_y_val_preds[i] = y_val_pred \n    my_y_val_targets[i] = y_val\n    \n    # Find best threshold for prediction\n    best_f1 = 0\n    best_threshold = 0\n    for threshold in np.arange(0.1,0.5, 0.01):\n        # calucate f1 score for allowed thresholds\n        f1_score_threshold = f1_score(y_true = y_val ,\n                                              y_pred = y_val_pred > threshold) # 0 or 1\n        if f1_score_threshold > best_f1:\n            best_f1 = f1_score_threshold\n            best_threshold = threshold\n            best_f1_dict[i] = best_f1\n            best_threshold_dict[i] = best_threshold","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"thresh_avg = 0\nthresh_sum = 0\nf1_avg = 0\nf1_sum = 0\n\nfor key, value in best_f1_dict.items():\n    print(\"Split: {} : Best F1 score: {:6.4f} reached with a threshold of {:6.4f}\"\n          .format(key, best_f1_dict[key], best_threshold_dict[key]))\n    thresh_sum += best_threshold_dict[key] \n    thresh_avg = thresh_sum/kfolds\n    f1_sum += best_f1_dict[key] \n    f1_avg = f1_sum/kfolds\n   \nprint(\"\")\nprint(\"Threshold for prediction: {:6.4f}\".format(thresh_avg))\nprint(\"Average F1-Score: {:5.3f}\".format(f1_avg))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Evaluation\n\n* For the evaluation of unbalanced datasets the accuracy as e.g. implemented in SKLEARN's function cross_val_score() is NOT recommended\n* A high F1 score is the target in this competition and combines precision and recall\n* **Plotting information captured during training by History Object and Callback Object**"},{"metadata":{"trusted":true},"cell_type":"code","source":"for i in my_history_dict:\n    \n    print('Split: {} '.format(i))\n    loss = my_history_dict[i].history['loss']\n    val_loss = my_history_dict[i].history['val_loss']\n    epochs = np.arange(1, len(loss) +1, 1) # x-axis\n\n    # Plotting:\n    f = plt.figure(figsize=(10,3))\n    # plots loss\n    f.add_subplot(1, 2, 1) # number of rows, number of columns,subplot you're currently on.\n    plt.xticks(epochs)\n    plt.plot(epochs, loss, 'bo', label='Loss Training', color = 'black')\n    plt.plot(epochs, val_loss, 'b', label='Loss Validation')\n    plt.title('Loss function Training / Validation')\n    plt.legend()\n \n    # plots f1 score\n    f.add_subplot(1, 2, 2)\n    best_val_f1s = my_metrics_dict[i].best_val_f1s\n   # plt.figure()\n    plt.xticks(epochs)\n    plt.plot(epochs, best_val_f1s, 'b', label='F1 Validation')\n    plt.title('F1 Validation')\n    plt.legend()\n    \n    #plt.subplots_adjust(wspace=0.30) # width reserved for blank space between subplots\n    f.tight_layout()\n    plt.show()","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":1}