{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation für Natural Language Processing (NLP)\n",
    "\n",
    "- Darstellungen unter Einsatz der populären NLP-Bibliothek [nltk](https://www.nltk.org/) (Installation und Download erforderlich).\n",
    "- Weitere verbreitete Bibliotheken sind [spacy](https://spacy.io/), [gensim](https://radimrehurek.com/gensim/) und [TextBlob](https://textblob.readthedocs.io/en/dev/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisierung und n-Gramme\n",
    "\n",
    "\n",
    "- Tokenisierung bezeichnet die Zerlegung eines Satzes in seine Bestandteile.\n",
    "- Mögliche Zerlegungsmethoden sind:\n",
    "  - Unigramme (Ein Wort)\n",
    "  - Bigramme (Zwei Wörter)\n",
    "  - Trigramme (Drei Wörter)\n",
    "  - allgemein spricht man von: n-Grammen (n Wörter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams:  ['The', 'United', 'States', 'is', 'a', 'country', '.']\n",
      "Bigrams:  [('The', 'United'), ('United', 'States'), ('States', 'is'), ('is', 'a'), ('a', 'country'), ('country', '.')]\n",
      "\n",
      "Meaningful Bigram:  ('United', 'States')\n"
     ]
    }
   ],
   "source": [
    "import nltk # library to support NLP tasks\n",
    "from nltk import word_tokenize # splits sentences in words/tokens\n",
    "\n",
    "# Tokenization in unigrams\n",
    "my_sentence = \"The United States is a country.\"\n",
    "words = word_tokenize(my_sentence)\n",
    "print(\"Unigrams: \", words)\n",
    "\n",
    "# Tokenization in bigrams\n",
    "my_sentence = \"The United States is a country.\"\n",
    "words = word_tokenize(my_sentence)\n",
    "bigrams = list(nltk.bigrams(words))\n",
    "print(\"Bigrams: \",bigrams )\n",
    "print(\"\\nMeaningful Bigram: \",  bigrams[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoS Tagging\n",
    "\n",
    "- PoS = Parts of Speech\n",
    "- Beim PoS Tagging wird jedem Wort (bzw. Interpunktionszeichen) eines Satzes eine Wortart zugewiesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoS tagged my_sentence: \n",
      " [('The', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('are', 'VBP'), ('a', 'DT'), ('country', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "my_sentence = \"The United States are a country.\"\n",
    "words = word_tokenize(my_sentence)\n",
    "pos_words = nltk.pos_tag(words) # PoS Tagging\n",
    "print(\"PoS tagged my_sentence: \\n\", pos_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erklärung zu den Wortarten:\n",
    "    - DT: determiner (Bestimmungswort)\n",
    "    - NNP: proper noun, singular (korrekt geschriebenes Substantiv im Singular\n",
    "    - NNPS: Proper noun, plural (korrekt geschriebenes Substantiv im Plural)\n",
    "    - VBP: verb present (Verb in Zeitfrom Präsens)\n",
    "    - DT: determiner (Bestimmungswort)\n",
    "    - NN: noun, singular or mass (Substantiv)\n",
    "\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopwörter\n",
    "\n",
    "- Stopwörter sind häufig in einem Satz auftretende Wörter mit wenig Bedeutungsinhalt\n",
    "- Stopwörter kennzeichnet die Eigenschaft, dass sie eine geringe Relevanz für das Verständnis eines Satzes besitzen.\n",
    "- Die Entfernung von Stopwörtern aus Texten ist ein Standard-Preprocessing Schritt.\n",
    "- Mögliche Stopwörter im Englischen sind beispielsweise \"a\", \"am\" und \"the\"\n",
    "- Stopwörter werden entfernt, weil sie wenig/keinen positiven Einfluss auf die Leistung eines Machine Learning Modells zur Textklassifikation besitzen. \n",
    "- Insbesondere wenn klassische Modelle wie SVM oder Naive Bayes zum Einsatz kommen, sollten Stopwörter entfernt werden. Machine Learning Modelle basierend auf Neuronalen Netzen sind in der Lage die für die jeweilige Aufgabe relevanten Features selbstständig zu identifizieren. Eine Entfernung von Stopwörter ist nicht zwingend erforderlich.\n",
    "- Die Entfernung von Stopwörtern reduziert darüber hinaus die zu verarbeitende Datenmenge erheblich. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 stopwords in english stopword list : \n",
      " ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "\n",
      "Orginial Tokens :\n",
      " ['The', 'United', 'States', 'is', 'a', 'country', '.']\n",
      "\n",
      "Tokens without stopwords :\n",
      " ['The', 'United', 'States', 'country', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords # nltk provides stop words lists\n",
    "\n",
    "english_stopwords = stopwords.words('English') # select english stopword list\n",
    "print(\"First 10 stopwords in english stopword list : \\n\", english_stopwords[:10])\n",
    "\n",
    "my_sentence = \"The United States is a country.\"\n",
    "words = word_tokenize(my_sentence)\n",
    "\n",
    "# Filter out stopwords from my_sentence which are contained in stopword list\n",
    "words_wo_stops = [word for word in words if word not in english_stopwords]\n",
    "\n",
    "print(\"\\nOrginial Tokens :\\n\", words)\n",
    "print(\"\\nTokens without stopwords :\\n\", words_wo_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens \"is\" und \"a\" wurden aus dem Satz entfernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"is\" and \"a\" in english_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalisierung\n",
    "\n",
    "- Im NLP-Kontext wird von Normalisierung gesprochen, wenn verschiedene Variationen eines Wortes oder Ausdrucks existieren und diese auf eine einheitliche Form zurückgeführt werden.\n",
    "- Eine mögliche Variation stellt die Konjugation von Verben dar. Englische Wörter wie \"doing\" oder \"does\" werden durch Normalisierung auf die Stammform \"do\" reduziert.\n",
    "- Auch Abkürzungen wie \"US\" werden durch Normalisierung in den Ländernamen \"United States\" transformiert.\n",
    "<br><br>\n",
    "- Es existieren verschiedene Normalisierungstechniken für Text. Bekannte Methoden sind: \n",
    "  - <b>Korrektur von Rechtschreibung</b>\n",
    "  - <b>Stemming (Stammformreduktion)</b>\n",
    "  - <b>Lemmatisierung</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Sentence:  The United States is a country.\n"
     ]
    }
   ],
   "source": [
    "# Simple text normalization: change US to United States\n",
    "my_sentence = \"The US is a country.\"\n",
    "my_normalized_sentence = my_sentence.replace(\"US\", \"United States\")\n",
    "print(\"Normalized Sentence: \", my_normalized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rechtschreibkorrektur\n",
    "\n",
    "- Die Korrektur von Rechtschreibung stellt einen aufwendigen Prozess in der Datenvorverarbeitung von Text dar.\n",
    "- Wenn für das Verständnis eines Satzes wichtige Wörter falsch geschrieben wurden, besteht die Gefahr, dass diese nicht ins spätere Machine Learning Modell überführt werden können (vgl. Embeddings). Dies würde einen Informationsverlust im Preprocessing darstellen und sollte vermieden werden.\n",
    "- Die Python Bibliothek [autocorrect](https://github.com/phatpiglet/autocorrect/) unterstützt das Korrigieren von Rechtschreibung in begrenztem Umfang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words:  ['HTe', 'Uited', 'States', 'is', 'a', 'contry', '.']\n",
      "Autocorrected Words:  ['The', 'United', 'States', 'is', 'a', 'country', 'a']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from autocorrect import spell\n",
    "\n",
    "my_sentence = \"HTe Uited States is a contry.\" # Spelling errors in \"Uited\" and \"contry\"\n",
    "words = word_tokenize(my_sentence)\n",
    "print(\"Original Words: \", words)\n",
    "\n",
    "autocorrected_words =  [spell(word) for word in words]\n",
    "print(\"Autocorrected Words: \", autocorrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong Correction to 'Count' instead of 'Country'.\n"
     ]
    }
   ],
   "source": [
    "# Autocorrect is not always working as expected!\n",
    "spell_error_word = \"Countr\" \n",
    "print(\"Wrong Correction to '{}' instead of 'Country'.\".format(spell(spell_error_word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming (Stammformreduktion)\n",
    "\n",
    "-  Stemming bezeichnet ein Verfahren, das Wörter auf ihre Stammform zurückführt. \n",
    "-  Für Verben stellt der Infinitiv die Stammform dar.\n",
    "- Durch Stemming können auch Nomen wie \"Production\" und \"Products\" auf die Stammform \"Product\" reduziert werden.\n",
    "<br><br>\n",
    "-  Für die Stammformreduktion existieren verschiedene Stemmer-Implementierungen. Populäre Stemmmer:\n",
    "  - PorterStemmer (ältester Stemmer)\n",
    "  - SnowballStemmer (Porter 2 - Weiterentwicklung PorterStemmer)\n",
    "  - LancasterStemmer (Paice-Husk-Stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'do'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "porter_stemmer = nltk.stem.PorterStemmer()\n",
    "porter_stemmer.stem(\"doing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'product'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer('english') # Stemmer with english language support\n",
    "snowball_stemmer.stem(\"Production\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatisierung\n",
    "\n",
    "- Lemmatisierung zielt darauf ab die grammatikalisch korrekte Form eines Wortes zu bilden. Die durch Lemmatisierung erzeugten Wörter werden Lemmata genannt.\n",
    "- Lemmatisierung kann insbesondere beim Einsatz von Stemming sinnvoll sein, da bei der Stammformreduktion existierende Wörter enstehen können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
