{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XLNET_Spacy_V1.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQkVgxE26yNT",
        "colab_type": "text"
      },
      "source": [
        "**Purpose of this Notebook**\n",
        "\n",
        "As stated in chapter 4.1.3, **Transfer Learning** has set a new state-of-the-art in Natural Language Processing (incl. NLU and NLG comp. chapter 2.2). The idea behind Transfer Learning, which has its origin in Computer Vision, has led to major breakthroughs for NLP in the last two years. Powerful language models were created by training them on a large corpus of unlabeled text data. The result are neural networks which are able to capture general facets/aspects of language by using their internal word and character representations.<br>\n",
        "The developers of the NLP-library spaCy have incorporated state-of-the-art-transformer architecturs such as BERT and XLNet in their model pipeline. **In this notebook the pipeline is used to illustrate how \"Generalized Autoregressive Pretraining for Language Understanding (XLNet)\" can be used for text classification in the Quora Case.** \n",
        "\n",
        "* XLNet Paper: https://arxiv.org/abs/1906.08237\n",
        "\n",
        "\n",
        "Further resources:\n",
        "*   https://github.com/explosion/spacy-transformers\n",
        "*   https://github.com/huggingface/transformers (spacy developers wrapped Hugging Face's transformers)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Lo3wWVau_1d",
        "colab_type": "text"
      },
      "source": [
        "Inspecting available RAM\n",
        "\n",
        "\n",
        "*   Sometimes Colab does not offer enough RAM (the batches for quora questions to not fit into RAM)\n",
        "*   Topic was discussed here: https://stackoverflow.com/questions/48750199/google-colaboratory-misleading-information-about-its-gpu-only-5-ram-available\n",
        "* The Code below checks avalable GPU RAM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5E4lZnh6yHI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "79vI_1CX-tR6",
        "colab_type": "code",
        "outputId": "6afb7126-84f6-4bed-d9aa-c0bb3accd2e9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        }
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "gpu = GPUs[0]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7410 sha256=c9e4f9a2c049fbe48f6857420b37ec2d9488e0b0ad1662efd31c3900dcf5c265\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6Lj18Biu-JQ",
        "colab_type": "code",
        "outputId": "690cbb48-1adc-42f8-fa83-5e19294895c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        " \n",
        "printm()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Gen RAM Free: 12.8 GB  | Proc size: 156.5 MB\n",
            "GPU RAM Free: 11441MB | Used: 0MB | Util   0% | Total 11441MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qe6pWFWnBf2u",
        "colab_type": "text"
      },
      "source": [
        "Expectd result: GPU RAM Free: 11441MB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DKJwzd3aOmG",
        "colab_type": "text"
      },
      "source": [
        "### **Step 1: Install State-of-the-Art language model XLNet**\n",
        "\n",
        "*   https://explosion.ai/blog/spacy-transformers?ref=Welcome.AI\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFLEtAoB6Vtw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install --upgrade spacy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eOjws8NE7Lsi",
        "colab_type": "code",
        "outputId": "a1014ab4-0efa-46e4-f075-5d95f7afe9c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import spacy\n",
        "print(\"Spacy Version: \", spacy.__version__) # Version 2.2.2 needed"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spacy Version:  2.2.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXDdRHoEZsdO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!pip install torch==1.1.0\n",
        "#!pip install spacy-pytorch-transformers[cuda100]==0.5.1\n",
        "#!python -m spacy download en_trf_xlnetbasecased_lg # only XLNet is used in this Notebook\n",
        "#!python -m spacy download en_trf_bertbaseuncased_lg "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZnby8EjaIcU",
        "colab_type": "text"
      },
      "source": [
        "### **Step 2: Load necessary libraries**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3dL5KN81aFvP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import thinc\n",
        "import random\n",
        "import spacy\n",
        "import GPUtil\n",
        "import torch\n",
        "from spacy.util import minibatch\n",
        "from tqdm.auto import tqdm\n",
        "import unicodedata\n",
        "import wasabi\n",
        "import numpy\n",
        "from collections import Counter\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W-uAhvKkaK0d",
        "colab_type": "code",
        "outputId": "523c1521-fbfe-4c4e-bac6-42e2d51f7a26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(\"Spacy Version: \", spacy.__version__)\n",
        "print(\"Torch version: \", torch.__version__)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Spacy Version:  2.2.2\n",
            "Torch version:  1.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7cQejkKbM6m",
        "colab_type": "code",
        "outputId": "1a95f086-4cf1-498c-818f-0dd2cf621a91",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "# Ensure that GPU is used \n",
        "spacy.util.fix_random_seed(0)\n",
        "is_using_gpu = spacy.prefer_gpu()\n",
        "if is_using_gpu:\n",
        "    print(\"GPU ON!\\n\")\n",
        "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
        "    print(\"GPU Usage\")\n",
        "    GPUtil.showUtilization()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU ON!\n",
            "\n",
            "GPU Usage\n",
            "| ID | GPU | MEM |\n",
            "------------------\n",
            "|  0 |  2% |  1% |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tPurjXaDbfxJ",
        "colab_type": "text"
      },
      "source": [
        "### **Step 3: Load Quora dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-WU4K6WbXfM",
        "colab_type": "code",
        "outputId": "89fc4303-f1de-49b8-98c3-3f66b12db615",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # Trainset locates in Google Drive. Has to be made available by mounting.\n",
        "df = pd.read_csv(\"/content/drive/My Drive/Colab Notebooks/train.csv\", nrows = 50000) \n",
        "print(df.shape)\n",
        "pd.set_option('display.max_colwidth', 1500)\n",
        "df.head()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "(50000, 3)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>qid</th>\n",
              "      <th>question_text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>00002165364db923c7e6</td>\n",
              "      <td>How did Quebec nationalists see their province as a nation in the 1960s?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>000032939017120e6e44</td>\n",
              "      <td>Do you have an adopted dog, how would you encourage people to adopt and not shop?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0000412ca6e4628ce2cf</td>\n",
              "      <td>Why does velocity affect time? Does velocity affect space geometry?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>000042bf85aa498cd78e</td>\n",
              "      <td>How did Otto von Guericke used the Magdeburg hemispheres?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0000455dfa3e01eae3af</td>\n",
              "      <td>Can I convert montra helicon D to a mountain bike by just changing the tyres?</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    qid  ... target\n",
              "0  00002165364db923c7e6  ...      0\n",
              "1  000032939017120e6e44  ...      0\n",
              "2  0000412ca6e4628ce2cf  ...      0\n",
              "3  000042bf85aa498cd78e  ...      0\n",
              "4  0000455dfa3e01eae3af  ...      0\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFXFShWUgUZW",
        "colab_type": "text"
      },
      "source": [
        "If the inputs to the transformer don't match how it was pretrained, it will have to rely much more on your small labelled training corpus, leading to lower accuracies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igdrtHpr8UMy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lIQONY3zooTj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def _prepare_partition(text_label_tuples, *, preprocess=False):\n",
        "    texts, labels = zip(*text_label_tuples)\n",
        "    # positive = 0 sincere\n",
        "    # negative = 1 insincere\n",
        "    cats = [{\"POSITIVE\": float(bool(y)), \"NEGATIVE\": float(not bool(y))} for y in labels]\n",
        "    return texts, cats\n",
        "\n",
        "def load_data(df):\n",
        "  # transform data as expected by the Spacy Model\n",
        "  # Create datasets (Only take up to max_seq_length words for memory)\n",
        "  max_seq_length = 130\n",
        "\n",
        "  # Shuffle df\n",
        "  df = shuffle(df)\n",
        "\n",
        "  # Extract relevant data for classification\n",
        "  df_texts = df['question_text'].tolist()\n",
        "  df_texts_padded = [question[:max_seq_length] for question in df_texts]\n",
        "  df_labels = df['target'].tolist()\n",
        "\n",
        "  # Create train-validation split\n",
        "  train_texts_padded, dev_texts_padded, train_labels, dev_labels = train_test_split(df_texts, df_labels, test_size=0.2,\n",
        "                                                                                    stratify = df_labels, random_state=90)\n",
        "\n",
        "  # Create tuples\n",
        "  train_data= zip(train_texts_padded, train_labels)\n",
        "  dev_data = zip(dev_texts_padded, dev_labels)\n",
        "\n",
        "  train_texts, train_labels = _prepare_partition(train_data, preprocess=False)\n",
        "  dev_texts, dev_labels = _prepare_partition(dev_data, preprocess=False)\n",
        "  return (train_texts, train_labels), (dev_texts, dev_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wrA-gqVOoLWJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_texts, train_cats), (eval_texts, eval_cats) = load_data(df)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "va6Yi6kqpvJG",
        "colab_type": "code",
        "outputId": "f9f3cb13-8699-4e7e-91e5-c6279f9845b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(eval_texts[0])\n",
        "print(eval_cats[0]) # negative: NOT toxic, insincere"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is it alright to share our feeling of love to opposite sex?\n",
            "{'POSITIVE': 0.0, 'NEGATIVE': 1.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-Cmq7sKeWRI",
        "colab_type": "text"
      },
      "source": [
        "### **Step 4: Load Spacy Model and prepare Pipeline**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWWDsCAxebO_",
        "colab_type": "code",
        "outputId": "20436577-79db-42b5-9b9f-10dca39246de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "model_choice = \"en_trf_xlnetbasecased_lg\" #  [\"en_trf_bertbaseuncased_lg\", \"en_trf_xlnetbasecased_lg\"]\n",
        "max_wpb = 1000 # number of tokens\n",
        "\n",
        "nlp = spacy.load(model_choice)\n",
        "print(f\"Loaded model '{model_choice}'\")\n",
        "print(nlp.pipe_names)\n",
        "if model_choice == \"en_trf_xlnetbasecased_lg\":\n",
        "  textcat = nlp.create_pipe(\n",
        "               \"trf_textcat\",                   #config={\"exclusive_classes\": True} #  \"trf_textcat\",\n",
        "                        config={\"architecture\": \"softmax_last_hidden\", \"words_per_batch\": max_wpb}\n",
        "      )\n",
        "elif model_choice == \"en_trf_bertbaseuncased_lg\":\n",
        "  textcat = nlp.create_pipe(\n",
        "          \"trf_textcat\", config = {\"architecture\": \"softmax_last_hidden\", \"words_per_batch\": max_wpb}\n",
        "      )\n",
        "else: \n",
        "  print(\"Choose a supported transformer model\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Loaded model 'en_trf_xlnetbasecased_lg'\n",
            "['sentencizer', 'trf_wordpiecer', 'trf_tok2vec']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0ZFjYsmdZyS",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://d33wubrfki0l68.cloudfront.net/39251284c89675c9f1db57a109d804077e06620e/9ecfb/blog/img/spacy-trf_pipeline.svg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7dvA1hsdKdh",
        "colab_type": "text"
      },
      "source": [
        "A spacy pipline with the following components was created:\n",
        "\n",
        "*  **sentencizer:** splits sentences on punctuation like ., ! or ? [https://spacy.io/usage/linguistic-features#sbd-component]\n",
        "*  **pytt_wordpiecer:** performs the model's wordpiece pre-processing\n",
        "* **pytt_tok2vec**: runs the transformer over the doc, and saves the results into the built-in doc.tensor attribute and several extension attributes \n",
        "* More info: https://explosion.ai/blog/spacy-transformers\n",
        "\n",
        "One Component is still missing: The component for text categorization: *trf_textcat*\n",
        "\n",
        "*  **trf_textcat** is based on spaCys [textCategorizer](https://spacy.io/api/textcategorizer).\n",
        "* The last component in the current pipeline translates the tokens of a sentence in contextual token representations (vectors)\n",
        "* These vectors are then used by trf_textcat to perform the binary classification task for Quora"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sjproXUOc5xj",
        "colab_type": "code",
        "outputId": "c900b932-8058-4f89-8dce-7df8856d9d80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# trf_textcat was already initialized above\n",
        "\n",
        "# add label to text classifier\n",
        "textcat.add_label(\"POSITIVE\")\n",
        "textcat.add_label(\"NEGATIVE\")\n",
        "\n",
        "# Add trf_textcat as last pipeline component\n",
        "nlp.add_pipe(textcat, last=True)\n",
        "print(nlp.pipe_names) # pipeline looks like this now"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sentencizer', 'trf_wordpiecer', 'trf_tok2vec', 'trf_textcat']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aCN-x5katTjP",
        "colab_type": "text"
      },
      "source": [
        "### **Step 5: Setting up model hyperparameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dbN1cRXjs0W6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_iter= 10 # = number of epochs\n",
        "# n_texts=75 # Changed number of texts to 75 to relieve pressue on GPU memory\n",
        "batch_size= 128 # batch-szie changed to 4 to relieve pressure on GPU memory\n",
        "learn_rate=1e-5\n",
        "pos_label=\"NEGATIVE\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h54Ar91ztahU",
        "colab_type": "text"
      },
      "source": [
        "### **Step 6: Create Evaluation function to monitor learning process**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aq3NP0uHtwDw",
        "colab_type": "text"
      },
      "source": [
        "### **Step 6: Model training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9U5gtZa9-2d",
        "colab_type": "code",
        "outputId": "0190e4fc-7f6d-4828-ac22-bf56e20d2370",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "# Model input\n",
        "print(f\"Using {len(train_texts)} training docs, {len(eval_texts)} evaluation \\n\")\n",
        "\n",
        "# Perparing training data input\n",
        "train_data = list(zip(train_texts, [{\"cats\": cats} for cats in train_cats]))\n",
        "print(train_data[0])\n",
        "\n",
        "# Inspecting Validation data\n",
        "print(\"Validation data text: \", eval_texts[0])\n",
        "print(\"Validation data label: \", eval_cats[0])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using 40000 training docs, 10000 evaluation \n",
            "\n",
            "(\"Why is durian so disgusting to those who don't like it?\", {'cats': {'POSITIVE': 0.0, 'NEGATIVE': 1.0}})\n",
            "Validation data text:  Is it alright to share our feeling of love to opposite sex?\n",
            "Validation data label:  {'POSITIVE': 0.0, 'NEGATIVE': 1.0}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2McLDteCHgtV",
        "colab_type": "code",
        "outputId": "28c58cbf-5248-410e-8fc4-24164125cb5a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "is_using_gpu = spacy.prefer_gpu()\n",
        "if is_using_gpu:\n",
        "    torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n",
        "\n",
        "#nlp = spacy.load(\"en_trf_bertbaseuncased_lg\")\n",
        "print(nlp.pipe_names) # [\"sentencizer\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
        "#textcat = nlp.create_pipe(\"trf_textcat\", config={\"exclusive_classes\": True})\n",
        "#for label in (\"POSITIVE\", \"NEGATIVE\"):\n",
        "#    textcat.add_label(label)\n",
        "#nlp.add_pipe(textcat)\n",
        "print(\"Final_pipeline: \", nlp.pipe_names)\n",
        "\n",
        "optimizer = nlp.resume_training()\n",
        "for i in range(n_iter):\n",
        "    random.shuffle(train_data)\n",
        "    losses = {}\n",
        "    for batch in minibatch(train_data, size=batch_size):       \n",
        "        texts, cats = zip(*batch)\n",
        "        nlp.update(texts, cats, sgd=optimizer, losses=losses)\n",
        "    print(i, losses)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['sentencizer', 'trf_wordpiecer', 'trf_tok2vec', 'trf_textcat']\n",
            "Final_pipeline:  ['sentencizer', 'trf_wordpiecer', 'trf_tok2vec', 'trf_textcat']\n",
            "0 {'trf_textcat': 0.0024444458842936}\n",
            "1 {'trf_textcat': 0.0022726016477463418}\n",
            "2 {'trf_textcat': 0.0023061066879108694}\n",
            "3 {'trf_textcat': 0.0022859878758936247}\n",
            "4 {'trf_textcat': 0.0022666700258469064}\n",
            "5 {'trf_textcat': 0.0022981258065328802}\n",
            "6 {'trf_textcat': 0.002265798757775883}\n",
            "7 {'trf_textcat': 0.0022597706896476666}\n",
            "8 {'trf_textcat': 0.0022798393233642855}\n",
            "9 {'trf_textcat': 0.0022726109815494056}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J9jj7Z0HG6Tt",
        "colab_type": "text"
      },
      "source": [
        "Loss of Text Categorizer decreases until it stagnates. The Loss osciallates around 0.0022. The quite low loss looks strange but the implementation of the spacy pipeline seems correct. A possible reason herefore could be the that the model was only fit with a quite small subsample of the original data (because of runtime, resource reasons).\n",
        "Other experiments with a different learning rate even resulted in an increasing loss. Reason herefore could have been a too high learning rate (comp. image below)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEacqPRlm2VA",
        "colab_type": "text"
      },
      "source": [
        "![alt text](https://miro.medium.com/max/1106/1*An4tZEyQAYgPAZl396JzWg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q6AETD4_m7iu",
        "colab_type": "text"
      },
      "source": [
        "However even using [Cyclical Learning Rates](https://towardsdatascience.com/adaptive-and-cyclical-learning-rates-using-pytorch-2bf904d18dee) did not give the expected results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qc3G7x6IzR-V",
        "colab_type": "text"
      },
      "source": [
        "### **Step 7: Model prediction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JOwT8rxRys7U",
        "colab_type": "code",
        "outputId": "9a5abf27-db79-4c47-b0db-7dc9d8e215c5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# Test the trained model\n",
        "test_text = eval_texts[0]\n",
        "doc = nlp(test_text)\n",
        "print(\"Sententence to perform Classification on: \\n\", test_text)\n",
        "print(\"Prediction returned by Softmax Function: \", doc.cats)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sententence to perform Classification on: \n",
            " Is it alright to share our feeling of love to opposite sex?\n",
            "Prediction returned by Softmax Function:  {'POSITIVE': 0.05213458463549614, 'NEGATIVE': 0.9478654265403748}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kI-RJMlTHZpB",
        "colab_type": "text"
      },
      "source": [
        "**Prediction is correct:   \n",
        "Model labels question with high confidence as not toxic, which is correct**   \n",
        "(\"NEGATIVE\" indicates class that is NOT TOXIC)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkrtPS1wnZyf",
        "colab_type": "text"
      },
      "source": [
        "### **Step 8: Evaluation**\n",
        "\n",
        "*   Spacy provides an easy to use interface to implement state-of-the-art NLP architectures. Positive is that spacy already incorporated XLNet which first was released some months ago. However the documentation of [spacy-transformers](https://github.com/explosion/spacy-transformers) still is insufficient because of the following reasons: \n",
        "  * First, it is not clear which config to use for the trf-textcat component. The demo recommends to use {\"architecture\": \"softmax_last_hidden\"}, however this architecture is not described in the [Spacy TextCategorizer Docs](https://spacy.io/api/textcategorizer#architectures). Since the Quora Case is a binary classification task config={\"exclusive_classes\": True} seems to be possible as well.\n",
        "  *   Second, using raw XLNet without Fine-Tuning does not lead to different predictions for the Quora questions. Normally the internal representations of XLNet should lead to different softmax outputs.\n",
        "  * Third, at the time of this writting it is unclear why it is not possible to only train the trf_textcat component alone without modifying the XLNet vector (tok2vec component).\n",
        "  * Furthermore the loss of the trf_textcat component above indicates that there seems to be something wrong with either the spacy implementation of the pytorch-transformers or the current adaption of the [spacy demo for text classification](https://github.com/explosion/spacy-transformers/blob/master/examples/train_textcat.py) to the quora case.\n",
        "  * A problem in the quora case is that the dataset is quite imbalanced. For this reason in the final prototype a search for an optimal threshold was conducted. However the spacy demo fixes a threshold at 0.5, which cannot be done in the Quora case (optimal threshold for the language model predictions are unknown). Because the Spacy pipeline processes documents as a stream (Generator Object) the experimental threshold search, used in the custom Keras model, can not be used.\n"
      ]
    }
  ]
}