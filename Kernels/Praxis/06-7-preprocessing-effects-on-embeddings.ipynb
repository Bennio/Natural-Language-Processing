{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Purpose of this kernel - continuance from kernel 6.6\n",
    "\n",
    "* word is it was originally written is tested on embeddings. (not only \"lower()-Version\" of the word) \n",
    "* Preprocessing function was adapted accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c944e8dbdeb4285b21de9b1c61766dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/opt/conda/lib/python3.6/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "# General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook \n",
    "tqdm_notebook().pandas()\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# Preprocessing\n",
    "import seaborn as sns\n",
    "import re\n",
    "from re import *\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer \n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Modeling\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "# Training\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "  # splits train-set into into train and validation folds\n",
    "    \n",
    "# Evaluation\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast Run Testing\n",
    "#total_train_samples = 100000 # max is 1306122\n",
    "#total_test_samples = 2000 # max is 375806\n",
    "total_train_samples = 1306122 # max is 1306122\n",
    "total_test_samples = 375806 # max is 375806\n",
    "\n",
    "# Preprocessing\n",
    "maxlen = 130 # 130 covers about 75% of all bad questions completely\n",
    "\n",
    "# Modeling\n",
    "embedding_dim = 300 # set to 300 to be able to compare with pre-trained embeddings\n",
    "\n",
    "# Training\n",
    "kfolds = 3\n",
    "model_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../input/train.csv\")\n",
    "str_ = 'Train data loaded'\n",
    "os.system('echo '+str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape for this run:  1306122 3\n",
      "Shape data tensor: (1306122,)\n",
      "Shape target tensor: (1306122,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province as a nation in the 1960s?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you encourage people to adopt and not shop?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity affect space geometry?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  ...   target\n",
       "0  00002165364db923c7e6  ...        0\n",
       "1  000032939017120e6e44  ...        0\n",
       "2  0000412ca6e4628ce2cf  ...        0\n",
       "\n",
       "[3 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[:total_train_samples] # for Testing purposes\n",
    "num_samples,n = df.shape\n",
    "print(\"Shape for this run: \", num_samples, n)\n",
    "\n",
    "X = df.loc[:, 'question_text'].values\n",
    "y = df.loc[:, 'target'].values\n",
    "\n",
    "# Since Neural Networks are only able to perform transformations on tensors \n",
    "y = np.asarray(y) # Transformation target labels to numpy array \n",
    "\n",
    "print('Shape data tensor:', X.shape) \n",
    "print('Shape target tensor:', y.shape) # 1D Tensor\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1500) # inrease display column size\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (1)  - tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(texts):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(texts) \n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "        padded_seq = pad_sequences(sequences, maxlen=maxlen)  \n",
    "        word_index = tokenizer.word_index  \n",
    "        \n",
    "        return padded_seq, word_index\n",
    "    \n",
    "# Apply tokenization on whole dataset\n",
    "#padded_seq, word_index = my_tokenizer(X)\n",
    "#os.system('echo Tokenization completed')\n",
    "#print(\"Found {} unique tokens\".format(len(word_index)))\n",
    "#print(\"Top 5 most frequent words: {}\".format(\n",
    "#    {word: word_index[word] for word in list(word_index)[:5]}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (2)  - Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings path\n",
    "_glove = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "_paragram =  '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "_wiki_news = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "_google_news = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "embeddings = [{'name': 'glove', 'embeddings_path': _glove},\n",
    "              {'name': 'paragram', 'embeddings_path': _paragram},\n",
    "              {'name': 'fasttext', 'embeddings_path': _wiki_news},\n",
    "              {'name': 'googlenews', 'embeddings_path': _google_news}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of functions to load and analyse embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for lemmatization from http://textmining.wp.hs-hannover.de/Preprocessing.html\n",
    "\n",
    "def wntag(pttag):\n",
    "    if pttag in ['JJ', 'JJR', 'JJS']:\n",
    "        return wn.ADJ\n",
    "    elif pttag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        return wn.NOUN\n",
    "    elif pttag in ['RB', 'RBR', 'RBS']:\n",
    "        return wn.ADV\n",
    "    elif pttag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def lemmatize(lemmatizer,word,pos):\n",
    "    if pos == None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(word,pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create embedding matrix\n",
    "embedding_matrices = {}\n",
    "words_in_embedding = {}\n",
    "def create_model_embedding_matrix(embeddings_name,word_index,embeddings_dict):\n",
    "\n",
    "    embedding_dim = 300 # (vector size 300!)\n",
    "    embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
    "    unknown_words_list = []\n",
    "    num_known_words = 0  \n",
    "        \n",
    "    ps = PorterStemmer()\n",
    "    ps_counter = 0\n",
    "    lc = LancasterStemmer()\n",
    "    lc_counter = 0\n",
    "    sb = SnowballStemmer(\"english\")\n",
    "    sb_counter = 0\n",
    "    lemma_counter = 0\n",
    "\n",
    "    # Filling up matrix\n",
    "    for word, i in word_index.items(): \n",
    "        \n",
    "        if embeddings_name in ['glove', 'paragram', 'fasttext']:\n",
    "            \n",
    "            embedding_vector = embeddings_dict.get(word) # get vector for word from embedding \n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                continue # if embedding found - process next word\n",
    "                \n",
    "            word_c = word.lower()\n",
    "            embedding_vector = embeddings_dict.get(word_c)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                continue # if embedding found - process next word\n",
    "                \n",
    "            word_c = word.capitalize()\n",
    "            embedding_vector = embeddings_dict.get(word_c)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                continue # if embedding found - process next word\n",
    "                \n",
    "            word_c = word.upper()\n",
    "            embedding_vector = embeddings_dict.get(word_c)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                continue # if embedding found - process next word\n",
    "                \n",
    "            word_c = ps.stem(word)\n",
    "            embedding_vector = embeddings_dict.get(word_c)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                ps_counter +=1\n",
    "                continue # if embedding found - process next word\n",
    "                \n",
    "            word_c = lc.stem(word)\n",
    "            embedding_vector = embeddings_dict.get(word_c)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                lc_counter +=1\n",
    "                continue # if embedding found - process next word\n",
    "                \n",
    "            word_c = sb.stem(word)\n",
    "            embedding_vector = embeddings_dict.get(word_c)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                sb_counter +=1\n",
    "                continue # if embedding found - process next word\n",
    "                \n",
    "            word_c = lemmatize(lemmatizer,pos_tag([word])[0][0],wntag(pos_tag([word])[0][1]))\n",
    "            embedding_vector = embeddings_dict.get(word_c)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                lemma_counter +=1\n",
    "                continue # if embedding found - process next word\n",
    "                \n",
    "            else:\n",
    "                unknown_words_list.append(word)\n",
    "                \n",
    "        if embeddings_name == 'googlenews':\n",
    "            try:\n",
    "                embedding_vector = embeddings_dict[word]  \n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1\n",
    "                    continue # if embedding found - process next word\n",
    "\n",
    "                word_c = word.lower()\n",
    "                embedding_vector = embeddings_dict.get(word_c)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1\n",
    "                    continue # if embedding found - process next word\n",
    "                \n",
    "                word_c = word.capitalize()\n",
    "                embedding_vector = embeddings_dict[word_c]\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1 \n",
    "                    continue # if embedding found - process next word\n",
    "                \n",
    "                word_c = word.upper()\n",
    "                embedding_vector = embeddings_dict.get(word_c)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1\n",
    "                    continue # if embedding found - process next word\n",
    "                    \n",
    "                word_c = ps.stem(word)\n",
    "                embedding_vector = embeddings_dict.get(word_c)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1\n",
    "                    ps_counter +=1\n",
    "                    continue # if embedding found - process next word\n",
    "                    \n",
    "                word_c = lc.stem(word)\n",
    "                embedding_vector = embeddings_dict.get(word_c)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1\n",
    "                    lc_counter +=1\n",
    "                    continue # if embedding found - process next word\n",
    "                    \n",
    "                word_c = sb.stem(word)\n",
    "                embedding_vector = embeddings_dict.get(word_c)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1\n",
    "                    sb_counter +=1\n",
    "                    continue # if embedding found - process next word\n",
    "                    \n",
    "                word_c = lemmatize(lemmatizer,pos_tag([word])[0][0],wntag(pos_tag([word])[0][1]))\n",
    "                embedding_vector = embeddings_dict.get(word_c)\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1\n",
    "                    lemma_counter +=1\n",
    "                    continue # if embedding found - process next word\n",
    "                    \n",
    "            except:\n",
    "                unknown_words_list.append(word)\n",
    "                \n",
    "    try: \n",
    "        words_in_embedding[embeddings_name] = list(embeddings_dict.keys())\n",
    "    except:\n",
    "        try:\n",
    "            words_in_embedding[embeddings_name] = list(embeddings_dict.wv.vocab)\n",
    "        except:\n",
    "            print(\"Error during generation of key list {}\".format(embeddings_name))\n",
    "            print(sys.exc_info()[0])\n",
    "    \n",
    "    print('  Embeddings_matrix created')\n",
    "    print('  Shape embedding_matrix: {}'.format(embedding_matrix.shape))\n",
    "    print('  Found Embeddings for {:.2f}% of all words'\n",
    "          .format((num_known_words / len(word_index))*100))\n",
    "    print(\"  num_known_words :\", num_known_words)\n",
    "    print(\"  num words in word_index: \", len(word_index))\n",
    "    print('  Unknown Words: {:.2f}%'.\n",
    "          format(((len(unknown_words_list)) / len(word_index))*100))\n",
    "    print(\"  Words found by PorterStemmer: {}\".format(ps_counter))\n",
    "    print(\"  Words found by LancasterStemmer: {}\".format(lc_counter))\n",
    "    print(\"  Words found by SnowballStemmer: {}\".format(sb_counter))\n",
    "    print(\"  Words found by Lemmatisation: {}\".format(lemma_counter))\n",
    "          \n",
    "    # Top 50 unknown words\n",
    "    print(\"  Top 50 unknown words:\\n {}\\n\".format(unknown_words_list[:50]))\n",
    "    \n",
    "    del num_known_words, unknown_words_list,ps,lc,sb, ps_counter, lc_counter, sb_counter\n",
    "    del embedding_matrix, lemma_counter; gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load + analyze Embeddings\n",
    "def load_and_analyse_Embeddings(embeddings_name, embeddings_path):\n",
    "    \n",
    "    if embeddings_name in ['glove', 'paragram', 'fasttext']:  \n",
    "        embeddings_dict = {} # create empty embedding dictionary\n",
    "        embedding_file = open(embeddings_path, encoding =\"utf8\", errors = 'ignore') # load embedding from path\n",
    "\n",
    "        # Fill embedding dict with word: vector(coefs) pairs\n",
    "        for line in embedding_file:\n",
    "            line_values = line.split(' ') # read in values of respective line (= vector)\n",
    "            word = line_values[0] #  # first value in line represents the word\n",
    "            coefs = np.asarray(line_values[1:], dtype='float32') # all values represent vector\n",
    "            embeddings_dict[word] = coefs # add key(word), value(vector) pairs to dict\n",
    "\n",
    "        embedding_file.close() \n",
    "        \n",
    "        os.system('echo '+ embeddings_name + 'loaded')\n",
    "        print('  ',embeddings_name, 'loaded')\n",
    "        print('  {} word vectors within {} dict'.format(len(embeddings_dict),embeddings_name))\n",
    "        \n",
    "        # Use pre-trained embedding to create final embeddings matrix\n",
    "        create_model_embedding_matrix(embeddings_name,word_index,embeddings_dict)\n",
    "        del embeddings_dict, line_values,word,coefs\n",
    "                \n",
    "    if embeddings_name == 'googlenews':\n",
    "        embeddings_file = KeyedVectors.load_word2vec_format(embeddings_path, binary=True)\n",
    "        \n",
    "        os.system('echo '+ embeddings_name + 'loaded')\n",
    "        print('  ',embeddings_name, 'loaded')\n",
    "        \n",
    "        # Use pre-trained embedding to create final embeddings matrix\n",
    "        create_model_embedding_matrix(embeddings_name,word_index,embeddings_file)\n",
    "        del embeddings_file\n",
    "        \n",
    "    # MEMORY MANAGEMENT!\n",
    "    del embeddings_name, embeddings_path\n",
    "    gc.collect()\n",
    "    \n",
    "   # return embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (3)  - Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition mapping and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \n",
    "                       \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \n",
    "                       \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \n",
    "                       \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n",
    "                       \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n",
    "                       \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n",
    "                       \"I'm\": \"I am\",\"i'm\": \"i am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
    "                       \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                       \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n",
    "                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n",
    "                       \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \n",
    "                       \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \n",
    "                       \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \n",
    "                       \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n",
    "                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                       \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n",
    "                       \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
    "                       \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                       \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n",
    "                       \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
    "                       \"weren't\": \"were not\",\"what`s\": \"what is\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
    "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                       \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                       \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                       \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \n",
    "                       \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "                       \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
    "                       \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n",
    "                       \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n",
    "                       \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "# dict from https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2 \n",
    "correct_spell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite',\n",
    "                    'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater',\n",
    "                    'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization',\n",
    "                    'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n",
    "                    'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What',\n",
    "                    'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are',\n",
    "                    'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many',\n",
    "                    'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best',\n",
    "                    'howdoes': 'how does', 'mastrubation': 'masturbation',\n",
    "                    'mastrubate': 'masturbate', \"mastrubating\": 'masturbating',\n",
    "                    \"mcdonald's\":'mcdonalds',\n",
    "                    'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist',\n",
    "                    'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', \n",
    "                    'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what',\n",
    "                    'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n",
    "                    'demonitization': 'demonetization', 'demonetisation': 'demonetization',\n",
    "                    'pokémon': 'pokemon', 'quoras': 'quora', 'quorans': 'quora'}\n",
    "\n",
    "# Kernel \"fork-embeddings-keras-v04\"\n",
    "specials_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \n",
    "                 \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', \n",
    "                 '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', \n",
    "                 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '\\u200b': ' ',\n",
    "                 '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': '', 'ε−': ''}\n",
    "\n",
    "punct = \"/-?!.,#$%\\()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numbers(x):\n",
    "    # replaces one digit by #, two following digits by ## etc.\n",
    "    x = re.sub('[0-9]{5,}', '#####', x) \n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "def preprocessing(x):\n",
    "    x = str(x)\n",
    "    x = re.sub('[’‘´`]', \"'\", x) \n",
    "    \n",
    "    for word in x.split():\n",
    "        if word.lower() in contraction_mapping.keys():\n",
    "            x = x.replace(word, contraction_mapping[word.lower()])\n",
    "        if word in correct_spell_dict.keys():\n",
    "            x = x.replace(word, correct_spell_dict[word])\n",
    "        if word in specials_mapping.keys():\n",
    "            x = re.sub(word, specials_mapping.get(word),x) \n",
    "            \n",
    "    x = re.sub('\\'s\\s+', ' \\'s ', x)\n",
    "    x = ' '.join(word_tokenize(x))    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "463adc9f0e0a41968a84d3466296d8da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1306122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a803e3b453a4e059f8f82b0f4b59538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1306122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('echo Applying preprocessing functions..')\n",
    "df[\"question_text\"] = df[\"question_text\"].fillna(\"_nan_\").progress_apply(lambda x: preprocessing(x))\n",
    "os.system('echo prepocessing done')\n",
    "df[\"question_text\"] = df[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n",
    "os.system('echo clean_numbers done')\n",
    "\n",
    "X = df.loc[:, 'question_text'].values\n",
    "y = np.asarray(df.loc[:, 'target'].values)\n",
    "\n",
    "padded_seq, word_index = my_tokenizer(X) # Tokenization\n",
    "os.system('echo Tokenization 2 completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration loop to compare different embeddings (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running procedure on glove:\n",
      "   glove loaded\n",
      "  2196016 word vectors within glove dict\n",
      "  Embeddings_matrix created\n",
      "  Shape embedding_matrix: (192333, 300)\n",
      "  Found Embeddings for 77.70% of all words\n",
      "  num_known_words : 149436\n",
      "  num words in word_index:  192332\n",
      "  Unknown Words: 22.30%\n",
      "  Words found by PorterStemmer: 3434\n",
      "  Words found by LancasterStemmer: 4642\n",
      "  Words found by SnowballStemmer: 3846\n",
      "  Words found by Lemmatisation: 79\n",
      "  Top 50 unknown words:\n",
      " ['brexit', 'cryptocurrencies', 'redmi', '₹', 'coinbase', 'oneplus', \"'i\", 'upwork', 'machedo', 'gdpr', 'adityanath', \"'a\", 'boruto', 'bnbr', 'alshamsi', 'dceu', 'litecoin', 'iiest', 'unacademy', \"qur'an\", \"5'\", 'zerodha', 'tensorflow', 'doklam', 'kavalireddi', 'lnmiit', '°c', 'muoet', 'etc…', 'nicmar', \"5'4\", 'vajiram', 'zebpay', 'srmjee', 'elitmus', \"5'9\", 'altcoins', 'altcoin', 'hackerrank', \"5'7\", 'awdhesh', \"5'3\", 'ryzen', \"5'2\", \"5'5\", 'demonetisation', 'koinex', 'baahubali', \"5'6\", 'mhcet']\n",
      "\n",
      "Running procedure on paragram:\n",
      "   paragram loaded\n",
      "  1703755 word vectors within paragram dict\n",
      "  Embeddings_matrix created\n",
      "  Shape embedding_matrix: (192333, 300)\n",
      "  Found Embeddings for 79.14% of all words\n",
      "  num_known_words : 152208\n",
      "  num words in word_index:  192332\n",
      "  Unknown Words: 20.86%\n",
      "  Words found by PorterStemmer: 4216\n",
      "  Words found by LancasterStemmer: 5275\n",
      "  Words found by SnowballStemmer: 3988\n",
      "  Words found by Lemmatisation: 77\n",
      "  Top 50 unknown words:\n",
      " ['brexit', 'cryptocurrencies', 'redmi', '₹', 'coinbase', 'oneplus', \"'i\", 'upwork', 'machedo', 'gdpr', 'adityanath', \"'a\", 'boruto', 'bnbr', 'alshamsi', 'dceu', 'litecoin', 'iiest', 'unacademy', \"qur'an\", \"5'\", 'zerodha', 'tensorflow', 'doklam', 'kavalireddi', 'lnmiit', '°c', 'muoet', 'etc…', 'nicmar', \"5'4\", 'vajiram', 'zebpay', 'srmjee', 'elitmus', \"5'9\", 'altcoins', 'altcoin', 'hackerrank', \"5'7\", 'awdhesh', \"5'3\", \"5'2\", \"5'5\", 'demonetisation', 'koinex', 'baahubali', \"5'6\", 'mhcet', \"5'8\"]\n",
      "\n",
      "Running procedure on fasttext:\n",
      "   fasttext loaded\n",
      "  999995 word vectors within fasttext dict\n",
      "  Embeddings_matrix created\n",
      "  Shape embedding_matrix: (192333, 300)\n",
      "  Found Embeddings for 71.89% of all words\n",
      "  num_known_words : 138258\n",
      "  num words in word_index:  192332\n",
      "  Unknown Words: 28.11%\n",
      "  Words found by PorterStemmer: 3447\n",
      "  Words found by LancasterStemmer: 4947\n",
      "  Words found by SnowballStemmer: 3114\n",
      "  Words found by Lemmatisation: 142\n",
      "  Top 50 unknown words:\n",
      " [\"''\", \"n't\", 'quorans', 'comedk', 'kvpy', 'quoran', 'wbjee', 'fortnite', 'marksheet', 'oneplus', 'uceed', 'bhakts', 'machedo', 'bnbr', 'chsl', 'naoh', 'josaa', 'alshamsi', 'devops', 'unacademy', 'sjws', \"qur'an\", 'aliexpress', 'zerodha', 'nmat', 'mongodb', 'tensorflow', 'intps', 'hairfall', 'jiit', 'doklam', 'kavalireddi', 'lnmiit', '°c', 'muoet', 'angularjs', 'etc…', 'woocommerce', \"d'angelo\", 'metoo', 'nicmar', \"5'4\", 'vajiram', 'adhaar', 'modiji', 'zebpay', 'srmjee', 'elitmus', 'infjs', \"5'9\"]\n",
      "\n",
      "Running procedure on googlenews:\n",
      "   googlenews loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:154: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Embeddings_matrix created\n",
      "  Shape embedding_matrix: (192333, 300)\n",
      "  Found Embeddings for 39.86% of all words\n",
      "  num_known_words : 76666\n",
      "  num words in word_index:  192332\n",
      "  Unknown Words: 60.14%\n",
      "  Words found by PorterStemmer: 0\n",
      "  Words found by LancasterStemmer: 0\n",
      "  Words found by SnowballStemmer: 0\n",
      "  Words found by Lemmatisation: 0\n",
      "  Top 50 unknown words:\n",
      " ['a', 'to', 'of', 'and', \"'s\", \"''\", 'quora', \"'\", '”', '“', 'instagram', 'upsc', 'bitcoin', 'mbbs', 'whatsapp', 'ece', 'aiims', 'iim', 'sbi', 'cgl', 'cryptocurrency', 'quorans', 'btech', 'snapchat', 'obc', 'jio', 'manipal', 'bba', 'icse', 'tcs', 'srm', 'wwii', 'blockchain', 'narendra', 'elon', 'iiit', 'bitsat', 'cgpa', 'ielts', 'brexit', 'mtech', 'iits', 'cryptocurrencies', 'ncert', 'behaviour', 'programme', 'clat', 'isro', 'upvotes', 'bca']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for embedding in embeddings:\n",
    "    emb_name = embedding['name']\n",
    "    emb_path = embedding['embeddings_path']\n",
    "    print(\"Running procedure on {}:\".format(emb_name))\n",
    "    \n",
    "    load_and_analyse_Embeddings(emb_name, emb_path) # loading embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search certain words in the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove: Matched!\n",
      "AngularJS\n",
      "angularjs\n",
      "paragram: Matched!\n",
      "angularjs\n",
      "fasttext: Matched!\n",
      "AngularJS\n"
     ]
    }
   ],
   "source": [
    "my_re = compile('angularjs', re.I) # ignore case\n",
    "for embedding in words_in_embedding:\n",
    "    if any(my_re.match(line) for line in words_in_embedding[embedding]):\n",
    "        print(\"{}: Matched!\".format(embedding))#\n",
    "\n",
    "    for line in words_in_embedding[embedding]:\n",
    "        if any(my_re.match(word) for word in line.split()):\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are there vectors for Interpunctuation in the embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove: Matched!\n",
      "paragram: Matched!\n",
      "fasttext: Matched!\n",
      "googlenews: Matched!\n"
     ]
    }
   ],
   "source": [
    "re = compile('[,\\.!\"()\"]') # search some interpunctuation marks\n",
    "for embedding in words_in_embedding:\n",
    "    if any(re.match(line) for line in words_in_embedding[embedding]):\n",
    "        print(\"{}: Matched!\".format(embedding))#\n",
    "        \n",
    "# no punctuation in googlenews!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are there any uppercase words in the embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove: Matched!\n",
      "['The', 'It', 'This', 'In', 'You', 'We', 'If', 'And', 'New', 'For']\n",
      "paragram: Matched!\n",
      "['UUUNKKK']\n",
      "fasttext: Matched!\n",
      "['The', 'Wikipedia', 'In', 'UTC', 'If', 'But', 'This', 'It', 'And', 'He']\n",
      "googlenews: Matched!\n",
      "['The', 'It', 'He', 'We', 'In', 'But', 'This', 'They', 'By', 'If']\n"
     ]
    }
   ],
   "source": [
    "re = compile('[A-Z]{1,}\\w+') # match leading upper case letter\n",
    "for embedding in words_in_embedding:\n",
    "        print(\"{}: Matched!\".format(embedding))\n",
    "        print(re.findall(' '.join(words_in_embedding[embedding]))[:10])\n",
    "        \n",
    "# yes there are capitalized letters!\n",
    "# paragram embedding seems not to have capital letters. Only one strange \"UUUNKKK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are there any uppercase only words in the embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove: \n",
      "['PM', 'AM', 'THE', 'US', 'TV', 'UK', 'OF', 'AND', 'USA', 'TO']\n",
      "paragram: \n",
      "['UUUNKKK']\n",
      "fasttext: \n",
      "['UTC', 'US', 'WP', 'UK', 'II', 'TV', 'USA', 'POV', 'EU', 'IP']\n",
      "googlenews: \n",
      "['AP', 'TV', 'UK', 'CEO', 'UN', 'NFL', 'EU', 'IT', 'WASHINGTON', 'NEW']\n"
     ]
    }
   ],
   "source": [
    "re = compile('[A-Z]{2,}') # match uppercase only words\n",
    "            # (at least two following upperacse letters)\n",
    "for embedding in words_in_embedding:\n",
    "        print(\"{}: \".format(embedding))\n",
    "        print(re.findall(' '.join(words_in_embedding[embedding]))[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Having a look at the sentences that contain unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_sre.SRE_Pattern' object has no attribute 'I'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-1227bfbcc5c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_re\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'angularjs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_sre.SRE_Pattern' object has no attribute 'I'"
     ]
    }
   ],
   "source": [
    "my_re = compile('angularjs', re.I)\n",
    "i = 0\n",
    "for line in df[\"question_text\"].values:\n",
    "    if any(my_re.match(word) for word in line.split()) and (i < 5):\n",
    "            print(line)\n",
    "            i += 1   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
