{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Purpose of this kernel - continuance from kernel 6.6\n",
    "\n",
    "* word is it was originally written is tested on embeddings. (not only \"lower()-Version\" of the word) \n",
    "* Preprocessing function was adapted accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3e0dd53f30f4167bd723411f2bb3c1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm, tqdm_notebook \n",
    "tqdm_notebook().pandas()\n",
    "import os\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "# Preprocessing\n",
    "import seaborn as sns\n",
    "import re\n",
    "from re import *\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from gensim.models import KeyedVectors\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer \n",
    "from nltk import pos_tag, word_tokenize\n",
    "from nltk.corpus import wordnet as wn\n",
    "lemmatizer = nltk.WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# Modeling\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding, Flatten, Dense\n",
    "\n",
    "# Training\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "  # splits train-set into into train and validation folds\n",
    "    \n",
    "# Evaluation\n",
    "from keras.callbacks import Callback\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fast Run Testing\n",
    "#total_train_samples = 10000 # max is 1306122\n",
    "#total_test_samples = 2000 # max is 375806\n",
    "total_train_samples = 1306122 # max is 1306122\n",
    "total_test_samples = 375806 # max is 375806\n",
    "\n",
    "# Preprocessing\n",
    "maxlen = 130 # 130 covers about 75% of all bad questions completely\n",
    "\n",
    "# Modeling\n",
    "embedding_dim = 300 # set to 300 to be able to compare with pre-trained embeddings\n",
    "\n",
    "# Training\n",
    "kfolds = 3\n",
    "model_epochs = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"../input/train.csv\")\n",
    "str_ = 'Train data loaded'\n",
    "os.system('echo '+str_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape for this run:  1306122 3\n",
      "Shape data tensor: (1306122,)\n",
      "Shape target tensor: (1306122,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province as a nation in the 1960s?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you encourage people to adopt and not shop?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity affect space geometry?</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid  ...   target\n",
       "0  00002165364db923c7e6  ...        0\n",
       "1  000032939017120e6e44  ...        0\n",
       "2  0000412ca6e4628ce2cf  ...        0\n",
       "\n",
       "[3 rows x 3 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[:total_train_samples] # for Testing purposes\n",
    "num_samples,n = df.shape\n",
    "print(\"Shape for this run: \", num_samples, n)\n",
    "\n",
    "X = df.loc[:, 'question_text'].values\n",
    "y = df.loc[:, 'target'].values\n",
    "\n",
    "# Since Neural Networks are only able to perform transformations on tensors \n",
    "y = np.asarray(y) # Transformation target labels to numpy array \n",
    "\n",
    "print('Shape data tensor:', X.shape) \n",
    "print('Shape target tensor:', y.shape) # 1D Tensor\n",
    "\n",
    "pd.set_option('display.max_colwidth', 1500) # inrease display column size\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (1)  - tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_tokenizer(texts):\n",
    "        tokenizer = Tokenizer()\n",
    "        tokenizer.fit_on_texts(texts) \n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "        padded_seq = pad_sequences(sequences, maxlen=maxlen)  \n",
    "        word_index = tokenizer.word_index  \n",
    "        \n",
    "        return padded_seq, word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (2)  - Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embeddings path\n",
    "_glove = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "_paragram =  '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "_wiki_news = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "_google_news = '../input/embeddings/GoogleNews-vectors-negative300/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "embeddings = [{'name': 'glove', 'embeddings_path': _glove},\n",
    "              {'name': 'paragram', 'embeddings_path': _paragram},\n",
    "              {'name': 'fasttext', 'embeddings_path': _wiki_news},\n",
    "              {'name': 'googlenews', 'embeddings_path': _google_news}\n",
    "             ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition of functions to load and analyse embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for lemmatization from http://textmining.wp.hs-hannover.de/Preprocessing.html\n",
    "\n",
    "def wntag(pttag):\n",
    "    if pttag in ['JJ', 'JJR', 'JJS']:\n",
    "        return wn.ADJ\n",
    "    elif pttag in ['NN', 'NNS', 'NNP', 'NNPS']:\n",
    "        return wn.NOUN\n",
    "    elif pttag in ['RB', 'RBR', 'RBS']:\n",
    "        return wn.ADV\n",
    "    elif pttag in ['VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ']:\n",
    "        return wn.VERB\n",
    "    return None\n",
    "\n",
    "def lemmatize(lemmatizer,word,pos):\n",
    "    if pos == None:\n",
    "        return word\n",
    "    else:\n",
    "        return lemmatizer.lemmatize(word,pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create embedding matrix\n",
    "embedding_matrices = {}\n",
    "words_in_embedding = {}\n",
    "def create_model_embedding_matrix(embeddings_name,word_index,embeddings_dict):\n",
    "\n",
    "    embedding_dim = 300 # (vector size 300!)\n",
    "    embedding_matrix = np.zeros((len(word_index)+1, embedding_dim))\n",
    "    unknown_words_list = []\n",
    "    num_known_words = 0  \n",
    "    \n",
    "    try: \n",
    "        only_words = re.findall(r'\\b[a-zA-Z]{3,9}\\b',' '.join(embeddings_dict.keys()))\n",
    "    except:\n",
    "        only_words = re.findall(r'\\b[a-zA-Z]{3,9}\\b',' '.join(embeddings_dict.wv.vocab))\n",
    "        \n",
    "    word_in_embedding_string = ' '.join(only_words) # for Regex: Only words\n",
    "        \n",
    "    ps = PorterStemmer()\n",
    "    ps_counter = 0\n",
    "    lc = LancasterStemmer()\n",
    "    lc_counter = 0\n",
    "    sb = SnowballStemmer(\"english\")\n",
    "    sb_counter = 0\n",
    "    lemma_counter = 0\n",
    "    re_counter = 0\n",
    "    j = 0\n",
    "\n",
    "    # Filling up matrix\n",
    "    for word, i in tqdm(word_index.items()): \n",
    "        \n",
    "        if embeddings_name in ['glove', 'paragram', 'fasttext']:\n",
    "            \n",
    "            embedding_vector = embeddings_dict.get(word) # get vector for word from embedding \n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                continue # if embedding found - process next word\n",
    "                \n",
    "            word_c = word.lower()\n",
    "            embedding_vector = embeddings_dict.get(word_c)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                continue # if embedding found - process next word\n",
    "                \n",
    "            word_c = word.capitalize()\n",
    "            embedding_vector = embeddings_dict.get(word_c)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                continue # if embedding found - process next word\n",
    "                \n",
    "            word_c = word.upper()\n",
    "            embedding_vector = embeddings_dict.get(word_c)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                continue # if embedding found - process next word\n",
    "   \n",
    "            word_c = ps.stem(word)\n",
    "            embedding_vector = embeddings_dict.get(word_c)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                ps_counter +=1\n",
    "                continue # if embedding found - process next word\n",
    "                \n",
    "            word_c = lc.stem(word)\n",
    "            embedding_vector = embeddings_dict.get(word_c)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                lc_counter +=1\n",
    "                continue # if embedding found - process next word\n",
    "                \n",
    "            word_c = sb.stem(word)\n",
    "            embedding_vector = embeddings_dict.get(word_c)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                sb_counter +=1\n",
    "                continue # if embedding found - process next word\n",
    "                             \n",
    "            word_c = lemmatize(lemmatizer,pos_tag([word])[0][0],wntag(pos_tag([word])[0][1]))\n",
    "            embedding_vector = embeddings_dict.get(word_c)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[i] = embedding_vector\n",
    "                num_known_words +=1\n",
    "                lemma_counter +=1\n",
    "                continue # if embedding found - process next word\n",
    "            \n",
    "            if re.match('^[a-zA-Z]{3,9}$',word):  # if word consists only out of letters\n",
    "                reObj = compile(word, re.I) # ignore case\n",
    "                m = reObj.search(word_in_embedding_string)\n",
    "                if m:\n",
    "                    word_c = m.group(0)\n",
    "                    if j < 5:\n",
    "                        print(\"{:15} --> {}\".format(word, word_c))\n",
    "                        os.system('echo '+ '{:15} --> {}'.format(word, word_c))\n",
    "                        j +=1\n",
    "                    embedding_vector = embeddings_dict.get(word_c)\n",
    "                    if embedding_vector is not None:\n",
    "                        embedding_matrix[i] = embedding_vector\n",
    "                        num_known_words +=1\n",
    "                        re_counter +=1\n",
    "                        continue # if embedding found - process next word               \n",
    "                            \n",
    "            else:\n",
    "                unknown_words_list.append(word)\n",
    "                \n",
    "        if embeddings_name == 'googlenews':\n",
    "            try:\n",
    "                embedding_vector = embeddings_dict[word]  \n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1\n",
    "                    continue # if embedding found - process next word\n",
    "\n",
    "                word_c = word.lower()\n",
    "                embedding_vector = embeddings_dict[word_c]\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1\n",
    "                    continue # if embedding found - process next word\n",
    "                \n",
    "                word_c = word.capitalize()\n",
    "                embedding_vector = embeddings_dict[word_c]\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1 \n",
    "                    continue # if embedding found - process next word\n",
    "                \n",
    "                word_c = word.upper()\n",
    "                embedding_vector = embeddings_dict[word_c]\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1\n",
    "                    continue # if embedding found - process next word\n",
    "                                   \n",
    "                word_c = ps.stem(word)\n",
    "                embedding_vector = embeddings_dict[word_c]\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1\n",
    "                    ps_counter +=1\n",
    "                    continue # if embedding found - process next word\n",
    "                    \n",
    "                word_c = lc.stem(word)\n",
    "                embedding_vector = embeddings_dict[word_c]\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1\n",
    "                    lc_counter +=1\n",
    "                    continue # if embedding found - process next word\n",
    "                    \n",
    "                word_c = sb.stem(word)\n",
    "                embedding_vector = embeddings_dict[word_c]\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1\n",
    "                    sb_counter +=1\n",
    "                    continue # if embedding found - process next word\n",
    "                    \n",
    "                word_c = lemmatize(lemmatizer,pos_tag([word])[0][0],wntag(pos_tag([word])[0][1]))\n",
    "                embedding_vector = embeddings_dict[word_c]\n",
    "                if embedding_vector is not None:\n",
    "                    embedding_matrix[i] = embedding_vector\n",
    "                    num_known_words +=1\n",
    "                    lemma_counter +=1\n",
    "                    continue # if embedding found - process next word\n",
    "                    \n",
    "                if re.match('^[a-zA-Z]{3,9}$',word):     \n",
    "                    reObj = compile(word, re.I) # ignore case\n",
    "                    m = reObj.search(word_in_embedding_string)\n",
    "                    if m:\n",
    "                        word_c = m.group(0)\n",
    "                        if j < 5:\n",
    "                            print(\"{:15} --> {}\".format(word, word_c))\n",
    "                            os.system('echo '+ \"{:15} --> {}\".format(word, word_c))\n",
    "                            j +=1\n",
    "                        embedding_vector = embeddings_dict[word_c]\n",
    "                        if embedding_vector is not None:\n",
    "                            embedding_matrix[i] = embedding_vector\n",
    "                            num_known_words +=1\n",
    "                            re_counter +=1\n",
    "                            continue # if embedding found - process next word               \n",
    "                   \n",
    "            except:\n",
    "                unknown_words_list.append(word)\n",
    "                \n",
    "    try: \n",
    "        words_in_embedding[embeddings_name] = list(embeddings_dict.keys())\n",
    "    except:\n",
    "        try:\n",
    "            words_in_embedding[embeddings_name] = list(embeddings_dict.wv.vocab)\n",
    "        except:\n",
    "            print(\"Error during generation of key list {}\".format(embeddings_name))\n",
    "            print(sys.exc_info()[0])\n",
    "    \n",
    "    print('  Embeddings_matrix created')\n",
    "    print('  Shape embedding_matrix: {}'.format(embedding_matrix.shape))\n",
    "    print('  Found Embeddings for {:.2f}% of all words'\n",
    "          .format((num_known_words / len(word_index))*100))\n",
    "    print(\"  num_known_words :\", num_known_words)\n",
    "    print(\"  num words in word_index: \", len(word_index))\n",
    "    print('  Unknown Words: {:.2f}%'.\n",
    "          format(((len(unknown_words_list)) / len(word_index))*100))\n",
    "    print(\"  Words found by PorterStemmer: {}\".format(ps_counter))\n",
    "    print(\"  Words found by LancasterStemmer: {}\".format(lc_counter))\n",
    "    print(\"  Words found by SnowballStemmer: {}\".format(sb_counter))\n",
    "    print(\"  Words found by Lemmatisation: {}\".format(lemma_counter))\n",
    "    print(\"  Words found by Regex: {}\".format(re_counter))\n",
    "          \n",
    "    # Top 50 unknown words\n",
    "    print(\"  Top 50 unknown words:\\n {}\\n\".format(unknown_words_list[:50]))\n",
    "    \n",
    "    del num_known_words, unknown_words_list,ps,lc,sb, ps_counter, lc_counter, sb_counter\n",
    "    del embedding_matrix, lemma_counter, re_counter, j, word_in_embedding_string, only_words\n",
    "    gc.collect() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load + analyze Embeddings\n",
    "def load_and_analyse_Embeddings(embeddings_name, embeddings_path):\n",
    "    \n",
    "    if embeddings_name in ['glove', 'paragram', 'fasttext']:  \n",
    "        embeddings_dict = {} # create empty embedding dictionary\n",
    "        embedding_file = open(embeddings_path, encoding =\"utf8\", errors = 'ignore') # load embedding from path\n",
    "\n",
    "        # Fill embedding dict with word: vector(coefs) pairs\n",
    "        for line in embedding_file:\n",
    "            line_values = line.split(' ') # read in values of respective line (= vector)\n",
    "            word = line_values[0] #  # first value in line represents the word\n",
    "            coefs = np.asarray(line_values[1:], dtype='float32') # all values represent vector\n",
    "            embeddings_dict[word] = coefs # add key(word), value(vector) pairs to dict\n",
    "\n",
    "        embedding_file.close() \n",
    "        \n",
    "        os.system('echo '+ embeddings_name + 'loaded')\n",
    "        print('  ',embeddings_name, 'loaded')\n",
    "        print('  {} word vectors within {} dict'.format(len(embeddings_dict),embeddings_name))\n",
    "        \n",
    "        # Use pre-trained embedding to create final embeddings matrix\n",
    "        create_model_embedding_matrix(embeddings_name,word_index,embeddings_dict)\n",
    "        del embeddings_dict, line_values,word,coefs\n",
    "                \n",
    "    if embeddings_name == 'googlenews':\n",
    "        embeddings_file = KeyedVectors.load_word2vec_format(embeddings_path, binary=True)\n",
    "        \n",
    "        os.system('echo '+ embeddings_name + 'loaded')\n",
    "        print('  ',embeddings_name, 'loaded')\n",
    "        \n",
    "        # Use pre-trained embedding to create final embeddings matrix\n",
    "        create_model_embedding_matrix(embeddings_name,word_index,embeddings_file)\n",
    "        del embeddings_file\n",
    "        \n",
    "    # MEMORY MANAGEMENT!\n",
    "    del embeddings_name, embeddings_path\n",
    "    gc.collect()\n",
    "    \n",
    "   # return embeddings_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation (3)  - Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Definition mapping and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \n",
    "                       \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \n",
    "                       \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \n",
    "                       \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \n",
    "                       \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \n",
    "                       \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\n",
    "                       \"I'm\": \"I am\",\"i'm\": \"i am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
    "                       \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                       \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \n",
    "                       \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \n",
    "                       \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\n",
    "                       \"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \n",
    "                       \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \n",
    "                       \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \n",
    "                       \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \n",
    "                       \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\n",
    "                       \"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n",
    "                       \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \n",
    "                       \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\n",
    "                       \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                       \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                       \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \n",
    "                       \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \n",
    "                       \"weren't\": \"were not\",\"what`s\": \"what is\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \n",
    "                       \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                       \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                       \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                       \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \n",
    "                       \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \n",
    "                       \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "                       \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\n",
    "                       \"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \n",
    "                       \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n",
    "                       \"you're\": \"you are\", \"you've\": \"you have\"}\n",
    "\n",
    "# dict from https://www.kaggle.com/theoviel/improve-your-score-with-text-preprocessing-v2 \n",
    "correct_spell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite',\n",
    "                    'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater',\n",
    "                    'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization',\n",
    "                    'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ',\n",
    "                    'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What',\n",
    "                    'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are',\n",
    "                    'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many',\n",
    "                    'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best',\n",
    "                    'howdoes': 'how does', 'mastrubation': 'masturbation',\n",
    "                    'mastrubate': 'masturbate', \"mastrubating\": 'masturbating',\n",
    "                    \"mcdonald's\":'mcdonalds',\n",
    "                    'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist',\n",
    "                    'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', \n",
    "                    'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what',\n",
    "                    'watsapp': 'whatsapp', 'demonitisation': 'demonetization',\n",
    "                    'demonitization': 'demonetization', 'demonetisation': 'demonetization',\n",
    "                    'pokémon': 'pokemon', 'quoras': 'quora', 'quorans': 'quora'}\n",
    "\n",
    "# Kernel \"fork-embeddings-keras-v04\"\n",
    "specials_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \n",
    "                 \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', \n",
    "                 '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', \n",
    "                 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', '\\u200b': ' ',\n",
    "                 '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': '', 'ε−': ''}\n",
    "\n",
    "punct = \"/-?!.,#$%\\()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_numbers(x):\n",
    "    # replaces one digit by #, two following digits by ## etc.\n",
    "    x = re.sub('[0-9]{5,}', '#####', x) \n",
    "    x = re.sub('[0-9]{4}', '####', x)\n",
    "    x = re.sub('[0-9]{3}', '###', x)\n",
    "    x = re.sub('[0-9]{2}', '##', x)\n",
    "    return x\n",
    "\n",
    "def preprocessing(x):\n",
    "    x = str(x)\n",
    "    x = re.sub('[’‘´`]', \"'\", x) \n",
    "    \n",
    "    for word in x.split():\n",
    "        if word.lower() in contraction_mapping.keys():\n",
    "            x = x.replace(word, contraction_mapping[word.lower()])\n",
    "        if word in correct_spell_dict.keys():\n",
    "            x = x.replace(word, correct_spell_dict[word])\n",
    "        if word in specials_mapping.keys():\n",
    "            x = re.sub(word, specials_mapping.get(word),x) \n",
    "            \n",
    "    x = re.sub('\\'s\\s+', ' \\'s ', x)\n",
    "    x = ' '.join(word_tokenize(x))    \n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a456918e2e0341b3bba9905d8a072dd5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1306122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf3d2732d8644c139e0bb37f55a9ce86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1306122), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('echo Applying preprocessing functions..')\n",
    "df[\"question_text\"] = df[\"question_text\"].fillna(\"_nan_\").progress_apply(lambda x: preprocessing(x))\n",
    "os.system('echo prepocessing done')\n",
    "df[\"question_text\"] = df[\"question_text\"].progress_apply(lambda x: clean_numbers(x))\n",
    "os.system('echo clean_numbers done')\n",
    "\n",
    "X = df.loc[:, 'question_text'].values\n",
    "y = np.asarray(df.loc[:, 'target'].values)\n",
    "\n",
    "padded_seq, word_index = my_tokenizer(X) # Tokenization\n",
    "os.system('echo Tokenization 2 completed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word length distribution (for Regex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/scipy/stats/stats.py:1713: FutureWarning: Using a non-tuple sequence for multidimensional indexing is deprecated; use `arr[tuple(seq)]` instead of `arr[seq]`. In the future this will be interpreted as an array index, `arr[np.array(seq)]`, which will result either in an error or a different result.\n",
      "  return np.add.reduce(sorted[indexer] * weights, axis=axis) / sumval\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f416b791fd0>"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "l = []\n",
    "for key, _ in tqdm(word_index.items()):\n",
    "    l.append(len(key))  \n",
    "sns.distplot(l, bins = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Iteration loop to compare different embeddings (3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running procedure on glove:\n",
      "   glove loaded\n",
      "  2196016 word vectors within glove dict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3665/192332 [00:02<01:52, 1680.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redmi           --> Redmi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 7788/192332 [00:02<01:18, 2341.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oneplus         --> oneplus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8574/192332 [00:03<01:11, 2588.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upwork          --> upwork\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 15895/192332 [00:07<03:45, 782.74it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftre            --> ftre\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 16192/192332 [00:07<03:02, 963.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hotstar         --> HotStar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192332/192332 [51:55<00:00, 61.74it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Embeddings_matrix created\n",
      "  Shape embedding_matrix: (192333, 300)\n",
      "  Found Embeddings for 77.99% of all words\n",
      "  num_known_words : 149998\n",
      "  num words in word_index:  192332\n",
      "  Unknown Words: 10.50%\n",
      "  Words found by PorterStemmer: 3434\n",
      "  Words found by LancasterStemmer: 4642\n",
      "  Words found by SnowballStemmer: 3846\n",
      "  Words found by Lemmatisation: 79\n",
      "  Words found by Regex: 562\n",
      "  Top 50 unknown words:\n",
      " ['cryptocurrencies', '₹', \"'i\", 'adityanath', \"'a\", \"qur'an\", \"5'\", 'tensorflow', 'kavalireddi', '°c', 'etc…', \"5'4\", \"5'9\", 'hackerrank', \"5'7\", \"5'3\", \"5'2\", \"5'5\", 'demonetisation', \"5'6\", \"5'8\", 'x²', 'nanodegree', 'what\\u200b', 'microservices', '√3', 'truecaller', '√2', 'chromecast', 'dies™', \"6'1\", 'bitconnect', \"'r\", \"6'2\", 'xxxtentacion', \"'x\", 'kmno4', 'codeforces', 'arrowverse', 'internshala', 'chapterwise', 'genderfluid', 'ravindrababu', 'kubernetes', 'undergraduation', \"6'4\", 'hackerearth', '√x', 'wikitribune', 'how\\u200b']\n",
      "\n",
      "Running procedure on paragram:\n",
      "   paragram loaded\n",
      "  1703755 word vectors within paragram dict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 3665/192332 [00:00<00:19, 9832.24it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "redmi           --> redmi\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8371/192332 [00:00<00:21, 8449.06it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oneplus         --> oneplus\n",
      "upwork          --> upwork\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 16019/192332 [00:04<02:04, 1416.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ftre            --> ftre\n",
      "hotstar         --> hotstar\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192332/192332 [36:10<00:00, 88.59it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Embeddings_matrix created\n",
      "  Shape embedding_matrix: (192333, 300)\n",
      "  Found Embeddings for 79.14% of all words\n",
      "  num_known_words : 152208\n",
      "  num words in word_index:  192332\n",
      "  Unknown Words: 10.05%\n",
      "  Words found by PorterStemmer: 4216\n",
      "  Words found by LancasterStemmer: 5275\n",
      "  Words found by SnowballStemmer: 3988\n",
      "  Words found by Lemmatisation: 77\n",
      "  Words found by Regex: 0\n",
      "  Top 50 unknown words:\n",
      " ['cryptocurrencies', '₹', \"'i\", 'adityanath', \"'a\", \"qur'an\", \"5'\", 'tensorflow', 'kavalireddi', '°c', 'etc…', \"5'4\", \"5'9\", 'hackerrank', \"5'7\", \"5'3\", \"5'2\", \"5'5\", 'demonetisation', \"5'6\", \"5'8\", 'x²', 'nanodegree', 'what\\u200b', 'microservices', '√3', '√2', 'chromecast', 'dies™', \"6'1\", 'bitconnect', \"'r\", \"6'2\", 'xxxtentacion', \"'x\", 'codeforces', 'arrowverse', 'internshala', 'chapterwise', 'genderfluid', 'ravindrababu', 'kubernetes', 'undergraduation', \"6'4\", 'hackerearth', '√x', 'wikitribune', 'how\\u200b', \"''the\", \"i'am\"]\n",
      "\n",
      "Running procedure on fasttext:\n",
      "   fasttext loaded\n",
      "  999995 word vectors within fasttext dict\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 7353/192332 [00:00<00:15, 11828.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "oneplus         --> OnePlus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 8256/192332 [00:01<00:36, 5015.60it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uceed           --> uceed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 8953/192332 [00:01<00:37, 4830.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chsl            --> chsl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 9574/192332 [00:01<01:14, 2452.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "naoh            --> NaOH\n",
      "devops          --> DevOps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 192332/192332 [38:18<00:00, 83.67it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Embeddings_matrix created\n",
      "  Shape embedding_matrix: (192333, 300)\n",
      "  Found Embeddings for 72.36% of all words\n",
      "  num_known_words : 139168\n",
      "  num words in word_index:  192332\n",
      "  Unknown Words: 11.93%\n",
      "  Words found by PorterStemmer: 3447\n",
      "  Words found by LancasterStemmer: 4947\n",
      "  Words found by SnowballStemmer: 3114\n",
      "  Words found by Lemmatisation: 142\n",
      "  Words found by Regex: 910\n",
      "  Top 50 unknown words:\n",
      " [\"''\", \"n't\", \"qur'an\", 'aliexpress', 'tensorflow', 'kavalireddi', '°c', 'etc…', 'woocommerce', \"d'angelo\", \"5'4\", \"5'9\", 'hackerrank', \"5'7\", \"5'3\", \"5'2\", \"5'5\", \"5'6\", \"5'8\", 'x²', 'nanodegree', 'what\\u200b', '√3', '√2', \"o'reilly\", 'dies™', \"6'1\", 'bitconnect', 'multibagger', \"6'2\", 'xxxtentacion', 'kmno4', 'codeforces', 'electroneum', 'internshala', 'chapterwise', 'ravindrababu', \"baha'i\", 'undergraduation', \"don'ts\", '1z0', \"6'4\", 'hackerearth', 'netbanking', 'sin2x', '√x', 'how\\u200b', 'bookmyshow', \"o'brien\", \"i'am\"]\n",
      "\n",
      "Running procedure on googlenews:\n",
      "   googlenews loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "100%|██████████| 192332/192332 [00:00<00:00, 232523.90it/s]\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:195: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Embeddings_matrix created\n",
      "  Shape embedding_matrix: (192333, 300)\n",
      "  Found Embeddings for 39.86% of all words\n",
      "  num_known_words : 76666\n",
      "  num words in word_index:  192332\n",
      "  Unknown Words: 60.14%\n",
      "  Words found by PorterStemmer: 0\n",
      "  Words found by LancasterStemmer: 0\n",
      "  Words found by SnowballStemmer: 0\n",
      "  Words found by Lemmatisation: 0\n",
      "  Words found by Regex: 0\n",
      "  Top 50 unknown words:\n",
      " ['a', 'to', 'of', 'and', \"'s\", \"''\", 'quora', \"'\", '”', '“', 'instagram', 'upsc', 'bitcoin', 'mbbs', 'whatsapp', 'ece', 'aiims', 'iim', 'sbi', 'cgl', 'cryptocurrency', 'quorans', 'btech', 'snapchat', 'obc', 'jio', 'manipal', 'bba', 'icse', 'tcs', 'srm', 'wwii', 'blockchain', 'narendra', 'elon', 'iiit', 'bitsat', 'cgpa', 'ielts', 'brexit', 'mtech', 'iits', 'cryptocurrencies', 'ncert', 'behaviour', 'programme', 'clat', 'isro', 'upvotes', 'bca']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for embedding in embeddings:\n",
    "    emb_name = embedding['name']\n",
    "    emb_path = embedding['embeddings_path']\n",
    "    print(\"Running procedure on {}:\".format(emb_name))\n",
    "    \n",
    "    load_and_analyse_Embeddings(emb_name, emb_path) # loading embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Search certain words in the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove: Matched!\n",
      "AngularJS\n",
      "angularjs\n",
      "paragram: Matched!\n",
      "angularjs\n",
      "fasttext: Matched!\n",
      "AngularJS\n"
     ]
    }
   ],
   "source": [
    "my_re = compile('angularjs', re.I) # ignore case\n",
    "for embedding in words_in_embedding:\n",
    "    if any(my_re.match(line) for line in words_in_embedding[embedding]):\n",
    "        print(\"{}: Matched!\".format(embedding))#\n",
    "\n",
    "    for line in words_in_embedding[embedding]:\n",
    "        if any(my_re.match(word) for word in line.split()):\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are there vectors for Interpunctuation in the embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove: Matched!\n",
      "paragram: Matched!\n",
      "fasttext: Matched!\n",
      "googlenews: Matched!\n"
     ]
    }
   ],
   "source": [
    "re = compile('[,\\.!\"()\"]') # search some interpunctuation marks\n",
    "for embedding in words_in_embedding:\n",
    "    if any(re.match(line) for line in words_in_embedding[embedding]):\n",
    "        print(\"{}: Matched!\".format(embedding))#\n",
    "        \n",
    "# no punctuation in googlenews!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are there any uppercase words in the embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove: Matched!\n",
      "['The', 'It', 'This', 'In', 'You', 'We', 'If', 'And', 'New', 'For']\n",
      "paragram: Matched!\n",
      "['UUUNKKK']\n",
      "fasttext: Matched!\n",
      "['The', 'Wikipedia', 'In', 'UTC', 'If', 'But', 'This', 'It', 'And', 'He']\n",
      "googlenews: Matched!\n",
      "['The', 'It', 'He', 'We', 'In', 'But', 'This', 'They', 'By', 'If']\n"
     ]
    }
   ],
   "source": [
    "re = compile('[A-Z]{1,}\\w+') # match leading upper case letter\n",
    "for embedding in words_in_embedding:\n",
    "        print(\"{}: Matched!\".format(embedding))\n",
    "        print(re.findall(' '.join(words_in_embedding[embedding]))[:10])\n",
    "        \n",
    "# yes there are capitalized letters!\n",
    "# paragram embedding seems not to have capital letters. Only one strange \"UUUNKKK\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Are there any uppercase only words in the embeddings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "glove: \n",
      "['PM', 'AM', 'THE', 'US', 'TV', 'UK', 'OF', 'AND', 'USA', 'TO']\n",
      "paragram: \n",
      "['UUUNKKK']\n",
      "fasttext: \n",
      "['UTC', 'US', 'WP', 'UK', 'II', 'TV', 'USA', 'POV', 'EU', 'IP']\n",
      "googlenews: \n",
      "['AP', 'TV', 'UK', 'CEO', 'UN', 'NFL', 'EU', 'IT', 'WASHINGTON', 'NEW']\n"
     ]
    }
   ],
   "source": [
    "re = compile('[A-Z]{2,}') # match uppercase only words\n",
    "            # (at least two following upperacse letters)\n",
    "for embedding in words_in_embedding:\n",
    "        print(\"{}: \".format(embedding))\n",
    "        print(re.findall(' '.join(words_in_embedding[embedding]))[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Having a look at the sentences that contain unknown words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'_sre.SRE_Pattern' object has no attribute 'I'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-81-1c8e1d205285>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_re\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'angularjs'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mI\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# ignore case\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmy_re\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: '_sre.SRE_Pattern' object has no attribute 'I'"
     ]
    }
   ],
   "source": [
    "my_re = compile('angularjs', re.I) # ignore case\n",
    "for line in df[\"question_text\"].values:\n",
    "    if any(my_re.match(word) for word in line.split()) and (i < 5):\n",
    "            print(line)\n",
    "            i += 1   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
