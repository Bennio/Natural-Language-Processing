{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation für Natural Language Processing (NLP)\n",
    "\n",
    "- Dieses Noteook enthält grundlegende Techniken für die Vorverarbeitung von Daten in Textform.\n",
    "- Darstellungen unter Einsatz der populären NLP-Bibliothek [nltk](https://www.nltk.org/) (Installation und Download erforderlich).\n",
    "- Weitere verbreitete Bibliotheken sind [spacy](https://spacy.io/), [gensim](https://radimrehurek.com/gensim/) und [TextBlob](https://textblob.readthedocs.io/en/dev/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenisierung und n-Gramme\n",
    "\n",
    "\n",
    "- Tokenisierung bezeichnet die Zerlegung eines Satzes in seine Bestandteile.\n",
    "- Mögliche Zerlegungsmethoden sind:\n",
    "  - Unigramme (Ein Wort)\n",
    "  - Bigramme (Zwei Wörter)\n",
    "  - Trigramme (Drei Wörter)\n",
    "  - allgemein spricht man von: n-Grammen (n Wörter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams:  ['The', 'United', 'States', 'is', 'a', 'country', '.']\n",
      "Bigrams:  [('The', 'United'), ('United', 'States'), ('States', 'is'), ('is', 'a'), ('a', 'country'), ('country', '.')]\n",
      "\n",
      "Meaningful Bigram:  ('United', 'States')\n"
     ]
    }
   ],
   "source": [
    "import nltk # library to support NLP tasks\n",
    "from nltk import word_tokenize # splits sentences in words/tokens\n",
    "\n",
    "# Tokenization in unigrams\n",
    "my_sentence = \"The United States is a country.\"\n",
    "words = word_tokenize(my_sentence)\n",
    "print(\"Unigrams: \", words)\n",
    "\n",
    "# Tokenization in bigrams\n",
    "my_sentence = \"The United States is a country.\"\n",
    "words = word_tokenize(my_sentence)\n",
    "bigrams = list(nltk.bigrams(words))\n",
    "print(\"Bigrams: \",bigrams )\n",
    "print(\"\\nMeaningful Bigram: \",  bigrams[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PoS Tagging\n",
    "\n",
    "- PoS = Parts of Speech\n",
    "- Beim PoS Tagging wird jedem Wort (bzw. Interpunktionszeichen) eines Satzes eine Wortart zugewiesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PoS tagged my_sentence: \n",
      " [('The', 'DT'), ('United', 'NNP'), ('States', 'NNPS'), ('are', 'VBP'), ('a', 'DT'), ('country', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "my_sentence = \"The United States are a country.\"\n",
    "words = word_tokenize(my_sentence)\n",
    "pos_words = nltk.pos_tag(words) # PoS Tagging\n",
    "print(\"PoS tagged my_sentence: \\n\", pos_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erklärung zu den Wortarten:\n",
    "    - DT: determiner (Bestimmungswort)\n",
    "    - NNP: proper noun, singular (korrekt geschriebenes Substantiv im Singular)\n",
    "    - NNPS: Proper noun, plural (korrekt geschriebenes Substantiv im Plural)\n",
    "    - VBP: verb present (Verb in Zeitfrom Präsens)\n",
    "    - DT: determiner (Bestimmungswort)\n",
    "    - NN: noun, singular or mass (Substantiv)\n",
    "\n",
    "https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stoppwörter\n",
    "\n",
    "- Stopwörter sind häufig in einem Satz auftretende Wörter mit wenig Bedeutungsinhalt\n",
    "- Stopwörter kennzeichnet die Eigenschaft, dass sie eine geringe Relevanz für das Verständnis eines Satzes besitzen.\n",
    "- Die Entfernung von Stopwörtern aus Texten ist ein Standard-Preprocessing Schritt.\n",
    "- Mögliche Stopwörter im Englischen sind beispielsweise \"a\", \"am\" und \"the\"\n",
    "- Stopwörter werden entfernt, weil sie wenig/keinen positiven Einfluss auf die Leistung eines Machine Learning Modells zur Textklassifikation besitzen. \n",
    "- Insbesondere wenn klassische Modelle wie SVM oder Naive Bayes zum Einsatz kommen, sollten Stopwörter entfernt werden. Machine Learning Modelle basierend auf Neuronalen Netzen sind in der Lage die für die jeweilige Aufgabe relevanten Features selbstständig zu identifizieren. Eine Entfernung von Stopwörter ist nicht zwingend erforderlich.\n",
    "- Die Entfernung von Stopwörtern reduziert darüber hinaus die zu verarbeitende Datenmenge erheblich. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 stopwords in english stopword list : \n",
      " ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n",
      "\n",
      "Orginial Tokens : ['The', 'United', 'States', 'is', 'a', 'country', '.']\n",
      "\n",
      "Tokens without stopwords : ['The', 'United', 'States', 'country', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords # nltk provides stop words lists\n",
    "\n",
    "english_stopwords = stopwords.words('English') # select english stopword list\n",
    "print(\"First 10 stopwords in english stopword list : \\n\", english_stopwords[:10])\n",
    "\n",
    "my_sentence = \"The United States is a country.\"\n",
    "words = word_tokenize(my_sentence)\n",
    "\n",
    "# Filter out stopwords from my_sentence which are contained in stopword list\n",
    "words_wo_stops = [word for word in words if word not in english_stopwords]\n",
    "\n",
    "print(\"\\nOrginial Tokens :\", words)\n",
    "print(\"\\nTokens without stopwords :\", words_wo_stops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokens \"is\" und \"a\" wurden aus dem Satz entfernt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"is\" and \"a\" in english_stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Normalisierung\n",
    "\n",
    "- Im NLP-Kontext wird von Normalisierung gesprochen, wenn verschiedene Variationen eines Wortes oder Ausdrucks existieren und diese auf eine einheitliche Form zurückgeführt werden.\n",
    "- Eine mögliche Variation stellt die Konjugation von Verben dar. Englische Wörter wie \"doing\" oder \"does\" werden durch Normalisierung auf die Stammform \"do\" reduziert.\n",
    "- Auch Abkürzungen wie \"US\" werden durch Normalisierung in den Ländernamen \"United States\" transformiert.\n",
    "<br><br>\n",
    "- Es existieren verschiedene Normalisierungstechniken für Text. Bekannte Methoden sind: \n",
    "  - <b>Korrektur von Rechtschreibung</b>\n",
    "  - <b>Stemming (Stammformreduktion)</b>\n",
    "  - <b>Lemmatisierung</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Sentence:  The United States is a country.\n"
     ]
    }
   ],
   "source": [
    "# Simple text normalization: change US to United States\n",
    "my_sentence = \"The US is a country.\"\n",
    "my_normalized_sentence = my_sentence.replace(\"US\", \"United States\")\n",
    "print(\"Normalized Sentence: \", my_normalized_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rechtschreibkorrektur\n",
    "\n",
    "- Die Korrektur von Rechtschreibung stellt einen aufwendigen Prozess in der Datenvorverarbeitung von Text dar.\n",
    "- Wenn für das Verständnis eines Satzes wichtige Wörter falsch geschrieben wurden, besteht die Gefahr, dass diese nicht ins spätere Machine Learning Modell überführt werden können (vgl. Embeddings). Dies würde einen Informationsverlust im Preprocessing darstellen und sollte vermieden werden.\n",
    "- Die Python Bibliothek [autocorrect](https://github.com/phatpiglet/autocorrect/) unterstützt das Korrigieren von Rechtschreibung in begrenztem Umfang."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install autocorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Words:  ['HTe', 'Uited', 'States', 'is', 'a', 'contry', '.']\n",
      "Autocorrected Words:  ['The', 'United', 'States', 'is', 'a', 'country', 'a']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "from autocorrect import spell\n",
    "\n",
    "my_sentence = \"HTe Uited States is a contry.\" # Spelling errors in \"Uited\" and \"contry\"\n",
    "words = word_tokenize(my_sentence)\n",
    "print(\"Original Words: \", words)\n",
    "\n",
    "autocorrected_words =  [spell(word) for word in words]\n",
    "print(\"Autocorrected Words: \", autocorrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrong Correction to 'Count' instead of 'Country'.\n"
     ]
    }
   ],
   "source": [
    "# Autocorrect is not always working as expected!\n",
    "spell_error_word = \"Countr\" \n",
    "print(\"Wrong Correction to '{}' instead of 'Country'.\".format(spell(spell_error_word)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stemming (Stammformreduktion)\n",
    "\n",
    "-  Stemming bezeichnet ein Verfahren, mit welchem verschiedene morphologische Varianten eines  Wortes auf ihre gemeinsame Stammform zurückführt werden. Die Idee ist, dass die lexikalische Bedeutung eines Wortes in seinem Stamm zu finden ist.\n",
    "-  Für Verben stellt der Infinitiv die Stammform dar.\n",
    "- Durch Stemming können auch Nomen wie \"Production\" und \"Products\" auf die Stammform \"Product\" reduziert werden.\n",
    "<br><br>\n",
    "-  Für die Stammformreduktion existieren verschiedene Stemmer-Implementierungen. Populäre Stemmmer:\n",
    "  - PorterStemmer (ältester Stemmer)\n",
    "  - SnowballStemmer (Porter 2 - Weiterentwicklung PorterStemmer)\n",
    "  - LancasterStemmer (Paice-Husk-Stemmer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'run'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "porter_stemmer = nltk.stem.PorterStemmer()\n",
    "porter_stemmer.stem(\"running\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'car'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.snowball import SnowballStemmer\n",
    "snowball_stemmer = SnowballStemmer('english') # Stemmer with english language support\n",
    "snowball_stemmer.stem(\"car's\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Beim Stemming besteht die Gefahr des sogenannten Over- und Under-Stemming.\n",
    "-  Beim Over-stemming werden zu viele Bestandteile eines Wortes entfernt, was dazu führen kann, dass zwei unterschiedliche Wörter auf die gleiche Stammform reduziert werden. Es besteht die Gefahr, dass ein Satz durch Stemming eine völlig neue Bedeutung erhält.  \n",
    "- Beim Under-stemming werden nicht zu viel, sondern zu wenig Buchstaben eines Wortes entfernt. Dies führt dazu, dass die korrekte Stammform nicht gebildet werden kann und ein grammatikalisch falsches Grundwort entsteht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over-stemming:  ['organs', 'organization'] -> ['organ', 'organ']\n",
      "Under-stemming:  ['absorption', 'absorbing'] -> ['absorpt', 'absorb']\n"
     ]
    }
   ],
   "source": [
    "snowball_stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Over-stemming \n",
    "words = [\"organs\", \"organization\"]\n",
    "print(\"Over-stemming: \",words, \"->\",[snowball_stemmer.stem(word) for word in words])\n",
    "\n",
    "# Under-stemming\n",
    "words = [\"absorption\", \"absorbing\"]\n",
    "print(\"Under-stemming: \",words, \"->\",[snowball_stemmer.stem(word) for word in words])\n",
    "\n",
    "# Examples based on https://www.intrafind.de/blog/the-difference-between-stemming-and-lemmatization-en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  Es existieren verschiedene Arten von Stemmern, welche auf unterschiedliche Weise versuchen die Grundform eines Wortes abzuleiten. So setzt der LancasterStemmer beispielsweise auf Affix Removal, welches die am weitesten verbreitete Stemming-Technik ist. Mögliche Affixe von Wörtern wurden hierfür explizit im [Quellcode des LancasterStemmers](https://www.nltk.org/_modules/nltk/stem/lancaster.html) definiert. Das zu \"stemmende\" Wort wird anschließend auf die definierten Affixe geprüft und bei einem Treffer wird der entsprechende Affix entfernt.\n",
    "-  Ein Nachteil von Stemming ist, dass die erzeugten Stammformen nicht zwingend wirklich existierende Wörter sein müssen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "troubl\n",
      "dest\n",
      "footbal\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lancaster_stemmer = LancasterStemmer()\n",
    "\n",
    "print(lancaster_stemmer.stem('trouble')) # word does not exists -> lemmatization\n",
    "print(lancaster_stemmer.stem('destabilize')) # word does not exists -> lemmatization\n",
    "print(lancaster_stemmer.stem('football')) # word does not exists -> lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatisierung\n",
    "\n",
    "- Lemmatisierung zielt darauf ab die grammatikalisch korrekte Form eines Wortes zu bilden. Die durch Lemmatisierung erzeugten Wörter werden Lemmata genannt.\n",
    "- Lemmatisierung kann insbesondere beim Einsatz von Stemming sinnvoll sein, da bei der Stammformreduktion nicht existierende Wörter enstehen können."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trouble\n",
      "destabilize\n",
      "football \n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "print(lemmatizer.lemmatize('trouble')) \n",
    "print(lemmatizer.lemmatize('destabilize'))\n",
    "print(lemmatizer.lemmatize('football '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named Entities Recognition (NER) \n",
    "\n",
    "-  auf Deutsch: Eigennamenerkennung\n",
    "- IM Unterschied zum Part of Speech Tagging, nimmt sich Named entities recognition dem Problem in einem Wörterbuch nicht existierender Wöter an. Dies betrifft inbesondere Eigennamen von z.B. Personen oder Ortsangaben\n",
    "- Named entities werden, sofern sie erkannt werden, bestimmten Kategorien zugeordnet.\n",
    "-  Die einfachste Kategorisierung stellt die Kennzeichnung als Named Entity ([Kategorie \"NE'](https://www.nltk.org/book/ch07.html)' dar).\n",
    "- Ausgehend von dieser Kategorie ist eine feinere Klassifizierung von Wörtern in Kategorien wie Personen, Orte, Organisationen oder auch Währungssymbole möglich.\n",
    "- Im folgenden Beispiel werden PoS Tagging und Named Entites Recognition gegenübergestellt. Der Firmenname \"BASF\" sowie die Stadtbezeichnung \"Ludwigshafen\" werden vom [chunk package der nltk-Bibliothek](https://www.nltk.org/api/nltk.chunk.html)  erkannt und als 'NE' kategorisiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('maxent_ne_chunker')\n",
    "# nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "PoS Tokens: \n",
      "('BASF', 'NNP')\n",
      "('is', 'VBZ')\n",
      "('a', 'DT')\n",
      "('company', 'NN')\n",
      "('located', 'VBN')\n",
      "('in', 'IN')\n",
      "('Ludwigshafen', 'NNP')\n",
      "\n",
      "Named entities: \n",
      "(S\n",
      "  (NE BASF/NNP)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  company/NN\n",
      "  located/VBN\n",
      "  in/IN\n",
      "  (NE Ludwigshafen/NNP))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "my_sentence = \"BASF is a company located in Ludwigshafen\"\n",
    "words = word_tokenize(my_sentence)\n",
    "pos_words = nltk.pos_tag(words)\n",
    "print(\"\\nPoS Tokens: \")\n",
    "[print(w) for w in pos_words]\n",
    "\n",
    "named_entites = nltk.ne_chunk(pos_words, binary = True) # if binary = true, Named entites are tagges as 'NE'\n",
    "print(\"\\nNamed entities: \")\n",
    "print(named_entites)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Sense disambiguation (WSD)\n",
    "\n",
    "- auf Deutsch: Disambiguierung\n",
    "- Word Sense disambiuation bezeichnet die Auflösung von Mehrdeutigkeiten bei der Verarbeitung natürlicher Sprache.\n",
    "- Einzelne Wörter, Satzteile oder ganze Sätze können trotz gleicher Schreibweise je nach Kontext unterschiedliches bedeuten. So kann das Wort \"bank\" je nach Kontext sowohl eine Flussufer als auch ein Kreditinstitut bezeichnen.\n",
    "- Für Menschen stellen solche Mehrdeutigkeiten meist kein Problem dar. Bei der maschinellen Sprachverarbeitung stellt Disambiguierung eine große Herausforderung dar.\n",
    "- Grundsätzlich helfen bei der Auflösung von Mehrdeutigkeiten linguistische und kontextuelle Informationen sowie Weltwissen.\n",
    "- Einen klassischer Algorithmus zur Erkennung von Ambiguität stellt der [Lesk Algorithmus](https://www.nltk.org/howto/wsd.html) dar, welcher in nltk implementiert ist. Der Algorithmus untersucht den Kontext, in welchem ein mehrdeutiges Wort auftaucht. Die Kontextwörter werden  mit den verschiedenen Synsets, die für das gesuchte Worte existieren verglichen. Das ambiguitäre Wort wird abschließend dem Synset zugeordnet, bei welchem die größten Übereinstimmungen gefunden wurden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('savings_bank.n.02')\n",
      "Synset('bank.v.07')\n",
      "Synset('mouse.v.02')\n",
      "Synset('mouse.v.02')\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.wsd import lesk\n",
    "from nltk import word_tokenize\n",
    "\n",
    "sentence1 = \"Keep your savings in the bank\" # credit institution\n",
    "sentence2 = \"It's so risky to drive over the banks of the road\" # river bank\n",
    "print(lesk(word_tokenize(sentence1), 'bank')) # synset correct\n",
    "print(lesk(word_tokenize(sentence2), 'bank')) # synset correct\n",
    "\n",
    "sentence3 = \"There is a tiny mouse in the basement!\"  # animal mouse\n",
    "sentence4 = \"I need new batteries for my mouse\"  # computer mouse\n",
    "print(lesk(word_tokenize(sentence3), 'mouse')) # synset wrong \n",
    "print(lesk(word_tokenize(sentence4), 'mouse')) # synset correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Existierende Synsets für die Wörter 'bank' und 'mouse'\n",
    "\n",
    "Code adapted from: https://proquest.techbus.safaribooksonline.de/book/programming/python/9781484243541/8dot-semantic-analysis/427287_2_en_8_chapter_xhtml?query=((Synset))+AND+(PUBDATEYEAR%3d2019)#snippet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of synsets for the word 'bank':  18\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Synset</th>\n",
       "      <th>Part of Speech</th>\n",
       "      <th>Definition</th>\n",
       "      <th>Lemmas</th>\n",
       "      <th>Examples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Synset('bank.n.01')</td>\n",
       "      <td>noun.object</td>\n",
       "      <td>sloping land (especially the slope beside a bo...</td>\n",
       "      <td>[bank]</td>\n",
       "      <td>[they pulled the canoe up on the bank, he sat ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Synset('depository_financial_institution.n.01')</td>\n",
       "      <td>noun.group</td>\n",
       "      <td>a financial institution that accepts deposits ...</td>\n",
       "      <td>[depository_financial_institution, bank, banki...</td>\n",
       "      <td>[he cashed a check at the bank, that bank hold...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Synset('bank.n.03')</td>\n",
       "      <td>noun.object</td>\n",
       "      <td>a long ridge or pile</td>\n",
       "      <td>[bank]</td>\n",
       "      <td>[a huge bank of earth]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Synset Part of Speech  \\\n",
       "0                              Synset('bank.n.01')    noun.object   \n",
       "1  Synset('depository_financial_institution.n.01')     noun.group   \n",
       "2                              Synset('bank.n.03')    noun.object   \n",
       "\n",
       "                                          Definition  \\\n",
       "0  sloping land (especially the slope beside a bo...   \n",
       "1  a financial institution that accepts deposits ...   \n",
       "2                               a long ridge or pile   \n",
       "\n",
       "                                              Lemmas  \\\n",
       "0                                             [bank]   \n",
       "1  [depository_financial_institution, bank, banki...   \n",
       "2                                             [bank]   \n",
       "\n",
       "                                            Examples  \n",
       "0  [they pulled the canoe up on the bank, he sat ...  \n",
       "1  [he cashed a check at the bank, that bank hold...  \n",
       "2                             [a huge bank of earth]  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import pandas as pd\n",
    "\n",
    "term1 = 'bank'\n",
    "synsets = wordnet.synsets(term1)\n",
    "\n",
    "# display all synsets that exist in NLTK´s WordNet interface for 'bank'\n",
    "print(\"Total number of synsets for the word 'bank': \", len(synsets))\n",
    "bank_df = pd.DataFrame([{'Synset': synset,\n",
    "                         'Part of Speech': synset.lexname(),\n",
    "                         'Definition': synset.definition(),\n",
    "                         'Lemmas': synset.lemma_names(),\n",
    "                         'Examples': synset.examples()}\n",
    "                             for synset in synsets])\n",
    "bank_df = bank_df[['Synset', 'Part of Speech', 'Definition', 'Lemmas', 'Examples']]\n",
    "bank_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of synsets for the word 'mouse':  6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Synset</th>\n",
       "      <th>Part of Speech</th>\n",
       "      <th>Definition</th>\n",
       "      <th>Lemmas</th>\n",
       "      <th>Examples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Synset('mouse.n.01')</td>\n",
       "      <td>noun.animal</td>\n",
       "      <td>any of numerous small rodents typically resemb...</td>\n",
       "      <td>[mouse]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Synset('shiner.n.01')</td>\n",
       "      <td>noun.state</td>\n",
       "      <td>a swollen bruise caused by a blow to the eye</td>\n",
       "      <td>[shiner, black_eye, mouse]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Synset('mouse.n.03')</td>\n",
       "      <td>noun.person</td>\n",
       "      <td>person who is quiet or timid</td>\n",
       "      <td>[mouse]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Synset('mouse.n.04')</td>\n",
       "      <td>noun.artifact</td>\n",
       "      <td>a hand-operated electronic device that control...</td>\n",
       "      <td>[mouse, computer_mouse]</td>\n",
       "      <td>[a mouse takes much more room than a trackball]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Synset('sneak.v.01')</td>\n",
       "      <td>verb.motion</td>\n",
       "      <td>to go stealthily or furtively</td>\n",
       "      <td>[sneak, mouse, creep, pussyfoot]</td>\n",
       "      <td>[..stead of sneaking around spying on the neig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Synset('mouse.v.02')</td>\n",
       "      <td>verb.contact</td>\n",
       "      <td>manipulate the mouse of a computer</td>\n",
       "      <td>[mouse]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Synset Part of Speech  \\\n",
       "0   Synset('mouse.n.01')    noun.animal   \n",
       "1  Synset('shiner.n.01')     noun.state   \n",
       "2   Synset('mouse.n.03')    noun.person   \n",
       "3   Synset('mouse.n.04')  noun.artifact   \n",
       "4   Synset('sneak.v.01')    verb.motion   \n",
       "5   Synset('mouse.v.02')   verb.contact   \n",
       "\n",
       "                                          Definition  \\\n",
       "0  any of numerous small rodents typically resemb...   \n",
       "1       a swollen bruise caused by a blow to the eye   \n",
       "2                       person who is quiet or timid   \n",
       "3  a hand-operated electronic device that control...   \n",
       "4                      to go stealthily or furtively   \n",
       "5                 manipulate the mouse of a computer   \n",
       "\n",
       "                             Lemmas  \\\n",
       "0                           [mouse]   \n",
       "1        [shiner, black_eye, mouse]   \n",
       "2                           [mouse]   \n",
       "3           [mouse, computer_mouse]   \n",
       "4  [sneak, mouse, creep, pussyfoot]   \n",
       "5                           [mouse]   \n",
       "\n",
       "                                            Examples  \n",
       "0                                                 []  \n",
       "1                                                 []  \n",
       "2                                                 []  \n",
       "3    [a mouse takes much more room than a trackball]  \n",
       "4  [..stead of sneaking around spying on the neig...  \n",
       "5                                                 []  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "import pandas as pd\n",
    "\n",
    "term2 = 'mouse'\n",
    "synsets = wordnet.synsets(term2)\n",
    "\n",
    "# display all synsets that exist in NLTK´s WordNet interface for 'mouse'\n",
    "print(\"Total number of synsets for the word 'mouse': \", len(synsets))\n",
    "mouse_df = pd.DataFrame([{'Synset': synset,\n",
    "                         'Part of Speech': synset.lexname(),\n",
    "                         'Definition': synset.definition(),\n",
    "                         'Lemmas': synset.lemma_names(),\n",
    "                         'Examples': synset.examples()}\n",
    "                             for synset in synsets])\n",
    "mouse_df = mouse_df[['Synset', 'Part of Speech', 'Definition', 'Lemmas', 'Examples']]\n",
    "mouse_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence boundary detection\n",
    "\n",
    "- Ziel der Methode ist es, das Ende eines Satzes und den Beginn eines Neuen festzustellen.\n",
    "- Was auf den ersten Blick trival erscheint, ist bei genauerere Betrachtung nicht ganz einfach. So werden z.B. Abkürzungen häufig mit einem Punkt versehen, weshalb die pauschale Regel, dass ein Punkt das Ende eines Satzes markiert, nicht zuverlässig funktioniert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correct: \n",
      " ['The United States is a country.', 'The current president of the U.S. is Donald Trump.', 'Do you know former presidents?', 'I think, Barack Obama was one.']\n",
      "\n",
      "Incorrect: \n",
      " ['The United States is a country.', 'The current president of the U.S. is Donald Trump.', 'Do you know former presidents?', 'E.g.', 'Barack Obama was one.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize  # method detects sentences\n",
    "\n",
    "correct_detection = sent_tokenize(\"The United States is a country. The current president \"+ \n",
    "                    \"of the U.S. is Donald Trump. Do you know former presidents? \"+\n",
    "                    \"I think, Barack Obama was one.\")\n",
    "\n",
    "incorrect_detection = sent_tokenize(\"The United States is a country. The current president \"+ \n",
    "                    \"of the U.S. is Donald Trump. Do you know former presidents? \"+\n",
    "                    \"E.g. Barack Obama was one.\")\n",
    "\n",
    "print(\"Correct: \\n\", correct_detection) # U.S. is correctly identified as abbreviation\n",
    "print(\"\\nIncorrect: \\n\", incorrect_detection) # E.g. not recognized abbreviation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vektorisierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary-Dictionary: \n",
      " {'the': 6, 'sun': 5, 'is': 1, 'shining': 4, 'weather': 7, 'great': 0, 'love': 2, 'nice': 3}\n",
      "One-Hot-Encoding Text: \n",
      " [[0 1 0 0 1 1 1 0]\n",
      " [1 1 0 0 0 0 1 1]\n",
      " [0 0 1 1 0 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer()\n",
    "\n",
    "text = np.array([\"The sun is shining\",\n",
    "                 \"The weather is great\",\n",
    "                 \"I love nice weather\"])\n",
    "\n",
    "bag = count.fit_transform(text)\n",
    "print(\"Vocabulary-Dictionary: \\n\", count.vocabulary_)\n",
    "print(\"One-Hot-Encoding Text: \\n\", bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary-Dictionary: \n",
      " {'the': 1, 'is': 2, 'weather': 3, 'sun': 4, 'shining': 5, 'great': 6, 'i': 7, 'love': 8, 'nice': 9}\n",
      "Sequence: \n",
      " [[1, 4, 2, 5], [1, 3, 2, 6], [7, 8, 9, 3]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer \n",
    "\n",
    "text = np.array([\"The sun is shining\",\n",
    "                 \"The weather is great\",\n",
    "                 \"I love nice weather\"])\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer() \n",
    "tokenizer.fit_on_texts(text) \n",
    "my_vocabulary = tokenizer.word_index\n",
    "print(\"Vocabulary-Dictionary: \\n\", my_vocabulary)\n",
    "\n",
    "# Vectorization\n",
    "print(\"Sequence: \\n\", my_tokenizer.texts_to_sequences(text))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
